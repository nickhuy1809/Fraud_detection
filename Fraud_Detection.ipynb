{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aea551de",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection\n",
    "\n",
    "## H·ªå V√Ä T√äN: Cao T·∫•n Ho√†ng Huy\n",
    "## MSSV: 23127051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c0628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai b√°o c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setup\n",
    "np.random.seed(42)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d303494c",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df99c6",
   "metadata": {},
   "source": [
    "## Load d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325dbf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('creditcard.csv', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Skip header v√† l·∫•y t√™n columns\n",
    "header = lines[0].strip().replace('\"', '').split(',')\n",
    "print(f\"Found {len(header)} columns: {header}\")\n",
    "\n",
    "data_list = []\n",
    "error_lines = []\n",
    "\n",
    "for i, line in enumerate(lines[1:], 1):\n",
    "    try:\n",
    "        # X·ª≠ l√Ω d√≤ng v√† chuy·ªÉn ƒë·ªïi sang float\n",
    "        line = line.strip()\n",
    "        if not line:  # Skip empty lines\n",
    "            continue\n",
    "            \n",
    "        values = line.split(',')\n",
    "        \n",
    "        # Remove quotes if present v√† convert to float\n",
    "        float_values = []\n",
    "        for val in values:\n",
    "            val = val.strip().strip('\"')\n",
    "            float_values.append(float(val))\n",
    "        \n",
    "        if len(float_values) == 31:  # Ensure we have all columns\n",
    "            data_list.append(float_values)\n",
    "        else:\n",
    "            error_lines.append((i, len(float_values)))\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_lines.append((i, str(e)))\n",
    "        if len(error_lines) < 10:  # Only show first 10 errors\n",
    "            print(f\"Error at line {i}: {e}\")\n",
    "            print(f\"Line content: {line[:100]}\")\n",
    "\n",
    "if error_lines:\n",
    "    print(f\"Found {len(error_lines)} problematic lines\")\n",
    "else:\n",
    "    print(\"All lines parsed successfully\")\n",
    "\n",
    "data = np.array(data_list, dtype=np.float64)\n",
    "\n",
    "print(f\"\\nDATASET OVERVIEW:\")\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {data.shape}\")\n",
    "print(f\"Type: {data.dtype}\")\n",
    "print(f\"Size: {data.nbytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "# T·∫°o mapping cho columns\n",
    "column_names = header\n",
    "print(f\"\\nCOLUMN INFORMATION:\")\n",
    "for i, col_name in enumerate(column_names):\n",
    "    print(f\"Column {i:2d}: {col_name}\")\n",
    "\n",
    "# Quick preview c·ªßa data\n",
    "print(f\"\\nDATA PREVIEW:\")\n",
    "print(f\"First 5 rows (showing Time, V1, V2, Amount, Class):\")\n",
    "preview_cols = [0, 1, 2, 29, 30]  # Time, V1, V2, Amount, Class\n",
    "preview_names = [column_names[i] for i in preview_cols]\n",
    "\n",
    "for i in range(min(5, data.shape[0])):\n",
    "    values = [f\"{data[i, col]:.2f}\" for col in preview_cols]\n",
    "    print(f\"   Row {i+1}: \" + \" | \".join(f\"{name}={val}\" for name, val in zip(preview_names, values)))\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nBASIC STATISTICS:\")\n",
    "print(f\"Total transactions: {data.shape[0]:,}\")\n",
    "print(f\"Total features: {data.shape[1]}\")\n",
    "\n",
    "# Class distribution\n",
    "class_column = data[:, -1]  # Last column is Class\n",
    "unique_classes, class_counts = np.unique(class_column, return_counts=True)\n",
    "print(f\"\\nCLASS DISTRIBUTION:\")\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    percentage = (count / len(class_column)) * 100\n",
    "    label = \"Normal\" if cls == 0.0 else \"Fraud\"\n",
    "    print(f\" {label} ({cls}): {count:>6,} transactions ({percentage:>5.2f}%)\")\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "if len(unique_classes) == 2:\n",
    "    normal_count = class_counts[0] if unique_classes[0] == 0 else class_counts[1]\n",
    "    fraud_count = class_counts[1] if unique_classes[1] == 1 else class_counts[0]\n",
    "    imbalance_ratio = normal_count / fraud_count\n",
    "    print(f\"Imbalance ratio: {imbalance_ratio:.1f}:1 (Normal:Fraud)\")\n",
    "\n",
    "print(f\"\\nDataset ready for exploration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d16b94f",
   "metadata": {},
   "source": [
    "## Ki·ªÉm tra missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bdf643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra Missing Values (NumPy only)\n",
    "print(\"\\nMISSING VALUES OF EACH ATTRIBUTE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Ki·ªÉm tra t·ªïng quan\n",
    "total_missing = np.isnan(data).sum()\n",
    "total_percentage = (total_missing / data.size) * 100\n",
    "\n",
    "print(f\"Data overview\")\n",
    "print(f\"  Dataset shape: {data.shape}\")\n",
    "print(f\"  Total missing values: {total_missing} ({total_percentage:.4f}%)\")\n",
    "\n",
    "# Ph√¢n t√≠ch missing values cho t·ª´ng attribute\n",
    "print(f\"\\nEach attribute:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "missing_summary = []\n",
    "\n",
    "for i, feature_name in enumerate(column_names):\n",
    "    column_data = data[:, i]\n",
    "    missing_in_column = np.isnan(column_data).sum()\n",
    "    missing_pct = (missing_in_column / len(column_data)) * 100\n",
    "    \n",
    "    missing_summary.append({\n",
    "        'feature': feature_name,\n",
    "        'missing_count': missing_in_column,\n",
    "        'missing_percentage': missing_pct\n",
    "    })\n",
    "    \n",
    "    # In th√¥ng tin cho t·∫•t c·∫£ features\n",
    "    print(f\"{feature_name:>8}: {missing_in_column:>6} missing ({missing_pct:>6.2f}%)\")\n",
    "\n",
    "# Ki·ªÉm tra xem c√≥ attribute n√†o c√≥ missing values kh√¥ng\n",
    "has_missing = any(item['missing_count'] > 0 for item in missing_summary)\n",
    "\n",
    "print(f\"\\nOVERALL\")\n",
    "if not has_missing:\n",
    "    print(\"NO MISSING DATA\")\n",
    "else:\n",
    "    missing_features = [item for item in missing_summary if item['missing_count'] > 0]\n",
    "    print(f\"There are {len(missing_features)} attributes have missing values:\")\n",
    "    for item in missing_features:\n",
    "        print(f\"     - {item['feature']}: {item['missing_count']} values ({item['missing_percentage']:.2f}%)\")\n",
    "\n",
    "# Hi·ªÉn th·ªã th·ªëng k√™ chi ti·∫øt cho m·ªôt s·ªë features quan tr·ªçng  \n",
    "print(f\"\\nIMPORTANT FEATURES:\")\n",
    "important_features = ['Time', 'Amount', 'Class']\n",
    "\n",
    "for feature_name in important_features:\n",
    "    # T√¨m index c·ªßa feature\n",
    "    feature_idx = column_names.index(feature_name)\n",
    "    column_data = data[:, feature_idx]\n",
    "    missing_count = np.isnan(column_data).sum()\n",
    "    valid_count = len(column_data) - missing_count\n",
    "    \n",
    "    print(f\"{feature_name}:\")\n",
    "    print(f\"Total values: {len(column_data):>8}\")\n",
    "    print(f\"Valid values: {valid_count:>8}\")\n",
    "    print(f\"Missing:      {missing_count:>8}\")\n",
    "    print(f\"Complete:     {(valid_count/len(column_data)*100):>6.2f}%\")\n",
    "    \n",
    "    # Hi·ªÉn th·ªã m·ªôt v√†i gi√° tr·ªã m·∫´u n·∫øu kh√¥ng missing\n",
    "    if missing_count == 0:\n",
    "        print(f\"Sample values: {column_data[:5]}\")\n",
    "        if feature_name == 'Class':\n",
    "            unique_vals = np.unique(column_data)\n",
    "            print(f\"Unique values: {unique_vals}\")\n",
    "    print()\n",
    "\n",
    "# Ki·ªÉm tra c√°c V features (PCA components)\n",
    "v_features = [name for name in column_names if name.startswith('V')]\n",
    "v_missing_count = 0\n",
    "for feature_name in v_features:\n",
    "    feature_idx = column_names.index(feature_name)\n",
    "    v_missing_count += np.isnan(data[:, feature_idx]).sum()\n",
    "\n",
    "print(f\"V FEATURES (PCA COMPONENTS):\")\n",
    "print(f\"    - Number of V - features: {len(v_features)} (V1 ƒë·∫øn V28)\")  \n",
    "print(f\"    - Number of missing V - features: {v_missing_count}\")\n",
    "\n",
    "# Th·ªëng k√™ m√¥ t·∫£ (NumPy implementation)\n",
    "print(f\"\\nDESCRIPTIVE STATISTICS (Pure NumPy)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def numpy_describe(arr, name):\n",
    "    # Lo·∫°i b·ªè NaN values n·∫øu c√≥\n",
    "    clean_arr = arr[~np.isnan(arr)]\n",
    "    if len(clean_arr) == 0:\n",
    "        return None\n",
    "        \n",
    "    return {\n",
    "        'count': len(clean_arr),\n",
    "        'mean': np.mean(clean_arr),\n",
    "        'std': np.std(clean_arr, ddof=1) if len(clean_arr) > 1 else 0,\n",
    "        'min': np.min(clean_arr),\n",
    "        'q25': np.percentile(clean_arr, 25),\n",
    "        'q50': np.percentile(clean_arr, 50),  # median\n",
    "        'q75': np.percentile(clean_arr, 75),\n",
    "        'max': np.max(clean_arr),\n",
    "        'skew': np.mean(((clean_arr - np.mean(clean_arr)) / np.std(clean_arr))**3) if np.std(clean_arr) > 0 else 0\n",
    "    }\n",
    "\n",
    "# Ph√¢n t√≠ch Time, Amount v√† Class\n",
    "key_features = ['Time', 'Amount', 'Class']\n",
    "\n",
    "for feature_name in key_features:\n",
    "    feature_idx = column_names.index(feature_name)\n",
    "    feature_data = data[:, feature_idx]\n",
    "    stats = numpy_describe(feature_data, feature_name)\n",
    "    \n",
    "    if stats is not None:\n",
    "        print(f\"\\n{feature_name.upper()}:\")\n",
    "        print(f\"  Count: {stats['count']:>10,}\")\n",
    "        print(f\"  Mean:  {stats['mean']:>10.2f}\")\n",
    "        print(f\"  Std:   {stats['std']:>10.2f}\")\n",
    "        print(f\"  Min:   {stats['min']:>10.2f}\")\n",
    "        print(f\"  25%:   {stats['q25']:>10.2f}\")\n",
    "        print(f\"  50%:   {stats['q50']:>10.2f}\")\n",
    "        print(f\"  75%:   {stats['q75']:>10.2f}\")\n",
    "        print(f\"  Max:   {stats['max']:>10.2f}\")\n",
    "        print(f\"  Skew:  {stats['skew']:>10.3f}\")\n",
    "        \n",
    "        # Th√¥ng tin ƒë·∫∑c bi·ªát cho Class\n",
    "        if feature_name == 'Class':\n",
    "            unique_values = np.unique(feature_data[~np.isnan(feature_data)])\n",
    "            print(f\"  Unique values: {unique_values}\")\n",
    "            for val in unique_values:\n",
    "                count = np.sum(feature_data == val)\n",
    "                pct = (count / len(feature_data)) * 100\n",
    "                label = \"Normal\" if val == 0 else \"Fraud\"\n",
    "                print(f\"    {label} ({val}): {count:>6} ({pct:>5.2f}%)\")\n",
    "    else:\n",
    "        print(f\"\\n{feature_name.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844063bb",
   "metadata": {},
   "source": [
    "## T√≥m t·∫Øt k·∫øt qu·∫£ ph√¢n t√≠ch Missing Values\n",
    "\n",
    "### **K·∫æT LU·∫¨N CH√çNH:**\n",
    "* **Dataset kh√¥ng c√≥ missing values** n√†o trong t·∫•t c·∫£ 31 attributes\n",
    "\n",
    "### **CHI TI·∫æT PH√ÇN T√çCH:**\n",
    "\n",
    "**1. T·ªïng quan Dataset:**\n",
    "- **Total records**: 284,807 transactions\n",
    "- **Total attributes**: 31 features\n",
    "- **Missing values**: 0 (0.0000%)\n",
    "- **Data quality**: Excellent\n",
    "\n",
    "**2. Breakdown theo lo·∫°i features:**\n",
    "- **Time**: 100% complete (0 missing)\n",
    "- **V1-V28 (PCA features)**: T·∫•t c·∫£ 28 features ƒë·ªÅu 100% complete  \n",
    "- **Amount**: 100% complete (0 missing)\n",
    "- **Class (target)**: 100% complete (0 missing)\n",
    "\n",
    "**3. Class Distribution:**\n",
    "- **Normal transactions**: 284,315 (99.83%)\n",
    "- **Fraud transactions**: 492 (0.17%)\n",
    "- **Imbalance ratio**: ~578:1 (highly imbalanced)\n",
    "\n",
    "### **K·∫øt qu·∫£:**\n",
    "- **Kh√¥ng c·∫ßn preprocessing cho missing data**\n",
    "- **Class imbalance** (ch·ªâ 0.17% fraud cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696dba8",
   "metadata": {},
   "source": [
    "## Ph√¢n t√≠ch Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e59218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ph√¢n t√≠ch Outliers (Pure NumPy Implementation)\n",
    "print(\"\\nPH√ÇN T√çCH OUTLIERS CHO T·ª™NG ATTRIBUTE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def detect_outliers_iqr(data_column):\n",
    "    # Calculate quartiles\n",
    "    q1 = np.percentile(data_column, 25)\n",
    "    q3 = np.percentile(data_column, 75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # Calculate outlier bounds\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Find outliers\n",
    "    outlier_mask = (data_column < lower_bound) | (data_column > upper_bound)\n",
    "    outlier_indices = np.where(outlier_mask)[0]\n",
    "    outlier_values = data_column[outlier_mask]\n",
    "    \n",
    "    return {\n",
    "        'indices': outlier_indices,\n",
    "        'values': outlier_values,\n",
    "        'count': len(outlier_indices),\n",
    "        'percentage': (len(outlier_indices) / len(data_column)) * 100,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound,\n",
    "        'q1': q1,\n",
    "        'q3': q3,\n",
    "        'iqr': iqr\n",
    "    }\n",
    "\n",
    "def detect_outliers_zscore(data_column, threshold=3):\n",
    "    mean_val = np.mean(data_column)\n",
    "    std_val = np.std(data_column)\n",
    "    z_scores = np.abs((data_column - mean_val) / std_val)\n",
    "    \n",
    "    outlier_mask = z_scores > threshold\n",
    "    outlier_indices = np.where(outlier_mask)[0]\n",
    "    outlier_values = data_column[outlier_mask]\n",
    "    \n",
    "    return {\n",
    "        'indices': outlier_indices,\n",
    "        'values': outlier_values,\n",
    "        'count': len(outlier_indices),\n",
    "        'percentage': (len(outlier_indices) / len(data_column)) * 100,\n",
    "        'z_scores': z_scores[outlier_mask],\n",
    "        'threshold': threshold\n",
    "    }\n",
    "\n",
    "# Ph√¢n t√≠ch outliers cho c√°c features quan tr·ªçng\n",
    "features_to_analyze = ['Time', 'Amount']\n",
    "print(f\"PH√ÇN T√çCH CHI TI·∫æT OUTLIERS:\")\n",
    "\n",
    "outlier_summary = {}\n",
    "\n",
    "for feature_name in features_to_analyze:\n",
    "    feature_idx = column_names.index(feature_name)\n",
    "    feature_data = data[:, feature_idx]\n",
    "    \n",
    "    print(f\"\\n{feature_name.upper()}:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Method 1: IQR Method\n",
    "    iqr_results = detect_outliers_iqr(feature_data)\n",
    "    print(f\"IQR Method:\")\n",
    "    print(f\"   Q1: {iqr_results['q1']:>12.2f}\")\n",
    "    print(f\"   Q3: {iqr_results['q3']:>12.2f}\")\n",
    "    print(f\"   IQR: {iqr_results['iqr']:>11.2f}\")\n",
    "    print(f\"   Lower Bound: {iqr_results['lower_bound']:>6.2f}\")\n",
    "    print(f\"   Upper Bound: {iqr_results['upper_bound']:>6.2f}\")\n",
    "    print(f\"   Outliers: {iqr_results['count']:>8} ({iqr_results['percentage']:>5.2f}%)\")\n",
    "    \n",
    "    # Method 2: Z-Score Method\n",
    "    zscore_results = detect_outliers_zscore(feature_data, threshold=3)\n",
    "    print(f\"\\nZ-Score Method (threshold=3):\")\n",
    "    print(f\"   Outliers: {zscore_results['count']:>8} ({zscore_results['percentage']:>5.2f}%)\")\n",
    "    \n",
    "    # Show some outlier examples\n",
    "    if iqr_results['count'] > 0:\n",
    "        print(f\"\\nSample Outlier Values (IQR):\")\n",
    "        sample_count = min(10, len(iqr_results['values']))\n",
    "        sample_values = iqr_results['values'][:sample_count]\n",
    "        sample_indices = iqr_results['indices'][:sample_count]\n",
    "        \n",
    "        for i, (idx, val) in enumerate(zip(sample_indices, sample_values)):\n",
    "            print(f\"   Row {idx:>6}: {val:>12.2f}\")\n",
    "    \n",
    "    # Store results for summary\n",
    "    outlier_summary[feature_name] = {\n",
    "        'iqr': iqr_results,\n",
    "        'zscore': zscore_results\n",
    "    }\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Ph√¢n t√≠ch outliers cho m·ªôt s·ªë V features (sample)\n",
    "print(f\"\\nV FEATURES OUTLIER SAMPLE (V1, V2, V3):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "v_sample_features = ['V1', 'V2', 'V3']\n",
    "for feature_name in v_sample_features:\n",
    "    feature_idx = column_names.index(feature_name)\n",
    "    feature_data = data[:, feature_idx]\n",
    "    \n",
    "    iqr_results = detect_outliers_iqr(feature_data)\n",
    "    zscore_results = detect_outliers_zscore(feature_data)\n",
    "    \n",
    "    print(f\"{feature_name}: IQR={iqr_results['count']:>4} ({iqr_results['percentage']:>5.2f}%) | \"\n",
    "          f\"Z-Score={zscore_results['count']:>4} ({zscore_results['percentage']:>5.2f}%)\")\n",
    "\n",
    "# Overall outlier analysis\n",
    "print(f\"\\nOVERALL OUTLIERS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for feature_name, results in outlier_summary.items():\n",
    "    iqr_count = results['iqr']['count']\n",
    "    zscore_count = results['zscore']['count']\n",
    "    total_records = data.shape[0]\n",
    "    \n",
    "    print(f\"{feature_name}:\")\n",
    "    print(f\"   Total records: {total_records:>8,}\")\n",
    "    print(f\"   IQR outliers:  {iqr_count:>8,} ({iqr_count/total_records*100:>5.2f}%)\")\n",
    "    print(f\"   Z-Score outliers: {zscore_count:>5,} ({zscore_count/total_records*100:>5.2f}%)\")\n",
    "    \n",
    "    # Outlier severity assessment\n",
    "    if iqr_count / total_records > 0.1:  # >10%\n",
    "        severity = \"High\"\n",
    "    elif iqr_count / total_records > 0.05:  # >5%\n",
    "        severity = \"Medium\" \n",
    "    else:\n",
    "        severity = \"Low\"\n",
    "    print(f\"   Severity: {severity}\")\n",
    "    print()\n",
    "\n",
    "# Outliers vs Class analysis\n",
    "print(f\"OUTLIERS VS FRAUD ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "class_idx = column_names.index('Class')\n",
    "class_data = data[:, class_idx]\n",
    "\n",
    "for feature_name in features_to_analyze:\n",
    "    feature_idx = column_names.index(feature_name)\n",
    "    iqr_results = outlier_summary[feature_name]['iqr']\n",
    "    \n",
    "    if iqr_results['count'] > 0:\n",
    "        # Check class distribution in outliers\n",
    "        outlier_classes = class_data[iqr_results['indices']]\n",
    "        normal_outliers = np.sum(outlier_classes == 0)\n",
    "        fraud_outliers = np.sum(outlier_classes == 1)\n",
    "        \n",
    "        print(f\"{feature_name} Outliers:\")\n",
    "        print(f\"   Normal transactions: {normal_outliers:>6} ({normal_outliers/iqr_results['count']*100:>5.2f}%)\")\n",
    "        print(f\"   Fraud transactions:  {fraud_outliers:>6} ({fraud_outliers/iqr_results['count']*100:>5.2f}%)\")\n",
    "        \n",
    "        # Compare with overall fraud rate\n",
    "        overall_fraud_rate = np.sum(class_data == 1) / len(class_data) * 100\n",
    "        outlier_fraud_rate = fraud_outliers / iqr_results['count'] * 100\n",
    "        \n",
    "        if outlier_fraud_rate > overall_fraud_rate * 2:\n",
    "            print(f\"Fraud rate in outliers ({outlier_fraud_rate:.2f}%) >> overall rate ({overall_fraud_rate:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"Fraud rate in outliers ({outlier_fraud_rate:.2f}%) ~ overall rate ({overall_fraud_rate:.2f}%)\")\n",
    "        print()\n",
    "\n",
    "print(f\"Outlier analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b187bd7",
   "metadata": {},
   "source": [
    "### T√≥m t·∫Øt ph√¢n t√≠ch Outliers\n",
    "\n",
    "#### **K·∫æT QU·∫¢ CH√çNH:**\n",
    "\n",
    "**1. Time Feature:**\n",
    "- **Kh√¥ng c√≥ outliers** (0.00% v·ªõi c·∫£ IQR v√† Z-Score methods)\n",
    "- **Ph√¢n ph·ªëi ƒë·ªÅu** trong kho·∫£ng th·ªùi gian quan s√°t\n",
    "- **Severity: Low** \n",
    "\n",
    "**2. Amount Feature:**\n",
    "- **C√≥ nhi·ªÅu outliers** (11.20% v·ªõi IQR method, 1.43% v·ªõi Z-Score)\n",
    "- **Severity: High** \n",
    "- **Sample outliers**: $378.66, $1402.95, $1142.02 (c√°c giao d·ªãch gi√° tr·ªã cao)\n",
    "- **Fraud rate trong outliers** (0.29%) t∆∞∆°ng ƒë∆∞∆°ng overall rate (0.17%)\n",
    "\n",
    "**3. V Features (PCA Components):**\n",
    "- **V1**: 2.48% outliers (IQR) \n",
    "- **V2**: 4.75% outliers (IQR)\n",
    "- **V3**: 1.18% outliers (IQR)\n",
    "\n",
    "#### **INSIGHTS:**\n",
    "\n",
    "**K·∫øt qu·∫£ nghi·ªám thu**\n",
    "- Time feature r·∫•t clean, kh√¥ng c√≥ outliers\n",
    "- Fraud transactions kh√¥ng t·∫≠p trung trong outliers\n",
    "- Outlier rate trong Amount kh√¥ng qu√° n·∫∑ng \n",
    "\n",
    "**ƒêi·ªÉm l∆∞u √Ω**\n",
    "- Amount c√≥ nhi·ªÅu outliers \n",
    "- V features c≈©ng c√≥ outliers nh∆∞ng ·ªü m·ª©c moderate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0231042c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f4e6d45",
   "metadata": {},
   "source": [
    "# DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d25822",
   "metadata": {},
   "source": [
    "## 1. Ki·ªÉm tra s·ª± m·∫•t c√¢n b·∫±ng d·ªØ li·ªáu (Class Imbalance Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f50fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ph√¢n t√≠ch Class Imbalance\n",
    "print(\"CLASS IMBALANCE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class_idx = column_names.index('Class')\n",
    "class_data = data[:, class_idx]\n",
    "unique_classes, class_counts = np.unique(class_data, return_counts=True)\n",
    "\n",
    "# T√≠nh to√°n t·ª∑ l·ªá\n",
    "total_transactions = len(class_data)\n",
    "normal_count = class_counts[0] if unique_classes[0] == 0 else class_counts[1]\n",
    "fraud_count = class_counts[1] if unique_classes[1] == 1 else class_counts[0]\n",
    "\n",
    "normal_pct = (normal_count / total_transactions) * 100\n",
    "fraud_pct = (fraud_count / total_transactions) * 100\n",
    "imbalance_ratio = normal_count / fraud_count\n",
    "\n",
    "print(f\"Class Distribution:\")\n",
    "print(f\" Normal (0): {normal_count:>8,} transactions ({normal_pct:>5.2f}%)\")\n",
    "print(f\" Fraud (1):  {fraud_count:>8,} transactions ({fraud_pct:>5.2f}%)\")\n",
    "print(f\" Total:      {total_transactions:>8,} transactions\")\n",
    "print(f\" Imbalance ratio: {imbalance_ratio:.1f}:1 (Normal:Fraud)\")\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# 1. Pie Chart\n",
    "labels = ['Normal Transactions', 'Fraud Transactions']\n",
    "sizes = [normal_count, fraud_count]\n",
    "colors = ['#2E8B57', '#DC143C']\n",
    "explode = (0, 0.1) \n",
    "\n",
    "wedges, texts, autotexts = ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.2f%%',\n",
    "                                   explode=explode, shadow=True, startangle=90)\n",
    "\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "    autotext.set_fontsize(11)\n",
    "\n",
    "ax1.set_title('Class Distribution - Pie Chart', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# 2. Bar Chart\n",
    "bars = ax2.bar(labels, sizes, color=colors, alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "for i, (bar, count) in enumerate(zip(bars, sizes)):\n",
    "    height = bar.get_height()\n",
    "    pct = (count / total_transactions) * 100\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + max(sizes)*0.01,\n",
    "             f'{count:,}\\n({pct:.2f}%)', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax2.set_ylabel('Number of Transactions', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Class Distribution - Bar Chart', fontsize=14, fontweight='bold', pad=20)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Th√™m insights chi ti·∫øt\n",
    "print(f\"\\nKEY INSIGHTS:\")\n",
    "print(f\" SEVERE CLASS IMBALANCE detected!\")\n",
    "print(f\" Fraud transactions only have {fraud_pct:.3f}% total dataset\")\n",
    "print(f\" For each {imbalance_ratio:.0f} normal transactions there are 1 fraud\")\n",
    "\n",
    "# T√≠nh to√°n th√™m metrics\n",
    "print(f\"\\nADDITIONAL METRICS:\")\n",
    "print(f\" Fraud prevalence: {fraud_pct:.4f}%\")\n",
    "print(f\" Random guess accuracy: {max(normal_pct, fraud_pct):.2f}%\")\n",
    "print(f\" If fraud avg loss = $100, potential daily loss estimate:\")\n",
    "print(f\" - Frauds per day (assume uniform): ~{fraud_count/2:.0f}\")\n",
    "print(f\" - Potential loss per day: ~${fraud_count/2*100:,.0f}\")\n",
    "\n",
    "# Class imbalance severity\n",
    "if imbalance_ratio > 500:\n",
    "    severity = \"EXTREME\"\n",
    "elif imbalance_ratio > 100:\n",
    "    severity = \"SEVERE\"  \n",
    "elif imbalance_ratio > 10:\n",
    "    severity = \"MODERATE\"\n",
    "else:\n",
    "    severity = \"MILD\"\n",
    "\n",
    "print(f\"\\nRatio: {imbalance_ratio:.1f}:1 is classified as {severity} imbalance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6533d",
   "metadata": {},
   "source": [
    "### **Insight t·ª´ Class Imbalance Analysis:**\n",
    "\n",
    "**üî¥ Ph√°t hi·ªán ch√≠nh:**\n",
    "- Dataset c√≥ **m·∫•t c√¢n b·∫±ng r·∫•t n·∫∑ng** v·ªõi t·ª∑ l·ªá 578:1\n",
    "- Ch·ªâ **0.173%** giao d·ªãch l√† fraud\n",
    "- M·ªói ng√†y c√≥ th·ªÉ c√≥ ~246 fraud cases (n·∫øu ph√¢n b·ªï ƒë·ªÅu)\n",
    "- **Cost of Missing Fraud**: N·∫øu m·ªói fraud trung b√¨nh m·∫•t $100, potential loss ~$24,600/day\n",
    "- Do t·ªâ l·ªá m·∫•t c√¢n b·∫±ng r·∫•t n·∫∑ng n√™n khi l√†m model - Accuracy ƒë∆°n thu·∫ßn kh√¥ng ƒë·ªß c·∫ßn ch√∫ tr·ªçng v√†o c·∫£ Precision/Recall\n",
    "- Model s·∫Ω bias v·ªÅ Normal class (c√≥ th·ªÉ ƒë·∫°t 99.83% accuracy b·∫±ng c√°ch predict t·∫•t c·∫£ l√† Normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c039a6c9",
   "metadata": {},
   "source": [
    "## 2. Ph√¢n t√≠ch giao d·ªãch theo th·ªùi gian (Time Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86fca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TIME PATTERN ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "time_idx = column_names.index('Time')\n",
    "class_idx = column_names.index('Class')\n",
    "time_data = data[:, time_idx]\n",
    "class_data = data[:, class_idx]\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi Time t·ª´ gi√¢y th√†nh gi·ªù trong ng√†y (0-24)\n",
    "# Time trong dataset l√† seconds from first transaction\n",
    "# Gi·∫£ ƒë·ªãnh r·∫±ng first transaction x·∫£y ra l√∫c 0:00\n",
    "hours = (time_data // 3600) % 24\n",
    "normal_hours = hours[class_data == 0]\n",
    "fraud_hours = hours[class_data == 1]\n",
    "\n",
    "print(f\"Time Data Overview:\")\n",
    "print(f\" Time range: {time_data.min():.0f} - {time_data.max():.0f} seconds\")\n",
    "print(f\" Duration: {(time_data.max() - time_data.min()) / 3600:.1f} hours\")\n",
    "print(f\" Hour range: {hours.min():.0f} - {hours.max():.0f}\")\n",
    "\n",
    "# T·∫°o histogram cho m·ªói gi·ªù trong ng√†y\n",
    "hour_bins = np.arange(0, 25, 1)  \n",
    "normal_hist, _ = np.histogram(normal_hours, bins=hour_bins)\n",
    "fraud_hist, _ = np.histogram(fraud_hours, bins=hour_bins)\n",
    "\n",
    "# T√≠nh fraud rate cho m·ªói gi·ªù\n",
    "total_hist = normal_hist + fraud_hist\n",
    "fraud_rate_hourly = np.divide(fraud_hist, total_hist, \n",
    "                              out=np.zeros_like(fraud_hist, dtype=float), \n",
    "                              where=total_hist!=0) * 100\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# 1. Transaction Distribution by Hour\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "hours_range = np.arange(0, 24)\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(hours_range - width/2, normal_hist, width, \n",
    "                label='Normal', color='#2E8B57', alpha=0.8)\n",
    "bars2 = ax1.bar(hours_range + width/2, fraud_hist, width,\n",
    "                label='Fraud', color='#DC143C', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Hour of Day', fontweight='bold')\n",
    "ax1.set_ylabel('Number of Transactions', fontweight='bold')\n",
    "ax1.set_title('Transaction Distribution by Hour of Day', fontweight='bold', pad=15)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_xticks(hours_range)\n",
    "\n",
    "# 2. Fraud Rate by Hour  \n",
    "ax2 = plt.subplot(2, 2, 2)\n",
    "bars3 = ax2.bar(hours_range, fraud_rate_hourly, color='orange', alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('Hour of Day', fontweight='bold')\n",
    "ax2.set_ylabel('Fraud Rate (%)', fontweight='bold')\n",
    "ax2.set_title('Fraud Rate by Hour of Day', fontweight='bold', pad=15)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_xticks(hours_range)\n",
    "\n",
    "# Highlight peak fraud hours\n",
    "peak_threshold = np.mean(fraud_rate_hourly) + np.std(fraud_rate_hourly)\n",
    "peak_hours = hours_range[fraud_rate_hourly > peak_threshold]\n",
    "for hour in peak_hours:\n",
    "    ax2.bar(hour, fraud_rate_hourly[hour], color='red', alpha=0.9, edgecolor='darkred')\n",
    "\n",
    "# 3. Cumulative Distribution\n",
    "ax3 = plt.subplot(2, 2, 3)\n",
    "ax3.hist(normal_hours, bins=24, alpha=0.7, label='Normal', \n",
    "         color='#2E8B57', density=True, cumulative=True)\n",
    "ax3.hist(fraud_hours, bins=24, alpha=0.7, label='Fraud', \n",
    "         color='#DC143C', density=True, cumulative=True)\n",
    "ax3.set_xlabel('Hour of Day', fontweight='bold')\n",
    "ax3.set_ylabel('Cumulative Density', fontweight='bold')\n",
    "ax3.set_title('Cumulative Distribution by Hour', fontweight='bold', pad=15)\n",
    "ax3.legend()\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 4. Box Plot Comparison\n",
    "ax4 = plt.subplot(2, 2, 4)\n",
    "data_to_plot = [normal_hours, fraud_hours]\n",
    "box_plot = ax4.boxplot(data_to_plot, labels=['Normal', 'Fraud'], \n",
    "                       patch_artist=True, notch=True)\n",
    "\n",
    "# Color the boxes\n",
    "colors = ['#2E8B57', '#DC143C']\n",
    "for patch, color in zip(box_plot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax4.set_ylabel('Hour of Day', fontweight='bold')\n",
    "ax4.set_title('Hour Distribution Box Plot', fontweight='bold', pad=15)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical Analysis\n",
    "print(f\"\\nSTATISTICAL ANALYSIS:\")\n",
    "print(f\" Normal Transactions:\")\n",
    "print(f\" Mean hour: {np.mean(normal_hours):>6.2f}\")\n",
    "print(f\" Median hour: {np.median(normal_hours):>4.1f}\")\n",
    "print(f\" Std dev: {np.std(normal_hours):>8.2f}\")\n",
    "\n",
    "print(f\"\\nFraud Transactions:\")  \n",
    "print(f\" Mean hour: {np.mean(fraud_hours):>6.2f}\")\n",
    "print(f\" Median hour: {np.median(fraud_hours):>4.1f}\")\n",
    "print(f\" Std dev: {np.std(fraud_hours):>8.2f}\")\n",
    "\n",
    "# Peak analysis\n",
    "overall_fraud_rate = len(fraud_hours) / len(hours) * 100\n",
    "peak_hours_list = peak_hours.tolist() if len(peak_hours) > 0 else []\n",
    "\n",
    "print(f\"\\nPEAK FRAUD HOURS ANALYSIS:\")\n",
    "print(f\" Overall fraud rate: {overall_fraud_rate:.4f}%\")\n",
    "print(f\" Peak threshold: {peak_threshold:.4f}%\")\n",
    "\n",
    "if len(peak_hours_list) > 0:\n",
    "    print(f\"Peak fraud hours: {peak_hours_list}\")\n",
    "    for hour in peak_hours_list:\n",
    "        rate = fraud_rate_hourly[hour]\n",
    "        multiplier = rate / overall_fraud_rate if overall_fraud_rate > 0 else 0\n",
    "        print(f\" Hour {hour:2d}: {rate:.4f}% ({multiplier:.1f}x higher than average)\")\n",
    "else:\n",
    "    print(f\"No significant peak hours detected\")\n",
    "\n",
    "# Time window analysis\n",
    "print(f\"\\nTIME WINDOW ANALYSIS:\")\n",
    "time_windows = [\n",
    "    (\"Late Night\", 0, 6),\n",
    "    (\"Morning\", 6, 12), \n",
    "    (\"Afternoon\", 12, 18),\n",
    "    (\"Evening\", 18, 24)\n",
    "]\n",
    "\n",
    "for window_name, start_h, end_h in time_windows:\n",
    "    window_mask = (hours >= start_h) & (hours < end_h)\n",
    "    window_total = np.sum(window_mask)\n",
    "    window_fraud = np.sum(class_data[window_mask] == 1)\n",
    "    window_fraud_rate = (window_fraud / window_total * 100) if window_total > 0 else 0\n",
    "    \n",
    "    print(f\" {window_name:>12} ({start_h:2d}:00-{end_h:2d}:00): \"\n",
    "          f\"{window_fraud:>3} frauds / {window_total:>6} total = {window_fraud_rate:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff3839e",
   "metadata": {},
   "source": [
    "### **Insights t·ª´ Time Pattern Analysis:**\n",
    "\n",
    "**Ph√°t hi·ªán quan tr·ªçng:**\n",
    "- **Hour 2:00-3:00**: Fraud rate **1.71%** (9.9x higher than average!)\n",
    "- **Hour 4:00-5:00**: Fraud rate **1.04%** (6.0x higher than average)\n",
    "- **Late Night (0:00-6:00)**: Fraud rate **0.518%** - highest time window\n",
    "\n",
    "**Ph√¢n t√≠ch c√°c pattern:**\n",
    "- **Fraud transactions** x·∫£y ra s·ªõm h∆°n (mean: 11.65h vs 14.05h so v·ªõi b√¨nh th∆∞·ªùng)\n",
    "- **Higher variance** trong fraud timing (std: 6.66 vs 5.83) \n",
    "- **Night owl effect**: Th∆∞·ªùng s·∫Ω di·ªÖn ra fraud trong s√°ng s·ªõm ho·∫∑c ƒë√™m khuya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6b40c9",
   "metadata": {},
   "source": [
    "## 3. Ph√¢n t√≠ch s·ªë ti·ªÅn giao d·ªãch (Amount Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8643e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRANSACTION AMOUNT ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "amount_idx = column_names.index('Amount')\n",
    "class_idx = column_names.index('Class')\n",
    "amount_data = data[:, amount_idx]\n",
    "class_data = data[:, class_idx]\n",
    "\n",
    "# T√°ch data theo class\n",
    "normal_amounts = amount_data[class_data == 0]\n",
    "fraud_amounts = amount_data[class_data == 1]\n",
    "\n",
    "print(f\"Amount Data Overview:\")\n",
    "print(f\"   Overall range: ${amount_data.min():.2f} - ${amount_data.max():,.2f}\")\n",
    "print(f\"   Normal range:  ${normal_amounts.min():.2f} - ${normal_amounts.max():,.2f}\")\n",
    "print(f\"   Fraud range:   ${fraud_amounts.min():.2f} - ${fraud_amounts.max():,.2f}\")\n",
    "\n",
    "# Statistical comparison\n",
    "print(f\"\\nStatistical Comparison:\")\n",
    "stats_comparison = {\n",
    "    'Metric': ['Count', 'Mean', 'Median', 'Std Dev', 'Min', 'Max', 'Q1', 'Q3'],\n",
    "    'Normal': [\n",
    "        f\"{len(normal_amounts):,}\",\n",
    "        f\"${np.mean(normal_amounts):.2f}\",\n",
    "        f\"${np.median(normal_amounts):.2f}\",\n",
    "        f\"${np.std(normal_amounts):.2f}\",\n",
    "        f\"${np.min(normal_amounts):.2f}\",\n",
    "        f\"${np.max(normal_amounts):,.2f}\",\n",
    "        f\"${np.percentile(normal_amounts, 25):.2f}\",\n",
    "        f\"${np.percentile(normal_amounts, 75):.2f}\"\n",
    "    ],\n",
    "    'Fraud': [\n",
    "        f\"{len(fraud_amounts):,}\",\n",
    "        f\"${np.mean(fraud_amounts):.2f}\",\n",
    "        f\"${np.median(fraud_amounts):.2f}\",\n",
    "        f\"${np.std(fraud_amounts):.2f}\",\n",
    "        f\"${np.min(fraud_amounts):.2f}\",\n",
    "        f\"${np.max(fraud_amounts):,.2f}\",\n",
    "        f\"${np.percentile(fraud_amounts, 25):.2f}\",\n",
    "        f\"${np.percentile(fraud_amounts, 75):.2f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for i, metric in enumerate(stats_comparison['Metric']):\n",
    "    print(f\"   {metric:>10}: Normal = {stats_comparison['Normal'][i]:>12} | Fraud = {stats_comparison['Fraud'][i]:>12}\")\n",
    "\n",
    "# T·∫°o comprehensive visualization\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 1. Histogram Comparison (Full Range)\n",
    "ax1 = plt.subplot(2, 4, 1)\n",
    "bins_full = np.linspace(0, np.max(amount_data), 100)\n",
    "ax1.hist(normal_amounts, bins=bins_full, alpha=0.7, label='Normal', \n",
    "         color='#2E8B57', density=True, edgecolor='black', linewidth=0.5)\n",
    "ax1.hist(fraud_amounts, bins=bins_full, alpha=0.7, label='Fraud', \n",
    "         color='#DC143C', density=True, edgecolor='black', linewidth=0.5)\n",
    "ax1.set_xlabel('Amount ($)', fontweight='bold')\n",
    "ax1.set_ylabel('Density', fontweight='bold')\n",
    "ax1.set_title('Amount Distribution (Full Range)', fontweight='bold', pad=15)\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# 2. Histogram Comparison (Zoomed: 0-1000)\n",
    "ax2 = plt.subplot(2, 4, 2)\n",
    "bins_zoom = np.linspace(0, 1000, 50)\n",
    "ax2.hist(normal_amounts[normal_amounts <= 1000], bins=bins_zoom, alpha=0.7, \n",
    "         label=f'Normal (‚â§$1000): {np.sum(normal_amounts <= 1000):,}', \n",
    "         color='#2E8B57', density=True, edgecolor='black', linewidth=0.5)\n",
    "ax2.hist(fraud_amounts[fraud_amounts <= 1000], bins=bins_zoom, alpha=0.7, \n",
    "         label=f'Fraud (‚â§$1000): {np.sum(fraud_amounts <= 1000):,}', \n",
    "         color='#DC143C', density=True, edgecolor='black', linewidth=0.5)\n",
    "ax2.set_xlabel('Amount ($)', fontweight='bold')\n",
    "ax2.set_ylabel('Density', fontweight='bold')\n",
    "ax2.set_title('Amount Distribution (0-$1,000)', fontweight='bold', pad=15)\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# 3. Log Scale Distribution\n",
    "ax3 = plt.subplot(2, 4, 3)\n",
    "# Remove zeros for log scale\n",
    "normal_nonzero = normal_amounts[normal_amounts > 0]\n",
    "fraud_nonzero = fraud_amounts[fraud_amounts > 0]\n",
    "\n",
    "bins_log = np.logspace(np.log10(0.1), np.log10(np.max(amount_data)), 50)\n",
    "ax3.hist(normal_nonzero, bins=bins_log, alpha=0.7, label='Normal', \n",
    "         color='#2E8B57', density=True)\n",
    "ax3.hist(fraud_nonzero, bins=bins_log, alpha=0.7, label='Fraud', \n",
    "         color='#DC143C', density=True)\n",
    "ax3.set_xscale('log')\n",
    "ax3.set_xlabel('Amount ($) - Log Scale', fontweight='bold')\n",
    "ax3.set_ylabel('Density', fontweight='bold')\n",
    "ax3.set_title('Amount Distribution (Log Scale)', fontweight='bold', pad=15)\n",
    "ax3.legend()\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 4. Box Plot Comparison\n",
    "ax4 = plt.subplot(2, 4, 4)\n",
    "data_to_plot = [normal_amounts, fraud_amounts]\n",
    "box_plot = ax4.boxplot(data_to_plot, tick_labels=['Normal', 'Fraud'], \n",
    "                       patch_artist=True, notch=True, showfliers=False)  # Hide outliers for clarity\n",
    "\n",
    "colors = ['#2E8B57', '#DC143C']\n",
    "for patch, color in zip(box_plot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax4.set_ylabel('Amount ($)', fontweight='bold')\n",
    "ax4.set_title('Amount Box Plot (No Outliers)', fontweight='bold', pad=15)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Amount Range Analysis\n",
    "ax5 = plt.subplot(2, 4, 5)\n",
    "amount_ranges = [\n",
    "    (0, 10, '$0-10'),\n",
    "    (10, 50, '$10-50'),\n",
    "    (50, 100, '$50-100'),\n",
    "    (100, 500, '$100-500'),\n",
    "    (500, 1000, '$500-1K'),\n",
    "    (1000, 5000, '$1K-5K'),\n",
    "    (5000, float('inf'), '$5K+')\n",
    "]\n",
    "\n",
    "range_names = []\n",
    "normal_counts = []\n",
    "fraud_counts = []\n",
    "fraud_rates = []\n",
    "\n",
    "for min_amt, max_amt, label in amount_ranges:\n",
    "    if max_amt == float('inf'):\n",
    "        normal_in_range = np.sum(normal_amounts >= min_amt)\n",
    "        fraud_in_range = np.sum(fraud_amounts >= min_amt)\n",
    "    else:\n",
    "        normal_in_range = np.sum((normal_amounts >= min_amt) & (normal_amounts < max_amt))\n",
    "        fraud_in_range = np.sum((fraud_amounts >= min_amt) & (fraud_amounts < max_amt))\n",
    "    \n",
    "    total_in_range = normal_in_range + fraud_in_range\n",
    "    fraud_rate = (fraud_in_range / total_in_range * 100) if total_in_range > 0 else 0\n",
    "    \n",
    "    range_names.append(label)\n",
    "    normal_counts.append(normal_in_range)\n",
    "    fraud_counts.append(fraud_in_range)\n",
    "    fraud_rates.append(fraud_rate)\n",
    "\n",
    "x_pos = np.arange(len(range_names))\n",
    "bars1 = ax5.bar(x_pos - 0.2, normal_counts, 0.4, label='Normal', \n",
    "               color='#2E8B57', alpha=0.8)\n",
    "bars2 = ax5.bar(x_pos + 0.2, fraud_counts, 0.4, label='Fraud', \n",
    "               color='#DC143C', alpha=0.8)\n",
    "\n",
    "ax5.set_xlabel('Amount Range', fontweight='bold')\n",
    "ax5.set_ylabel('Number of Transactions', fontweight='bold')\n",
    "ax5.set_title('Transactions by Amount Range', fontweight='bold', pad=15)\n",
    "ax5.set_xticks(x_pos)\n",
    "ax5.set_xticklabels(range_names, rotation=45)\n",
    "ax5.legend()\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. Fraud Rate by Amount Range\n",
    "ax6 = plt.subplot(2, 4, 6)\n",
    "colors_fraud_rate = ['red' if rate > np.mean(fraud_rates) else 'orange' for rate in fraud_rates]\n",
    "bars3 = ax6.bar(range_names, fraud_rates, color=colors_fraud_rate, alpha=0.8, edgecolor='black')\n",
    "ax6.set_xlabel('Amount Range', fontweight='bold')\n",
    "ax6.set_ylabel('Fraud Rate (%)', fontweight='bold')\n",
    "ax6.set_title('Fraud Rate by Amount Range', fontweight='bold', pad=15)\n",
    "ax6.tick_params(axis='x', rotation=45)\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, rate in zip(bars3, fraud_rates):\n",
    "    height = bar.get_height()\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2., height + max(fraud_rates)*0.01,\n",
    "             f'{rate:.3f}%', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "# 7. Zero Amount Analysis\n",
    "ax7 = plt.subplot(2, 4, 7)\n",
    "zero_normal = np.sum(normal_amounts == 0)\n",
    "zero_fraud = np.sum(fraud_amounts == 0)\n",
    "nonzero_normal = np.sum(normal_amounts > 0)\n",
    "nonzero_fraud = np.sum(fraud_amounts > 0)\n",
    "\n",
    "categories = ['Zero Amount', 'Non-Zero Amount']\n",
    "normal_values = [zero_normal, nonzero_normal]\n",
    "fraud_values = [zero_fraud, nonzero_fraud]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "bars1 = ax7.bar(x - 0.2, normal_values, 0.4, label='Normal', color='#2E8B57', alpha=0.8)\n",
    "bars2 = ax7.bar(x + 0.2, fraud_values, 0.4, label='Fraud', color='#DC143C', alpha=0.8)\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax7.text(bar.get_x() + bar.get_width()/2., height + max(max(normal_values), max(fraud_values))*0.01,\n",
    "                f'{int(height):,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "ax7.set_xlabel('Transaction Type', fontweight='bold')\n",
    "ax7.set_ylabel('Count', fontweight='bold')\n",
    "ax7.set_title('Zero vs Non-Zero Amounts', fontweight='bold', pad=15)\n",
    "ax7.set_xticks(x)\n",
    "ax7.set_xticklabels(categories)\n",
    "ax7.legend()\n",
    "ax7.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 8. Percentile Analysis\n",
    "ax8 = plt.subplot(2, 4, 8)\n",
    "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "normal_percentiles = [np.percentile(normal_amounts, p) for p in percentiles]\n",
    "fraud_percentiles = [np.percentile(fraud_amounts, p) for p in percentiles]\n",
    "\n",
    "ax8.plot(percentiles, normal_percentiles, 'o-', label='Normal', color='#2E8B57', \n",
    "         linewidth=2, markersize=8)\n",
    "ax8.plot(percentiles, fraud_percentiles, 's-', label='Fraud', color='#DC143C', \n",
    "         linewidth=2, markersize=8)\n",
    "\n",
    "ax8.set_xlabel('Percentile', fontweight='bold')\n",
    "ax8.set_ylabel('Amount ($)', fontweight='bold')\n",
    "ax8.set_title('Amount Percentiles Comparison', fontweight='bold', pad=15)\n",
    "ax8.legend()\n",
    "ax8.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDETAILED AMOUNT ANALYSIS:\")\n",
    "\n",
    "zero_fraud_rate = zero_fraud / (zero_normal + zero_fraud) * 100 if (zero_normal + zero_fraud) > 0 else 0\n",
    "overall_fraud_rate = len(fraud_amounts) / len(amount_data) * 100\n",
    "\n",
    "print(f\"Zero Amount Transactions:\")\n",
    "print(f\" Normal: {zero_normal:>8,} | Fraud: {zero_fraud:>6,}\")\n",
    "print(f\" Zero amount fraud rate: {zero_fraud_rate:.4f}%\")\n",
    "print(f\" Compare to overall rate: {overall_fraud_rate:.4f}%\")\n",
    "\n",
    "print(f\"\\nAmount Range Analysis:\")\n",
    "for i, (range_name, normal_count, fraud_count, fraud_rate) in enumerate(zip(range_names, normal_counts, fraud_counts, fraud_rates)):\n",
    "    total = normal_count + fraud_count\n",
    "    multiplier = fraud_rate / overall_fraud_rate if overall_fraud_rate > 0 else 0\n",
    "    print(f\"   {range_name:>8}: {fraud_count:>4} frauds / {total:>7,} total = {fraud_rate:.4f}% ({multiplier:.1f}x)\")\n",
    "\n",
    "# High amount analysis\n",
    "high_amount_threshold = np.percentile(amount_data, 99)  # Top 1%\n",
    "high_normal = np.sum(normal_amounts >= high_amount_threshold)\n",
    "high_fraud = np.sum(fraud_amounts >= high_amount_threshold)\n",
    "high_fraud_rate = high_fraud / (high_normal + high_fraud) * 100 if (high_normal + high_fraud) > 0 else 0\n",
    "\n",
    "print(f\"\\nHigh Amount Analysis (‚â•${high_amount_threshold:.2f} - 99th percentile):\")\n",
    "print(f\" High amount transactions: {high_normal + high_fraud:,}\")\n",
    "print(f\" High amount frauds: {high_fraud}\")\n",
    "print(f\" High amount fraud rate: {high_fraud_rate:.4f}%\")\n",
    "\n",
    "# Low amount insights\n",
    "low_amount_threshold = np.percentile(amount_data, 25)  # Bottom 25%\n",
    "low_normal = np.sum(normal_amounts <= low_amount_threshold)\n",
    "low_fraud = np.sum(fraud_amounts <= low_amount_threshold)\n",
    "low_fraud_rate = low_fraud / (low_normal + low_fraud) * 100 if (low_normal + low_fraud) > 0 else 0\n",
    "\n",
    "print(f\"\\nLow Amount Analysis (‚â§${low_amount_threshold:.2f} - 25th percentile):\")\n",
    "print(f\" Low amount transactions: {low_normal + low_fraud:,}\")\n",
    "print(f\" Low amount frauds: {low_fraud}\")\n",
    "print(f\" Low amount fraud rate: {low_fraud_rate:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228ee869",
   "metadata": {},
   "source": [
    "### **PH√ÇN T√çCH CHUY√äN S√ÇU: H√ÄNH VI V·ªÄ S·ªê TI·ªÄN (AMOUNT INSIGHTS)**\n",
    "\n",
    "**C√°c ph√°t hi·ªán ch√≠nh v·ªÅ M√¥ h√¨nh d√≤ng ti·ªÅn:**\n",
    "* **Gi√° tr·ªã giao d·ªãch gian l·∫≠n TH·∫§P H∆†N giao d·ªãch th∆∞·ªùng:** Trung v·ªã (Median) c·ªßa m·ªôt giao d·ªãch l·ª´a ƒë·∫£o ch·ªâ l√† **9,25 USD**, th·∫•p h∆°n nhi·ªÅu so v·ªõi m·ª©c 22,00 USD c·ªßa ng∆∞·ªùi d√πng th·∫≠t.\n",
    "* **R·ªßi ro c·ª±c ƒë·∫°i ·ªü giao d·ªãch 0 ƒë·ªìng:** T·ª∑ l·ªá gian l·∫≠n t·∫°i c√°c giao d·ªãch c√≥ gi√° tr·ªã b·∫±ng 0 l√™n t·ªõi **1,48%**, trong khi t·ª∑ l·ªá trung b√¨nh to√†n t·∫≠p d·ªØ li·ªáu ch·ªâ l√† 0,17%. ƒêi·ªÅu n√†y ƒë·ªìng nghƒ©a **r·ªßi ro cao g·∫•p 8,6 l·∫ßn**. **L√Ω gi·∫£i:** ƒê√¢y r·∫•t c√≥ th·ªÉ l√† c√°c cu·ªôc t·∫•n c√¥ng thƒÉm d√≤ (**Probing attacks**). K·∫ª gian l·∫≠n th·ª±c hi·ªán c√°c giao d·ªãch 0 ƒë·ªìng ho·∫∑c gi√° tr·ªã c·ª±c nh·ªè ch·ªâ ƒë·ªÉ ki·ªÉm tra xem th·∫ª c√≤n ho·∫°t ƒë·ªông (live card) hay kh√¥ng tr∆∞·ªõc khi th·ª±c hi·ªán c√°c c√∫ l·ª´a ƒë·∫£o l·ªõn h∆°n ho·∫∑c b√°n th√¥ng tin th·∫ª ra ch·ª£ ƒëen.\n",
    "\n",
    "* **V√πng nguy hi·ªÉm nh·∫•t:** T·∫≠p trung ·ªü hai kho·∫£ng:\n",
    "    * Nh√≥m si√™u nh·ªè: **0 - 10 USD** (t·ª∑ l·ªá gian l·∫≠n 0,26%).\n",
    "    * Nh√≥m trung b√¨nh: **500 - 1.000 USD** (t·ª∑ l·ªá gian l·∫≠n 0,40%).\n",
    "* **M·ª©c tr·∫ßn th·∫•p:** S·ªë ti·ªÅn gian l·∫≠n l·ªõn nh·∫•t ghi nh·∫≠n ƒë∆∞·ª£c ch·ªâ l√† **2.125,87 USD**, trong khi giao d·ªãch th√¥ng th∆∞·ªùng l√™n t·ªõi 25.691,16 USD. T·ªôi ph·∫°m m·∫°ng ch·ªß ƒë·ªông tr√°nh c√°c giao d·ªãch gi√° tr·ªã qu√° l·ªõn.\n",
    "\n",
    "**Nh·ªØng s·ª± th·∫≠t b·∫•t ng·ªù (Surprising Insights):**\n",
    "1.  **Chi·∫øn thu·∫≠t \"X√© l·∫ª\":** K·∫ª gian l·∫≠n ∆∞u ti√™n c√°c kho·∫£n ti·ªÅn nh·ªè ƒë·ªÉ \"·∫©n m√¨nh\", tr√°nh k√≠ch ho·∫°t c√°c ng∆∞·ª°ng ki·ªÉm so√°t c·ªßa ng√¢n h√†ng.\n",
    "2.  **ƒê√≤n t·∫•n c√¥ng thƒÉm d√≤ (Probing Attacks):** C√°c giao d·ªãch 0 ƒë·ªìng ho·∫∑c gi√° tr·ªã c·ª±c nh·ªè th∆∞·ªùng l√† h√†nh ƒë·ªông ki·ªÉm tra th·∫ª (card testing) ƒë·ªÉ x√°c th·ª±c th·∫ª c√≤n ho·∫°t ƒë·ªông hay kh√¥ng.\n",
    "3.  **V·∫Øng b√≥ng c√°c v·ª• l·ª´a ƒë·∫£o \"kh·ªïng l·ªì\":** Kh√¥ng c√≥ giao d·ªãch gian l·∫≠n n√†o v∆∞·ª£t qu√° 2.126 USD. ƒêi·ªÅu n√†y cho th·∫•y h·ªá th·ªëng hi·ªán t·∫°i ch·∫∑n t·ªët c√°c giao d·ªãch l·ªõn, ho·∫∑c chi·∫øn thu·∫≠t c·ªßa t·ªôi ph·∫°m ƒë√£ thay ƒë·ªïi sang khai th√°c s·ªë l∆∞·ª£ng nhi·ªÅu thay v√¨ gi√° tr·ªã l·ªõn.\n",
    "4.  **R·ªßi ro ph√¢n c·ª±c (Bimodal Risk):** Bi·ªÉu ƒë·ªì r·ªßi ro c√≥ hai ƒë·ªânh r√µ r·ªát: M·ªôt ·ªü m·ª©c r·∫•t th·∫•p (0-10 USD) v√† m·ªôt ·ªü m·ª©c trung b√¨nh (500-1.000 USD).\n",
    "\n",
    "**So s√°nh th·ªëng k√™ (Statistical Comparison):**\n",
    "* **ƒê·ªô bi·∫øn ƒë·ªông cao:** D√π gi√° tr·ªã t·ªëi ƒëa th·∫•p h∆°n, nh∆∞ng ƒë·ªô l·ªách chu·∫©n (Standard Deviation) c·ªßa c√°c giao d·ªãch gian l·∫≠n l·∫°i cao h∆°n (256 USD so v·ªõi 250 USD).\n",
    "* **Ph√¢n ph·ªëi l·ªách:** Gian l·∫≠n c√≥ trung v·ªã (Median) th·∫•p h∆°n nh∆∞ng gi√° tr·ªã trung b√¨nh (Mean) l·∫°i cao h∆°n, cho th·∫•y bi·ªÉu ƒë·ªì ph√¢n ph·ªëi b·ªã l·ªách ph·∫£i (right-skewed).\n",
    "* **C·∫•u tr√∫c ph√¢n v·ªã kh√°c bi·ªát:** Ph√¢n ph·ªëi c·ªßa t·∫≠p d·ªØ li·ªáu gian l·∫≠n d·ªãch chuy·ªÉn m·∫°nh v·ªÅ ph√≠a b√™n tr√°i (gi√° tr·ªã nh·ªè) so v·ªõi t·∫≠p d·ªØ li·ªáu th∆∞·ªùng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f540c24",
   "metadata": {},
   "source": [
    "## Time vs Amount Correlation & Additional Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f159573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time vs Amount + Additional Insights\n",
    "print(\"DEEP ANALYST\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get indices for features\n",
    "time_idx = column_names.index('Time')\n",
    "amount_idx = column_names.index('Amount')\n",
    "class_idx = column_names.index('Class')\n",
    "\n",
    "# Extract data\n",
    "time_data = data[:, time_idx]\n",
    "amount_data = data[:, amount_idx]\n",
    "class_data = data[:, class_idx]\n",
    "\n",
    "# Convert time to hours\n",
    "hours = (time_data // 3600) % 24\n",
    "\n",
    "# Separate by class\n",
    "normal_mask = class_data == 0\n",
    "fraud_mask = class_data == 1\n",
    "\n",
    "normal_hours = hours[normal_mask]\n",
    "fraud_hours = hours[fraud_mask]\n",
    "normal_amounts = amount_data[normal_mask]\n",
    "fraud_amounts = amount_data[fraud_mask]\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(16, 18))\n",
    "\n",
    "# 1. Time vs Amount Scatter Plot (Fraud highlighted)\n",
    "ax1 = plt.subplot(3, 2, 1)\n",
    "ax1.scatter(normal_hours, normal_amounts, \n",
    "           alpha=0.1, s=0.5, color='#2E8B57', label=f'Normal ({len(normal_hours):,})')\n",
    "ax1.scatter(fraud_hours, fraud_amounts, \n",
    "           alpha=0.8, s=15, color='#DC143C', label=f'Fraud ({len(fraud_hours):,})', \n",
    "           edgecolors='darkred', linewidth=0.3)\n",
    "\n",
    "ax1.set_xlabel('Hour of Day', fontweight='bold')\n",
    "ax1.set_ylabel('Amount ($)', fontweight='bold')\n",
    "ax1.set_title('Time vs Amount Pattern', fontweight='bold', pad=15)\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.set_ylim(0, 2000) \n",
    "\n",
    "# 2. Average Amount by Hour\n",
    "ax2 = plt.subplot(3, 2, 2)\n",
    "hourly_avg_normal = []\n",
    "hourly_avg_fraud = []\n",
    "hourly_std_normal = []\n",
    "hourly_std_fraud = []\n",
    "\n",
    "for hour in range(24):\n",
    "    hour_normal = normal_amounts[normal_hours == hour]\n",
    "    hour_fraud = fraud_amounts[fraud_hours == hour]\n",
    "    \n",
    "    hourly_avg_normal.append(np.mean(hour_normal) if len(hour_normal) > 0 else 0)\n",
    "    hourly_avg_fraud.append(np.mean(hour_fraud) if len(hour_fraud) > 0 else 0)\n",
    "    hourly_std_normal.append(np.std(hour_normal) if len(hour_normal) > 0 else 0)\n",
    "    hourly_std_fraud.append(np.std(hour_fraud) if len(hour_fraud) > 0 else 0)\n",
    "\n",
    "hours_range = np.arange(24)\n",
    "ax2.plot(hours_range, hourly_avg_normal, 'o-', color='#2E8B57', linewidth=2, \n",
    "         markersize=6, label='Normal Avg')\n",
    "ax2.plot(hours_range, hourly_avg_fraud, 's-', color='#DC143C', linewidth=2, \n",
    "         markersize=6, label='Fraud Avg')\n",
    "\n",
    "# Fill between for std deviation\n",
    "ax2.fill_between(hours_range, \n",
    "                 np.array(hourly_avg_normal) - np.array(hourly_std_normal),\n",
    "                 np.array(hourly_avg_normal) + np.array(hourly_std_normal),\n",
    "                 alpha=0.2, color='#2E8B57')\n",
    "\n",
    "ax2.set_xlabel('Hour of Day', fontweight='bold')\n",
    "ax2.set_ylabel('Average Amount ($)', fontweight='bold')\n",
    "ax2.set_title('Average Amount by Hour', fontweight='bold', pad=15)\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# 3. Fraud Count and Amount by Hour\n",
    "ax3 = plt.subplot(3, 2, 3)\n",
    "hourly_fraud_count = [np.sum(fraud_hours == hour) for hour in range(24)]\n",
    "hourly_fraud_amount = [np.sum(fraud_amounts[fraud_hours == hour]) for hour in range(24)]\n",
    "\n",
    "ax3_twin = ax3.twinx()\n",
    "\n",
    "bars1 = ax3.bar(hours_range, hourly_fraud_count, alpha=0.7, color='#DC143C', \n",
    "               label='Fraud Count')\n",
    "line1 = ax3_twin.plot(hours_range, hourly_fraud_amount, 'o-', color='orange', \n",
    "                     linewidth=3, markersize=8, label='Total Fraud Amount')\n",
    "\n",
    "ax3.set_xlabel('Hour of Day', fontweight='bold')\n",
    "ax3.set_ylabel('Fraud Count', fontweight='bold', color='#DC143C')\n",
    "ax3_twin.set_ylabel('Total Fraud Amount ($)', fontweight='bold', color='orange')\n",
    "ax3.set_title('Fraud Count & Amount by Hour', fontweight='bold', pad=15)\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# Combine legends\n",
    "lines1, labels1 = ax3.get_legend_handles_labels()\n",
    "lines2, labels2 = ax3_twin.get_legend_handles_labels()\n",
    "ax3.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "# 4. V Features Fraud Analysis (Sample top V features)\n",
    "ax4 = plt.subplot(3, 2, 4)\n",
    "v_features = ['V1', 'V2', 'V3', 'V4', 'V5']\n",
    "v_fraud_means = []\n",
    "v_normal_means = []\n",
    "v_effect_sizes = []\n",
    "\n",
    "for v_feature in v_features:\n",
    "    v_idx = column_names.index(v_feature)\n",
    "    v_data = data[:, v_idx]\n",
    "    \n",
    "    normal_v = v_data[normal_mask]\n",
    "    fraud_v = v_data[fraud_mask]\n",
    "    \n",
    "    normal_mean = np.mean(normal_v)\n",
    "    fraud_mean = np.mean(fraud_v)\n",
    "    \n",
    "    # Effect size (Cohen's d approximation)\n",
    "    pooled_std = np.sqrt(((len(normal_v) - 1) * np.var(normal_v) + \n",
    "                         (len(fraud_v) - 1) * np.var(fraud_v)) / \n",
    "                        (len(normal_v) + len(fraud_v) - 2))\n",
    "    effect_size = abs(normal_mean - fraud_mean) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    v_normal_means.append(normal_mean)\n",
    "    v_fraud_means.append(fraud_mean)\n",
    "    v_effect_sizes.append(effect_size)\n",
    "\n",
    "x_pos = np.arange(len(v_features))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax4.bar(x_pos - width/2, v_normal_means, width, label='Normal', \n",
    "               color='#2E8B57', alpha=0.8)\n",
    "bars2 = ax4.bar(x_pos + width/2, v_fraud_means, width, label='Fraud', \n",
    "               color='#DC143C', alpha=0.8)\n",
    "\n",
    "ax4.set_xlabel('V Features', fontweight='bold')\n",
    "ax4.set_ylabel('Mean Value', fontweight='bold')\n",
    "ax4.set_title('V Features: Normal vs Fraud Means', fontweight='bold', pad=15)\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(v_features)\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Transaction Volume Analysis over Time\n",
    "ax5 = plt.subplot(3, 2, 5)\n",
    "# Create time bins (e.g., every 1000 seconds)\n",
    "time_bins = np.arange(0, time_data.max() + 1000, 1000)\n",
    "normal_time_hist, _ = np.histogram(time_data[normal_mask], bins=time_bins)\n",
    "fraud_time_hist, _ = np.histogram(time_data[fraud_mask], bins=time_bins)\n",
    "\n",
    "time_centers = (time_bins[:-1] + time_bins[1:]) / 2 / 3600  # Convert to hours\n",
    "\n",
    "ax5.plot(time_centers, normal_time_hist, color='#2E8B57', linewidth=2, \n",
    "         label='Normal', alpha=0.8)\n",
    "ax5.plot(time_centers, fraud_time_hist, color='#DC143C', linewidth=3, \n",
    "         label='Fraud', alpha=0.9)\n",
    "\n",
    "ax5.set_xlabel('Time (Hours from Start)', fontweight='bold')\n",
    "ax5.set_ylabel('Transaction Count', fontweight='bold')\n",
    "ax5.set_title('Transaction Volume Over Time', fontweight='bold', pad=15)\n",
    "ax5.legend()\n",
    "ax5.grid(alpha=0.3)\n",
    "\n",
    "# 6. Amount Distribution by Time Periods\n",
    "ax6 = plt.subplot(3, 2, 6)\n",
    "time_periods = [\n",
    "    ('Late Night\\n(0-6h)', 0, 6),\n",
    "    ('Morning\\n(6-12h)', 6, 12),\n",
    "    ('Afternoon\\n(12-18h)', 12, 18),\n",
    "    ('Evening\\n(18-24h)', 18, 24)\n",
    "]\n",
    "\n",
    "period_names = [period[0] for period in time_periods]\n",
    "normal_avg_amounts = []\n",
    "fraud_avg_amounts = []\n",
    "\n",
    "for period_name, start_h, end_h in time_periods:\n",
    "    period_mask = (hours >= start_h) & (hours < end_h)\n",
    "    \n",
    "    normal_period_amounts = amount_data[normal_mask & period_mask]\n",
    "    fraud_period_amounts = amount_data[fraud_mask & period_mask]\n",
    "    \n",
    "    normal_avg_amounts.append(np.mean(normal_period_amounts) if len(normal_period_amounts) > 0 else 0)\n",
    "    fraud_avg_amounts.append(np.mean(fraud_period_amounts) if len(fraud_period_amounts) > 0 else 0)\n",
    "\n",
    "x = np.arange(len(period_names))\n",
    "bars1 = ax6.bar(x - 0.2, normal_avg_amounts, 0.4, label='Normal', \n",
    "               color='#2E8B57', alpha=0.8)\n",
    "bars2 = ax6.bar(x + 0.2, fraud_avg_amounts, 0.4, label='Fraud', \n",
    "               color='#DC143C', alpha=0.8)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                f'${height:.0f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "ax6.set_xlabel('Time Period', fontweight='bold')\n",
    "ax6.set_ylabel('Average Amount ($)', fontweight='bold')\n",
    "ax6.set_title('Average Amount by Time Period', fontweight='bold', pad=15)\n",
    "ax6.set_xticks(x)\n",
    "ax6.set_xticklabels(period_names)\n",
    "ax6.legend()\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Advanced Statistical Analysis\n",
    "print(f\"\\nSTATISTICAL INSIGHTS:\")\n",
    "\n",
    "# Time-Amount correlation\n",
    "normal_corr = np.corrcoef(normal_hours, normal_amounts)[0, 1]\n",
    "fraud_corr = np.corrcoef(fraud_hours, fraud_amounts)[0, 1]\n",
    "\n",
    "print(f\"Time-Amount Correlation:\")\n",
    "print(f\" Normal transactions: {normal_corr:>8.4f}\")\n",
    "print(f\" Fraud transactions:  {fraud_corr:>8.4f}\")\n",
    "print(f\" Difference: {abs(fraud_corr - normal_corr):>8.4f}\")\n",
    "\n",
    "# Peak fraud hour analysis with amount\n",
    "peak_hours = [2, 4]  \n",
    "print(f\"\\nüîç Peak Fraud Hours Analysis:\")\n",
    "for hour in peak_hours:\n",
    "    hour_normal_amt = normal_amounts[normal_hours == hour]\n",
    "    hour_fraud_amt = fraud_amounts[fraud_hours == hour]\n",
    "    \n",
    "    print(f\" Hour {hour}:\")\n",
    "    print(f\" Normal avg amount: ${np.mean(hour_normal_amt):>8.2f} (n={len(hour_normal_amt)})\")\n",
    "    print(f\" Fraud avg amount:  ${np.mean(hour_fraud_amt):>8.2f} (n={len(hour_fraud_amt)})\")\n",
    "\n",
    "# V Features effect sizes\n",
    "print(f\"\\n V Features Effect Sizes (Normal vs Fraud):\")\n",
    "for i, (feature, effect_size) in enumerate(zip(v_features, v_effect_sizes)):\n",
    "    if effect_size > 0.5:\n",
    "        magnitude = \"LARGE\"\n",
    "    elif effect_size > 0.3:\n",
    "        magnitude = \"MEDIUM\"\n",
    "    elif effect_size > 0.1:\n",
    "        magnitude = \"SMALL\"\n",
    "    else:\n",
    "        magnitude = \"NEGLIGIBLE\"\n",
    "    \n",
    "    print(f\" {feature}: {effect_size:>6.3f} ({magnitude})\")\n",
    "\n",
    "# Transaction velocity analysis\n",
    "print(f\"\\nTransaction Velocity Analysis:\")\n",
    "time_diffs_normal = np.diff(np.sort(time_data[normal_mask]))\n",
    "time_diffs_fraud = np.diff(np.sort(time_data[fraud_mask]))\n",
    "\n",
    "print(f\" Normal avg gap:  {np.mean(time_diffs_normal):>8.2f} seconds\")\n",
    "print(f\" Fraud avg gap:   {np.mean(time_diffs_fraud):>8.2f} seconds\")\n",
    "print(f\" Normal median:   {np.median(time_diffs_normal):>8.2f} seconds\")\n",
    "print(f\" Fraud median:    {np.median(time_diffs_fraud):>8.2f} seconds\")\n",
    "\n",
    "# High-risk combinations\n",
    "print(f\"\\nHIGH-RISK PATTERN COMBINATIONS:\")\n",
    "\n",
    "# Late night + small amount\n",
    "late_night_mask = (hours >= 0) & (hours < 6)\n",
    "small_amount_mask = amount_data <= 10\n",
    "\n",
    "combo1_normal = np.sum(normal_mask & late_night_mask & small_amount_mask)\n",
    "combo1_fraud = np.sum(fraud_mask & late_night_mask & small_amount_mask)\n",
    "combo1_total = combo1_normal + combo1_fraud\n",
    "combo1_fraud_rate = combo1_fraud / combo1_total * 100 if combo1_total > 0 else 0\n",
    "\n",
    "print(f\" Late Night (0-6h) + Small Amount (‚â§$10):\")\n",
    "print(f\"  Fraud rate: {combo1_fraud_rate:.4f}% ({combo1_fraud}/{combo1_total})\")\n",
    "print(f\"  Risk multiplier: {combo1_fraud_rate / (len(fraud_amounts) / len(amount_data) * 100):.1f}x\")\n",
    "\n",
    "# Zero amount + any time\n",
    "zero_amount_mask = amount_data == 0\n",
    "combo2_normal = np.sum(normal_mask & zero_amount_mask)\n",
    "combo2_fraud = np.sum(fraud_mask & zero_amount_mask)\n",
    "combo2_total = combo2_normal + combo2_fraud\n",
    "combo2_fraud_rate = combo2_fraud / combo2_total * 100 if combo2_total > 0 else 0\n",
    "\n",
    "print(f\"\\nZero Amount Transactions:\")\n",
    "print(f\" Fraud rate: {combo2_fraud_rate:.4f}% ({combo2_fraud}/{combo2_total})\")\n",
    "print(f\" Risk multiplier: {combo2_fraud_rate / (len(fraud_amounts) / len(amount_data) * 100):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee9992",
   "metadata": {},
   "source": [
    "### **T·ªîNG K·∫æT & NH·ªÆNG ƒêI·ªÇM C·ªêT L√ïI (KEY TAKEAWAYS)**\n",
    "\n",
    "**1. V·∫•n ƒë·ªÅ m·∫•t c√¢n b·∫±ng d·ªØ li·ªáu (Class Imbalance)**\n",
    "  * **T·ª∑ l·ªá 578:1:** D·ªØ li·ªáu b·ªã l·ªách c·ª±c k·ª≥ nghi√™m tr·ªçng. C·ª© 578 giao d·ªãch b√¨nh th∆∞·ªùng m·ªõi c√≥ 1 giao d·ªãch gian l·∫≠n.\n",
    "* **T·ª∑ l·ªá th·ª±c t·∫ø:** C√°c v·ª• gian l·∫≠n ch·ªâ chi·∫øm **0,173%** t·ªïng s·ªë giao d·ªãch. Con s·ªë n√†y ph·∫£n √°nh ƒë√∫ng th·ª±c t·∫ø kh·∫Øc nghi·ªát c·ªßa d·ªØ li·ªáu t√†i ch√≠nh.\n",
    "* **T√°c ƒë·ªông:** N·∫øu t·ª∑ l·ªá n√†y duy tr√¨ ·ªïn ƒë·ªãnh, m·ª©c thi·ªát h·∫°i ti·ªÅm nƒÉng ∆∞·ªõc t√≠nh l√™n t·ªõi **24.600 USD/ng√†y**.\n",
    "\n",
    "**2. Quy lu·∫≠t th·ªùi gian c·ªßa t·ªôi ph·∫°m**\n",
    "* **Khung gi·ªù \"v√†ng\":** R·ªßi ro cao nh·∫•t n·∫±m trong kho·∫£ng **2:00 - 3:00 s√°ng** (nguy c∆° cao g·∫•p 9,9 l·∫ßn b√¨nh th∆∞·ªùng) v√† **4:00 - 5:00 s√°ng** (g·∫•p 6 l·∫ßn).\n",
    "* **Hi·ªáu ·ª©ng ƒë√™m khuya:** Kho·∫£ng th·ªùi gian t·ª´ 0:00 ƒë·∫øn 6:00 s√°ng c√≥ t·ª∑ l·ªá gian l·∫≠n l√† 0,518%, cao h∆°n nhi·ªÅu so v·ªõi bu·ªïi t·ªëi (ch·ªâ 0,124%).\n",
    "* **H√†nh vi:** C√°c giao d·ªãch gian l·∫≠n c√≥ xu h∆∞·ªõng x·∫£y ra s·ªõm h∆°n trong ng√†y (trung b√¨nh v√†o l√∫c 11h30) so v·ªõi giao d·ªãch th√¥ng th∆∞·ªùng (trung b√¨nh v√†o l√∫c 14h).\n",
    "\n",
    "**3. H√†nh vi chi ti√™u: Chi·∫øn thu·∫≠t \"Nh·ªè gi·ªçt\"**\n",
    "* **Th√¥ng minh & Nh·ªè l·∫ª:** K·∫ª gian l·∫≠n th∆∞·ªùng ch·ªçn c√°c kho·∫£n ti·ªÅn nh·ªè ƒë·ªÉ tr√°nh b·ªã ph√°t hi·ªán (trung v·ªã l√† 9,25 USD so v·ªõi 22,00 USD c·ªßa ng∆∞·ªùi th∆∞·ªùng).\n",
    "* **B√°o ƒë·ªông ƒë·ªè v·ªõi giao d·ªãch 0 ƒë·ªìng:** T·ª∑ l·ªá gian l·∫≠n ·ªü c√°c giao d·ªãch 0 USD l√™n t·ªõi 1,48%, trong khi t·ª∑ l·ªá chung ch·ªâ l√† 0,17% (t·ª©c r·ªßi ro g·∫•p 8,6 l·∫ßn).\n",
    "* **Kh√¥ng c√≥ c√∫ l·ª´a \"kh·ªïng l·ªì\":** Giao d·ªãch gian l·∫≠n l·ªõn nh·∫•t ch·ªâ l√† 2.126 USD, th·∫•p h∆°n nhi·ªÅu so v·ªõi m·ª©c t·ªëi ƒëa c·ªßa giao d·ªãch th∆∞·ªùng l√† 25.691 USD.\n",
    "* **V√πng nguy hi·ªÉm:** T·∫≠p trung nhi·ªÅu nh·∫•t ·ªü c√°c kho·∫£n ti·ªÅn c·ª±c nh·ªè (0-10 USD) ho·∫∑c t·∫ßm trung (500-1.000 USD).\n",
    "\n",
    "**4. Ph√¢n t√≠ch m√¥ h√¨nh n√¢ng cao**\n",
    "* **C√°c ƒë·∫∑c tr∆∞ng V:** C√°c bi·∫øn ·∫©n danh (V features) c√≥ m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng r·∫•t l·ªõn (effect sizes t·ª´ 2.2 ƒë·∫øn 4.7). ƒê√¢y l√† nh·ªØng d·∫•u hi·ªáu ph√¢n lo·∫°i c·ª±c t·ªët sau khi ƒë√£ x·ª≠ l√Ω qua thu·∫≠t to√°n PCA.\n",
    "* **T∆∞∆°ng quan Th·ªùi gian - Ti·ªÅn:** C√≥ s·ª± kh√°c bi·ªát r√µ r·ªát v·ªÅ h√†nh vi. Gian l·∫≠n c√≥ m·ªëi t∆∞∆°ng quan d∆∞∆°ng nh·∫π, trong khi giao d·ªãch th∆∞·ªùng g·∫ßn nh∆∞ kh√¥ng c√≥ t∆∞∆°ng quan.\n",
    "* **T·ªëc ƒë·ªô giao d·ªãch:** K·∫ª gian l·∫≠n c√≥ kho·∫£ng ngh·ªâ gi·ªØa c√°c l·∫ßn th·ª±c hi·ªán d√†i h∆°n (trung v·ªã 346 gi√¢y) so v·ªõi ng∆∞·ªùi d√πng th·∫≠t (0,6 gi√¢y).\n",
    "\n",
    "---\n",
    "\n",
    "**C√ÅC T·ªî H·ª¢P R·ª¶I RO CAO (HIGH-RISK COMBINATIONS):**\n",
    "1. **ƒê√™m khuya + S·ªë ti·ªÅn nh·ªè:** H·ªá s·ªë r·ªßi ro g·∫•p **5,1 l·∫ßn**.\n",
    "2.  **Gi√° tr·ªã c·ª±c ƒëoan c·ªßa bi·∫øn V:** Khi c√°c bi·∫øn V c√≥ ch·ªâ s·ªë bi·∫øn ƒë·ªông m·∫°nh, ƒë√¢y l√† t√≠n hi·ªáu c·∫£nh b√°o r·∫•t r√µ r√†ng.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64174cf1",
   "metadata": {},
   "source": [
    "# MODEL IMPLEMENTATION\n",
    "\n",
    "1. Data Preprocessing (Log transformation, Train/Test split, Standardization, Undersampling)\n",
    "2. Logistic Regression Model (Pure NumPy implementation) \n",
    "3. Model Evaluation (Confusion Matrix, Precision, Recall, F1-Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3797f408",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "\n",
    "### Step 1: Prepare data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f2eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Chu·∫©n b·ªã d·ªØ li·ªáu cho modeling\n",
    "print(\"DATA PREPROCESSING FOR MODELING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Extract features v√† target\n",
    "# Features: Time, V1-V28, Amount (30 features total)\n",
    "# Target: Class (0=Normal, 1=Fraud)\n",
    "X = data[:, :-1]  # T·∫•t c·∫£ columns tr·ª´ column cu·ªëi (Class)\n",
    "y = data[:, -1]   # Column cu·ªëi (Class)\n",
    "# Ki·ªÉm tra distribution c·ªßa Amount tr∆∞·ªõc khi transform\n",
    "amount_idx = column_names.index('Amount')\n",
    "amount_original = X[:, amount_idx]\n",
    "\n",
    "print(f\"\\nOriginal Amount statistics:\")\n",
    "print(f\"  Min: ${np.min(amount_original):.2f}\")\n",
    "print(f\"  Max: ${np.max(amount_original):,.2f}\")\n",
    "print(f\"  Mean: ${np.mean(amount_original):.2f}\")\n",
    "print(f\"  Std: ${np.std(amount_original):.2f}\")\n",
    "print(f\"  Skewness: {np.mean(((amount_original - np.mean(amount_original)) / np.std(amount_original))**3):.3f}\")\n",
    "\n",
    "# Log transformation cho Amount column ƒë·ªÉ x·ª≠ l√Ω skewness\n",
    "# Th√™m 1 ƒë·ªÉ tr√°nh log(0) cho zero amounts\n",
    "X_processed = X.copy()\n",
    "X_processed[:, amount_idx] = np.log1p(amount_original)  # log1p = log(1 + x)\n",
    "\n",
    "print(f\"\\nAfter log transformation:\")\n",
    "amount_transformed = X_processed[:, amount_idx]\n",
    "print(f\"  Min: {np.min(amount_transformed):.2f}\")\n",
    "print(f\"  Max: {np.max(amount_transformed):.2f}\")\n",
    "print(f\"  Mean: {np.mean(amount_transformed):.2f}\")\n",
    "print(f\"  Std: {np.std(amount_transformed):.2f}\")\n",
    "print(f\"  Skewness: {np.mean(((amount_transformed - np.mean(amount_transformed)) / np.std(amount_transformed))**3):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56bd73d",
   "metadata": {},
   "source": [
    "### Step 2: Chia t·∫≠p Train/Test (80/20)\n",
    "- Th·ª±c hi·ªán shuffle indices th·ªß c√¥ng ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh ng·∫´u nhi√™n\n",
    "- Chia d·ªØ li·ªáu theo t·ª∑ l·ªá 80% train, 20% test\n",
    "- ƒê·∫£m b·∫£o t·ª∑ l·ªá class ƒë∆∞·ª£c preserve trong c·∫£ 2 t·∫≠p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e02cb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train/Test Split (80/20) with stratification\n",
    "print(\"TRAIN/TEST SPLIT\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Stratified split ƒë·ªÉ ƒë·∫£m b·∫£o t·ª∑ l·ªá class ƒë∆∞·ª£c preserve\n",
    "def stratified_train_test_split(X, y, test_size=0.2, random_seed=42):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: Features array\n",
    "        y: Target array  \n",
    "        test_size: T·ª∑ l·ªá test set\n",
    "        random_seed: Seed cho t√≠nh ng·∫´u nhi√™n\n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # T√¨m indices cho t·ª´ng class\n",
    "    normal_indices = np.where(y == 0)[0]\n",
    "    fraud_indices = np.where(y == 1)[0]\n",
    "    \n",
    "    # Shuffle indices cho t·ª´ng class\n",
    "    np.random.shuffle(normal_indices)\n",
    "    np.random.shuffle(fraud_indices)\n",
    "    \n",
    "    # T√≠nh s·ªë l∆∞·ª£ng test samples cho t·ª´ng class\n",
    "    n_normal_test = int(len(normal_indices) * test_size)\n",
    "    n_fraud_test = int(len(fraud_indices) * test_size)\n",
    "    \n",
    "    # Chia indices\n",
    "    normal_test_idx = normal_indices[:n_normal_test]\n",
    "    normal_train_idx = normal_indices[n_normal_test:]\n",
    "    \n",
    "    fraud_test_idx = fraud_indices[:n_fraud_test]\n",
    "    fraud_train_idx = fraud_indices[n_fraud_test:]\n",
    "    \n",
    "    # Combine indices\n",
    "    train_indices = np.concatenate([normal_train_idx, fraud_train_idx])\n",
    "    test_indices = np.concatenate([normal_test_idx, fraud_test_idx])\n",
    "    \n",
    "    # Shuffle train indices ƒë·ªÉ tr·ªôn normal v√† fraud\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "    \n",
    "    return (X[train_indices], X[test_indices], \n",
    "            y[train_indices], y[test_indices])\n",
    "\n",
    "# Th·ª±c hi·ªán split\n",
    "X_train, X_test, y_train, y_test = stratified_train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_seed=42\n",
    ")\n",
    "\n",
    "# Ki·ªÉm tra k·∫øt qu·∫£ split\n",
    "print(f\"Train set:\")\n",
    "print(f\"  Shape: {X_train.shape}\")\n",
    "print(f\"  Normal: {np.sum(y_train == 0):,} ({np.mean(y_train == 0)*100:.2f}%)\")\n",
    "print(f\"  Fraud:  {np.sum(y_train == 1):,} ({np.mean(y_train == 1)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Shape: {X_test.shape}\")\n",
    "print(f\"  Normal: {np.sum(y_test == 0):,} ({np.mean(y_test == 0)*100:.2f}%)\")\n",
    "print(f\"  Fraud:  {np.sum(y_test == 1):,} ({np.mean(y_test == 1)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nSplit ratio: {len(X_train)/(len(X_train)+len(X_test))*100:.1f}% train, {len(X_test)/(len(X_train)+len(X_test))*100:.1f}% test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f1fb3",
   "metadata": {},
   "source": [
    "### Step 3: Chu·∫©n h√≥a d·ªØ li·ªáu (Standardization)\n",
    "- T√≠nh mean v√† std t·ª´ t·∫≠p train ƒë·ªÉ tr√°nh data leakage\n",
    "- √Åp d·ª•ng Z-score normalization: (x - mean) / std\n",
    "- Transform c·∫£ train v√† test set v·ªõi c√πng parameters t·ª´ train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7066b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Standardization (Z-score normalization)\n",
    "print(\"STANDARDIZATION\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "class StandardScaler:\n",
    "    \"\"\"\n",
    "    Chu·∫©n h√≥a d·ªØ li·ªáu b·∫±ng Z-score: (x - mean) / std\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        self.fitted = False\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        H·ªçc mean v√† std t·ª´ training data\n",
    "        Args:\n",
    "            X: Training data array shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        self.mean = np.mean(X, axis=0)  # Mean cho t·ª´ng feature\n",
    "        self.std = np.std(X, axis=0)    # Std cho t·ª´ng feature\n",
    "        \n",
    "        # Tr√°nh chia cho 0 n·∫øu std = 0\n",
    "        self.std = np.where(self.std == 0, 1, self.std)\n",
    "        self.fitted = True\n",
    "        \n",
    "        print(f\"Fitted scaler:\")\n",
    "        print(f\"  Features: {len(self.mean)}\")\n",
    "        print(f\"  Mean range: [{np.min(self.mean):.4f}, {np.max(self.mean):.4f}]\")\n",
    "        print(f\"  Std range: [{np.min(self.std):.4f}, {np.max(self.std):.4f}]\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        √Åp d·ª•ng standardization l√™n data\n",
    "        Args:\n",
    "            X: Data c·∫ßn transform shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            X_scaled: Standardized data\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Scaler ch∆∞a ƒë∆∞·ª£c fit! H√£y g·ªçi fit() tr∆∞·ªõc.\")\n",
    "        \n",
    "        # Z-score transformation: (X - mean) / std\n",
    "        X_scaled = (X - self.mean) / self.std\n",
    "        return X_scaled\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "# Kh·ªüi t·∫°o v√† fit scaler tr√™n train data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform test data b·∫±ng parameters t·ª´ train\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Ki·ªÉm tra k·∫øt qu·∫£ standardization\n",
    "print(f\"\\nStandardization results:\")\n",
    "print(f\"Train scaled - Mean: {np.mean(X_train_scaled, axis=0)[:5]} (first 5 features)\")\n",
    "print(f\"Train scaled - Std:  {np.std(X_train_scaled, axis=0)[:5]} (first 5 features)\")\n",
    "print(f\"Test scaled - Mean:  {np.mean(X_test_scaled, axis=0)[:5]} (first 5 features)\")\n",
    "print(f\"Test scaled - Std:   {np.std(X_test_scaled, axis=0)[:5]} (first 5 features)\")\n",
    "\n",
    "print(f\"\\nData sau standardization:\")\n",
    "print(f\"  Train shape: {X_train_scaled.shape}\")\n",
    "print(f\"  Test shape: {X_test_scaled.shape}\")\n",
    "print(f\"  Train range: [{np.min(X_train_scaled):.3f}, {np.max(X_train_scaled):.3f}]\")\n",
    "print(f\"  Test range: [{np.min(X_test_scaled):.3f}, {np.max(X_test_scaled):.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d6711",
   "metadata": {},
   "source": [
    "### Step 4: Feature Engineering - Polynomial Features\n",
    "- T·∫°o interaction features ƒë·ªÉ capture non-linear relationships\n",
    "- T·∫≠p trung v√†o V features quan tr·ªçng v√† Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f27c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Engineering - Polynomial Features (Pure NumPy)\n",
    "print(\"FEATURE ENGINEERING - POLYNOMIAL FEATURES\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def create_polynomial_features(X, feature_indices, degree=2):\n",
    "    \"\"\"\n",
    "    T·∫°o polynomial v√† interaction features cho selected features\n",
    "    Args:\n",
    "        X: Original features array \n",
    "        feature_indices: Indices of features to create polynomials\n",
    "        degree: Maximum degree (default=2)\n",
    "    Returns:\n",
    "        X_poly: Array v·ªõi original + polynomial features\n",
    "    \"\"\"\n",
    "    X_poly = X.copy()\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Polynomial features (degree 2)\n",
    "    for i in feature_indices:\n",
    "        feature_squared = X[:, i] ** 2\n",
    "        X_poly = np.column_stack([X_poly, feature_squared])\n",
    "    \n",
    "    # Interaction features (pairwise products)\n",
    "    for i in range(len(feature_indices)):\n",
    "        for j in range(i + 1, len(feature_indices)):\n",
    "            idx1, idx2 = feature_indices[i], feature_indices[j]\n",
    "            interaction = X[:, idx1] * X[:, idx2]\n",
    "            X_poly = np.column_stack([X_poly, interaction])\n",
    "    \n",
    "    return X_poly\n",
    "\n",
    "# Ch·ªçn features quan tr·ªçng ƒë·ªÉ t·∫°o polynomials\n",
    "# Time, Amount, v√† top V features (V14, V4, V11, V12, V10)\n",
    "important_feature_indices = [\n",
    "    column_names.index('Time'),\n",
    "    column_names.index('Amount'), \n",
    "    column_names.index('V14'),\n",
    "    column_names.index('V4'),\n",
    "    column_names.index('V11'),\n",
    "    column_names.index('V12'),\n",
    "    column_names.index('V10')\n",
    "]\n",
    "\n",
    "print(f\"Creating polynomial features for:\")\n",
    "for idx in important_feature_indices:\n",
    "    print(f\"  - {column_names[idx]} (index {idx})\")\n",
    "\n",
    "# T·∫°o polynomial features cho processed data\n",
    "X_poly = create_polynomial_features(X_processed, important_feature_indices, degree=2)\n",
    "\n",
    "print(f\"\\nFeature engineering results:\")\n",
    "print(f\"  Original features: {X_processed.shape[1]}\")\n",
    "print(f\"  Polynomial features added: {X_poly.shape[1] - X_processed.shape[1]}\")\n",
    "print(f\"  Total features: {X_poly.shape[1]}\")\n",
    "print(f\"  Data shape: {X_poly.shape}\")\n",
    "\n",
    "# Train/Test split v·ªõi polynomial features\n",
    "print(f\"\\nSplitting polynomial features...\")\n",
    "X_train_poly, X_test_poly, y_train_poly, y_test_poly = stratified_train_test_split(\n",
    "    X_poly, y, test_size=0.2, random_seed=42\n",
    ")\n",
    "\n",
    "# Standardization v·ªõi polynomial features\n",
    "print(f\"Standardizing polynomial features...\")\n",
    "scaler_poly = StandardScaler()\n",
    "X_train_poly_scaled = scaler_poly.fit_transform(X_train_poly)\n",
    "X_test_poly_scaled = scaler_poly.transform(X_test_poly)\n",
    "\n",
    "print(f\"Polynomial features ready!\")\n",
    "print(f\"  Train set: {X_train_poly_scaled.shape}\")\n",
    "print(f\"  Test set: {X_test_poly_scaled.shape}\")\n",
    "print(f\"  Feature range: [{np.min(X_train_poly_scaled):.3f}, {np.max(X_train_poly_scaled):.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1b07b8",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression Implementation\n",
    "### Sigmoid Function\n",
    "H√†m sigmoid ƒë·ªÉ convert linear output th√†nh probability (0-1 range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08574013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    H√†m sigmoid ·ªïn ƒë·ªãnh s·ªë h·ªçc\n",
    "    œÉ(z) = 1 / (1 + e^(-z))\n",
    "    \n",
    "    Args:\n",
    "        z: Linear combination (w^T * x + b)\n",
    "    Returns:\n",
    "        Probability values between 0 and 1\n",
    "    \"\"\"\n",
    "    # Clipping ƒë·ªÉ tr√°nh overflow/underflow\n",
    "    z = np.clip(z, -500, 500)\n",
    "    \n",
    "    # Stable sigmoid computation\n",
    "    # N·∫øu z >= 0: œÉ(z) = 1 / (1 + e^(-z))\n",
    "    # N·∫øu z < 0: œÉ(z) = e^z / (1 + e^z) ƒë·ªÉ tr√°nh overflow\n",
    "    \n",
    "    positive_mask = z >= 0\n",
    "    negative_mask = ~positive_mask\n",
    "    \n",
    "    result = np.zeros_like(z, dtype=np.float64)\n",
    "    \n",
    "    # Tr∆∞·ªùng h·ª£p z >= 0\n",
    "    exp_neg_z = np.exp(-z[positive_mask])\n",
    "    result[positive_mask] = 1 / (1 + exp_neg_z)\n",
    "    \n",
    "    # Tr∆∞·ªùng h·ª£p z < 0\n",
    "    exp_z = np.exp(z[negative_mask])\n",
    "    result[negative_mask] = exp_z / (1 + exp_z)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test sigmoid function\n",
    "print(\"Testing sigmoid function:\")\n",
    "test_values = np.array([-1000, -10, -1, 0, 1, 10, 1000])\n",
    "sigmoid_results = sigmoid(test_values)\n",
    "\n",
    "for val, sig in zip(test_values, sigmoid_results):\n",
    "    print(f\" sigmoid({val:4d}) = {sig:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be32c301",
   "metadata": {},
   "source": [
    "### Binary Cross Entropy Loss Function\n",
    "H√†m loss ƒë·ªÉ ƒëo l∆∞·ªùng sai s·ªë gi·ªØa prediction v√† actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dec5dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Binary Cross Entropy Loss\n",
    "    L = -1/n * Œ£[y*log(p) + (1-y)*log(1-p)]\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual labels (0 or 1) shape (n_samples,)\n",
    "        y_pred: Predicted probabilities shape (n_samples,)\n",
    "    Returns:\n",
    "        Average loss value\n",
    "    \"\"\"\n",
    "    n = len(y_true)\n",
    "    \n",
    "    # Clip predictions ƒë·ªÉ tr√°nh log(0)\n",
    "    y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    \n",
    "    # Binary cross entropy formula\n",
    "    loss = -(1/n) * np.sum(\n",
    "        y_true * np.log(y_pred_clipped) + \n",
    "        (1 - y_true) * np.log(1 - y_pred_clipped)\n",
    "    )\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Test loss function\n",
    "print(\"Testing binary cross entropy loss:\")\n",
    "y_true_test = np.array([0, 1, 0, 1, 1])\n",
    "y_pred_test = np.array([0.1, 0.9, 0.2, 0.8, 0.7])\n",
    "\n",
    "loss_value = binary_cross_entropy_loss(y_true_test, y_pred_test)\n",
    "print(f\" Test loss: {loss_value:.4f}\")\n",
    "\n",
    "# Test edge cases\n",
    "print(\"\\nEdge cases:\")\n",
    "perfect_pred = binary_cross_entropy_loss(y_true_test, y_true_test.astype(float))\n",
    "print(f\" Perfect prediction loss: {perfect_pred:.6f}\")\n",
    "\n",
    "worst_pred = binary_cross_entropy_loss(y_true_test, 1 - y_true_test.astype(float))\n",
    "print(f\" Worst prediction loss: {worst_pred:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33540761",
   "metadata": {},
   "source": [
    "### Enhanced Logistic Regression with Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c771dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionWithWeights:\n",
    "    \"\"\"\n",
    "    Enhanced Logistic Regression v·ªõi class weights ƒë·ªÉ handle imbalance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_iterations=2000, tolerance=1e-6, \n",
    "                 class_weights=None, regularization=0.01, verbose=True):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.class_weights = class_weights\n",
    "        self.regularization = regularization  # L2 regularization\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss_history = []\n",
    "        self.fitted = False\n",
    "    \n",
    "    def _calculate_class_weights(self, y):\n",
    "        \"\"\"T√≠nh class weights t·ª± ƒë·ªông\"\"\"\n",
    "        unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "        total_samples = len(y)\n",
    "        n_classes = len(unique_classes)\n",
    "        \n",
    "        # Balanced weights: n_samples / (n_classes * count_per_class)\n",
    "        weights = {}\n",
    "        for cls, count in zip(unique_classes, class_counts):\n",
    "            weights[cls] = total_samples / (n_classes * count)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def _get_sample_weights(self, y):\n",
    "        \"\"\"T·∫°o sample weights array\"\"\"\n",
    "        if self.class_weights is None:\n",
    "            self.class_weights = self._calculate_class_weights(y)\n",
    "        \n",
    "        sample_weights = np.zeros_like(y, dtype=np.float64)\n",
    "        for cls, weight in self.class_weights.items():\n",
    "            sample_weights[y == cls] = weight\n",
    "        \n",
    "        return sample_weights\n",
    "    \n",
    "    def _initialize_parameters(self, n_features):\n",
    "        # Xavier initialization v·ªõi scale nh·ªè h∆°n cho nhi·ªÅu features\n",
    "        limit = np.sqrt(1 / n_features) * 0.5\n",
    "        self.weights = np.random.uniform(-limit, limit, size=n_features)\n",
    "        self.bias = 0.0\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Initialized parameters for {n_features} features\")\n",
    "    \n",
    "    def _forward_pass(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = sigmoid(z)\n",
    "        return z, y_pred\n",
    "    \n",
    "    def _compute_weighted_loss(self, y_true, y_pred, sample_weights):\n",
    "        \"\"\"Binary cross entropy v·ªõi weighted samples\"\"\"\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        # Weighted loss\n",
    "        loss = -(sample_weights * (y_true * np.log(y_pred_clipped) + \n",
    "                                 (1 - y_true) * np.log(1 - y_pred_clipped)))\n",
    "        \n",
    "        # L2 regularization\n",
    "        l2_penalty = self.regularization * np.sum(self.weights ** 2)\n",
    "        \n",
    "        return np.mean(loss) + l2_penalty\n",
    "    \n",
    "    def _compute_weighted_gradients(self, X, y_true, y_pred, sample_weights):\n",
    "        \"\"\"Gradients v·ªõi weighted samples v√† L2 regularization\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        error = y_pred - y_true\n",
    "        \n",
    "        # Weighted gradients\n",
    "        weighted_error = error * sample_weights\n",
    "        \n",
    "        # Gradients v·ªõi L2 regularization\n",
    "        dw = (1/n_samples) * np.dot(X.T, weighted_error) + 2 * self.regularization * self.weights\n",
    "        db = (1/n_samples) * np.sum(weighted_error)\n",
    "        \n",
    "        return dw, db\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._initialize_parameters(n_features)\n",
    "        \n",
    "        # Calculate class weights and sample weights\n",
    "        sample_weights = self._get_sample_weights(y)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\nTraining Enhanced Logistic Regression\")\n",
    "            print(f\"Features: {n_features}, Samples: {n_samples}\")\n",
    "            print(f\"Class weights: {self.class_weights}\")\n",
    "            print(f\"Regularization: {self.regularization}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        prev_loss = float('inf')\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            # Forward pass\n",
    "            z, y_pred = self._forward_pass(X)\n",
    "            \n",
    "            # Compute weighted loss\n",
    "            current_loss = self._compute_weighted_loss(y, y_pred, sample_weights)\n",
    "            self.loss_history.append(current_loss)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw, db = self._compute_weighted_gradients(X, y, y_pred, sample_weights)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Progress\n",
    "            if self.verbose and (iteration + 1) % 200 == 0:\n",
    "                print(f\"Iteration {iteration + 1:4d}: Loss = {current_loss:.6f}\")\n",
    "            \n",
    "            # Convergence check\n",
    "            if abs(prev_loss - current_loss) < self.tolerance:\n",
    "                if self.verbose:\n",
    "                    print(f\"Converged at iteration {iteration + 1}\")\n",
    "                break\n",
    "            \n",
    "            prev_loss = current_loss\n",
    "        \n",
    "        self.fitted = True\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Training completed! Final loss: {current_loss:.6f}\")\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Model ch∆∞a ƒë∆∞·ª£c train!\")\n",
    "        _, y_pred = self._forward_pass(X)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c709ed",
   "metadata": {},
   "source": [
    "## 3. Enhanced Model Training\n",
    "\n",
    "### Train model v·ªõi polynomial features + class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a128ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. USE FULL TRAINING DATA \n",
    "print(\"Using FULL training data\")\n",
    "X_train_full = X_train_poly_scaled \n",
    "y_train_full = y_train_poly\n",
    "\n",
    "print(f\"Full training data: {X_train_full.shape}\")\n",
    "print(f\" Normal: {np.sum(y_train_full==0):,}\")\n",
    "print(f\" Fraud:  {np.sum(y_train_full==1):,}\")\n",
    "print(f\" Imbalance ratio: {np.sum(y_train_full==0)/np.sum(y_train_full==1):.1f}:1\")\n",
    "\n",
    "# 2. FEATURE SELECTION ƒë·ªÉ gi·∫£m overfitting\n",
    "print(f\"\\nFeature selection to reduce overfitting\")\n",
    "\n",
    "def select_top_features(X_train, y_train, top_k=25):\n",
    "    correlations = []\n",
    "    for i in range(X_train.shape[1]):\n",
    "        corr = np.corrcoef(X_train[:, i], y_train)[0, 1]\n",
    "        correlations.append(abs(corr) if not np.isnan(corr) else 0.0)\n",
    "    \n",
    "    # Ch·ªçn top K features\n",
    "    top_indices = np.argsort(correlations)[-top_k:]\n",
    "    return top_indices, np.array(correlations)\n",
    "\n",
    "top_feature_indices, feature_correlations = select_top_features(X_train_full, y_train_full, top_k=25)\n",
    "X_train_selected = X_train_full[:, top_feature_indices]\n",
    "X_test_selected = X_test_poly_scaled[:, top_feature_indices]\n",
    "\n",
    "print(f\"Original features: {X_train_full.shape[1]}\")\n",
    "print(f\"Selected features: {X_train_selected.shape[1]}\")\n",
    "print(f\"Top correlations: {np.sort(feature_correlations[top_feature_indices])[-5:]}\")\n",
    "\n",
    "enhanced_model = LogisticRegressionWithWeights(\n",
    "    learning_rate=0.01,    # Standard learning rate\n",
    "    max_iterations=1500,   # More iterations\n",
    "    tolerance=1e-7,        # Better convergence\n",
    "    regularization=0.1,    # 10x stronger regularization\n",
    "    class_weights={0: 1.0, 1: 20.0},  # Manual strong fraud weight\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining improved model...\")\n",
    "enhanced_model.fit(X_train_selected, y_train_full)\n",
    "\n",
    "# Training progress visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(enhanced_model.loss_history, 'b-', linewidth=2, label='Improved Model')\n",
    "plt.title('Improved Model Training Progress', fontweight='bold')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Weighted Loss')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nModel training completed!\")\n",
    "print(f\"Final weights range: [{np.min(enhanced_model.weights):.4f}, {np.max(enhanced_model.weights):.4f}]\")\n",
    "print(f\"Class weights used: {enhanced_model.class_weights}\")\n",
    "print(f\"Regularization: {enhanced_model.regularization}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b92950a",
   "metadata": {},
   "source": [
    "## 4. Simplified Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2abd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_numpy(y_true, y_pred):\n",
    "    \"\"\"Confusion Matrix implementation\"\"\"\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    cm = np.array([[tn, fp], [fn, tp]])\n",
    "    return cm, tp, tn, fp, fn\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    cm, tp, tn, fp, fn = confusion_matrix_numpy(y_true, y_pred)\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'confusion_matrix': cm,\n",
    "        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        'f1_score': f1_score,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "def print_simple_evaluation(y_true, y_pred, dataset_name=\"Test\"):\n",
    "    \"\"\"Simple evaluation report\"\"\"\n",
    "    metrics = calculate_metrics(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{dataset_name.upper()} SET EVALUATION\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Key metrics\n",
    "    print(f\"Precision: {metrics['precision']:8.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:8.4f}\")\n",
    "    print(f\"F1-Score: {metrics['f1_score']:8.4f}\")\n",
    "    print(f\"Specificity: {metrics['specificity']:8.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\" Predicted\")\n",
    "    print(f\" Normal Fraud\")\n",
    "    print(f\"Normal {metrics['tn']:5d}  {metrics['fp']:5d}\")\n",
    "    print(f\"Fraud {metrics['fn']:5d}  {metrics['tp']:5d}\")\n",
    "    \n",
    "    # Business impact\n",
    "    total_fraud = metrics['tp'] + metrics['fn']\n",
    "    fraud_caught_pct = metrics['tp'] / total_fraud * 100 if total_fraud > 0 else 0\n",
    "    false_alarm_rate = metrics['fp'] / (metrics['fp'] + metrics['tn']) * 100\n",
    "    \n",
    "    print(f\"\\nBusiness Impact:\")\n",
    "    print(f\"Fraud detection rate: {fraud_caught_pct:5.1f}% ({metrics['tp']}/{total_fraud})\")\n",
    "    print(f\"False alarm rate: {false_alarm_rate:5.2f}%\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6ee5b5",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab6b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IMPROVED MODEL EVALUATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# 4. THRESHOLD OPTIMIZATION ƒë·ªÉ maximize performance\n",
    "print(\"Fix 4: Threshold optimization\")\n",
    "\n",
    "def evaluate_thresholds_simple(y_true, y_proba, thresholds):\n",
    "    results = []\n",
    "    for thresh in thresholds:\n",
    "        y_pred = (y_proba >= thresh).astype(int)\n",
    "        metrics = calculate_metrics(y_true, y_pred)\n",
    "        results.append({\n",
    "            'threshold': thresh,\n",
    "            'precision': metrics['precision'],\n",
    "            'recall': metrics['recall'], \n",
    "            'f1_score': metrics['f1_score'],\n",
    "            'tp': metrics['tp'],\n",
    "            'fp': metrics['fp'],\n",
    "            'fn': metrics['fn']\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Test multiple thresholds\n",
    "enhanced_test_probs = enhanced_model.predict_proba(X_test_selected)\n",
    "thresholds = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "threshold_results = evaluate_thresholds_simple(y_test_poly, enhanced_test_probs, thresholds)\n",
    "\n",
    "print(f\"\\nThreshold optimization results:\")\n",
    "print(f\"{'Threshold':<10} {'Precision':<10} {'Recall':<8} {'F1-Score':<10} {'TP':<4} {'FP':<4} {'FN':<4}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "best_f1 = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "for result in threshold_results:\n",
    "    print(f\"{result['threshold']:<10.2f} {result['precision']:<10.4f} {result['recall']:<8.4f} {result['f1_score']:<10.4f} {result['tp']:<4d} {result['fp']:<4d} {result['fn']:<4d}\")\n",
    "    \n",
    "    if result['f1_score'] > best_f1:\n",
    "        best_f1 = result['f1_score']\n",
    "        best_threshold = result['threshold']\n",
    "\n",
    "print(f\"\\nBest threshold: {best_threshold} (F1 = {best_f1:.4f})\")\n",
    "\n",
    "# Final predictions v·ªõi best threshold\n",
    "enhanced_test_pred = enhanced_model.predict(X_test_selected, threshold=best_threshold)\n",
    "enhanced_metrics = print_simple_evaluation(y_test_poly, enhanced_test_pred, \"Improved Test\")\n",
    "\n",
    "# Probability analysis\n",
    "enhanced_fraud_probs = enhanced_test_probs[y_test_poly == 1]\n",
    "enhanced_normal_probs = enhanced_test_probs[y_test_poly == 0]\n",
    "\n",
    "print(f\"\\n\" + \"=\"*40)\n",
    "print(\"PROBABILITY SEPARATION ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Fraud probabilities:\")\n",
    "print(f\"  Mean: {np.mean(enhanced_fraud_probs):.4f}\")\n",
    "print(f\"  Median: {np.median(enhanced_fraud_probs):.4f}\")\n",
    "print(f\"  Std:  {np.std(enhanced_fraud_probs):.4f}\")\n",
    "\n",
    "print(f\"Normal probabilities:\")\n",
    "print(f\"  Mean: {np.mean(enhanced_normal_probs):.4f}\")\n",
    "print(f\"  Median: {np.median(enhanced_normal_probs):.4f}\")\n",
    "print(f\"  Std:  {np.std(enhanced_normal_probs):.4f}\")\n",
    "\n",
    "# Separation quality\n",
    "mean_diff = np.mean(enhanced_fraud_probs) - np.mean(enhanced_normal_probs)\n",
    "pooled_std = np.sqrt((np.var(enhanced_fraud_probs) + np.var(enhanced_normal_probs)) / 2)\n",
    "separation_score = mean_diff / pooled_std if pooled_std > 0 else 0\n",
    "\n",
    "print(f\"Probability separation score: {separation_score:.3f}\")\n",
    "print(f\"Quality: {'EXCELLENT' if separation_score > 1.5 else 'GOOD' if separation_score > 1.0 else 'FAIR' if separation_score > 0.5 else 'POOR'}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(enhanced_normal_probs, bins=50, alpha=0.7, label=f'Normal (n={len(enhanced_normal_probs):,})', \n",
    "         color='#2E8B57', density=True)\n",
    "plt.hist(enhanced_fraud_probs, bins=50, alpha=0.7, label=f'Fraud (n={len(enhanced_fraud_probs):,})', \n",
    "         color='#DC143C', density=True)\n",
    "plt.axvline(x=best_threshold, color='black', linestyle='--', label=f'Best Threshold={best_threshold:.2f}')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Improved Model - Probability Distribution')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "cm = enhanced_metrics['confusion_matrix']\n",
    "im = plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "plt.colorbar(im)\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > cm.max()/2 else \"black\",\n",
    "                fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title(f'Improved Model - Confusion Matrix\\n(Threshold = {best_threshold})')\n",
    "plt.xticks([0, 1], ['Normal', 'Fraud'])\n",
    "plt.yticks([0, 1], ['Normal', 'Fraud'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5238ee",
   "metadata": {},
   "source": [
    "## 6. Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e2a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL IMPROVED MODEL SUMMARY\n",
    "print(\"IMPROVED MODEL SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "print(f\"\\nMODEL ARCHITECTURE:\")\n",
    "print(f\"  Base: Logistic Regression (Pure NumPy)\")\n",
    "print(f\"  Features: {X_train_selected.shape[1]} selected features\")\n",
    "print(f\"  Training samples: {len(X_train_full):,}\")\n",
    "print(f\"  Regularization: L2 (Œª = {enhanced_model.regularization})\")\n",
    "print(f\"  Class weights: {enhanced_model.class_weights}\")\n",
    "\n",
    "print(f\"\\nFINAL PERFORMANCE (Test Set, Threshold = {best_threshold}):\")\n",
    "print(f\"  Precision:     {enhanced_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:        {enhanced_metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score:      {enhanced_metrics['f1_score']:.4f}\")\n",
    "print(f\"  Specificity:   {enhanced_metrics['specificity']:.4f}\")\n",
    "\n",
    "# Business impact calculation\n",
    "total_test_fraud = np.sum(y_test_poly == 1)\n",
    "fraud_detected = enhanced_metrics['tp']\n",
    "fraud_missed = enhanced_metrics['fn']\n",
    "avg_fraud_amount = 122.21  # From EDA\n",
    "\n",
    "estimated_fraud_prevented = fraud_detected * avg_fraud_amount\n",
    "estimated_fraud_loss = fraud_missed * avg_fraud_amount\n",
    "\n",
    "print(f\"\\nBUSINESS IMPACT:\")\n",
    "print(f\"  Total fraud cases: {total_test_fraud}\")\n",
    "print(f\"  Fraud detected:    {fraud_detected} ({fraud_detected/total_test_fraud*100:.1f}%)\")\n",
    "print(f\"  Fraud missed:      {fraud_missed} ({fraud_missed/total_test_fraud*100:.1f}%)\")\n",
    "print(f\"  Est. fraud prevented: ${estimated_fraud_prevented:,.2f}\")\n",
    "print(f\"  Est. fraud loss:      ${estimated_fraud_loss:,.2f}\")\n",
    "\n",
    "# Performance assessment\n",
    "if enhanced_metrics['f1_score'] >= 0.80:\n",
    "    performance_level = \"EXCELLENT\"\n",
    "    icon = \"üéØ\"\n",
    "elif enhanced_metrics['f1_score'] >= 0.75:\n",
    "    performance_level = \"GOOD\" \n",
    "    icon = \"‚úÖ\"\n",
    "elif enhanced_metrics['f1_score'] >= 0.70:\n",
    "    performance_level = \"ACCEPTABLE\"\n",
    "    icon = \"‚ö†Ô∏è\"\n",
    "else:\n",
    "    performance_level = \"NEEDS IMPROVEMENT\"\n",
    "    icon = \"üî¥\"\n",
    "\n",
    "print(f\"\\n{icon} FINAL ASSESSMENT:\")\n",
    "print(f\"  Model Performance: {performance_level}\")\n",
    "print(f\"  F1-Score: {enhanced_metrics['f1_score']:.4f}\")\n",
    "\n",
    "if enhanced_metrics['recall'] >= 0.75:\n",
    "    print(f\" Good fraud detection rate ({enhanced_metrics['recall']*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\" Moderate fraud detection rate ({enhanced_metrics['recall']*100:.1f}%)\")\n",
    "\n",
    "if enhanced_metrics['precision'] >= 0.80:\n",
    "    print(f\" Low false alarm rate\")\n",
    "else:\n",
    "    print(f\" Moderate false alarm rate\")\n",
    "\n",
    "print(f\"\\nKEY IMPROVEMENTS:\")\n",
    "print(f\" Full training data prevents underfitting\")\n",
    "print(f\" Feature selection reduces overfitting\") \n",
    "print(f\" Strong regularization improves generalization\")\n",
    "print(f\" Optimized threshold maximizes F1-score\")\n",
    "print(f\" Balanced class weights handle imbalance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
