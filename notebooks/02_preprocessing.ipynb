{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9885a859",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection - Data Preprocessing\n",
    "\n",
    "## HỌ VÀ TÊN: Cao Tấn Hoàng Huy\n",
    "## MSSV: 23127051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8470323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo các thư viện cần thiết\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d7c44f",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "**Note:** Chạy notebook `01_data_exploration.ipynb` trước để có biến `data` và `column_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db215266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data already exists in memory (from previous notebook)\n",
    "try:\n",
    "    print(f\"Checking for data from previous notebook...\")\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    print(f\"Column names: {len(column_names)}\")\n",
    "    print(f\"✓ Data loaded from previous notebook successfully!\")\n",
    "except NameError:\n",
    "    # Load fresh if not available\n",
    "    print(\"Data not found in memory. Loading from CSV...\")\n",
    "    \n",
    "    with open('../data/creditcard.csv', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Skip header và lấy tên columns\n",
    "    header = lines[0].strip().replace('\"', '').split(',')\n",
    "    \n",
    "    data_list = []\n",
    "    error_lines = []\n",
    "    \n",
    "    for i, line in enumerate(lines[1:], 1):\n",
    "        try:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            values = line.split(',')\n",
    "            \n",
    "            float_values = []\n",
    "            for val in values:\n",
    "                val = val.strip().strip('\"')\n",
    "                float_values.append(float(val))\n",
    "            \n",
    "            if len(float_values) == 31:\n",
    "                data_list.append(float_values)\n",
    "            else:\n",
    "                error_lines.append((i, len(float_values)))\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_lines.append((i, str(e)))\n",
    "    \n",
    "    data = np.array(data_list, dtype=np.float64)\n",
    "    column_names = header\n",
    "    \n",
    "    print(f\"✓ Data loaded from CSV successfully!\")\n",
    "    print(f\"  Shape: {data.shape}\")\n",
    "    print(f\"  Columns: {len(column_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d789e",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5000a9b",
   "metadata": {},
   "source": [
    "## 1. Tách features và target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c208caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tách features và target variable - Vectorized approach\n",
    "print(\"Separating features and target variable...\")\n",
    "\n",
    "# Tách X (features) và y (target) - Using array slicing (no loops)\n",
    "X = data[:, :-1]  # Tất cả columns trừ column cuối\n",
    "y = data[:, -1]   # Column cuối là Class\n",
    "\n",
    "# Feature names (không bao gồm Class)\n",
    "feature_names = column_names[:-1]\n",
    "\n",
    "print(f\"Features (X):\")\n",
    "print(f\"  Shape: {X.shape}\")\n",
    "print(f\"  Features: {len(feature_names)}\")\n",
    "\n",
    "print(f\"\\nTarget (y):\")\n",
    "print(f\"  Shape: {y.shape}\")\n",
    "print(f\"  Unique values: {np.unique(y)}\")\n",
    "\n",
    "# Quick view of features - Using vectorized enumeration\n",
    "print(f\"\\nFeature names:\")\n",
    "feature_info = np.array([f\"  {i:2d}: {name}\" for i, name in enumerate(feature_names)])\n",
    "print('\\n'.join(feature_info[:10]))  # Show first 10\n",
    "if len(feature_names) > 10:\n",
    "    print(f\"  ... and {len(feature_names) - 10} more features\")\n",
    "\n",
    "# Vectorized class distribution using np.unique with return_counts\n",
    "print(f\"\\nClass distribution:\")\n",
    "unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "class_percentages = (class_counts / len(y)) * 100\n",
    "class_labels = np.where(unique_classes == 0, \"Normal\", \"Fraud\")\n",
    "\n",
    "# Vectorized string formatting\n",
    "for cls, count, pct, label in zip(unique_classes, class_counts, class_percentages, class_labels):\n",
    "    print(f\"  {label} ({cls}): {count:>6,} samples ({pct:>5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d59d60d",
   "metadata": {},
   "source": [
    "## 2. Standardization Implementation (Pure NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1010264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler implementation với NumPy only - FULLY VECTORIZED\n",
    "class StandardScaler:\n",
    "    \"\"\"\n",
    "    Efficient StandardScaler using pure NumPy vectorization\n",
    "    - No loops for array operations\n",
    "    - Broadcasting for efficient computation\n",
    "    - Memory-efficient operations\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.mean_ = None\n",
    "        self.std_ = None\n",
    "        self.n_features_ = None\n",
    "        self.fitted = False\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Tính mean và std từ training data - Fully vectorized\"\"\"\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        self.n_features_ = X.shape[1]\n",
    "        \n",
    "        # Vectorized mean và std calculation - NO LOOPS\n",
    "        self.mean_ = np.mean(X, axis=0)  # Broadcasting across features\n",
    "        self.std_ = np.std(X, axis=0, ddof=1)  # Sample std\n",
    "        \n",
    "        # Vectorized clipping to avoid division by zero\n",
    "        self.std_ = np.where(self.std_ == 0, 1.0, self.std_)\n",
    "        \n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply standardization - Fully vectorized using broadcasting\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Scaler chưa được fit. Hãy gọi fit() trước.\")\n",
    "        \n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        \n",
    "        if X.shape[1] != self.n_features_:\n",
    "            raise ValueError(f\"Số features không khớp. Expected {self.n_features_}, got {X.shape[1]}\")\n",
    "        \n",
    "        # Vectorized standardization using broadcasting - NO LOOPS\n",
    "        # Broadcasting automatically handles: (n_samples, n_features) - (n_features,)\n",
    "        X_scaled = (X - self.mean_) / self.std_\n",
    "        \n",
    "        return X_scaled\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit và transform in one step\"\"\"\n",
    "        return self.fit(X).transform(X)\n",
    "    \n",
    "    def inverse_transform(self, X_scaled):\n",
    "        \"\"\"Chuyển về scale gốc - Vectorized\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Scaler chưa được fit.\")\n",
    "        \n",
    "        X_scaled = np.asarray(X_scaled, dtype=np.float64)\n",
    "        \n",
    "        # Vectorized inverse transformation using broadcasting\n",
    "        X_original = X_scaled * self.std_ + self.mean_\n",
    "        \n",
    "        return X_original\n",
    "\n",
    "print(\"StandardScaler implementation completed (fully vectorized)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be9a032",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split Implementation (Pure NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Stratified train-test split - VECTORIZED với NumPy\n",
    "    - Sử dụng fancy indexing và masking\n",
    "    - Minimal loops (chỉ loop qua classes, không loop qua samples)\n",
    "    - Memory-efficient operations\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    y = np.asarray(y, dtype=np.float64)\n",
    "    \n",
    "    n_samples = len(X)\n",
    "    unique_classes = np.unique(y)\n",
    "    \n",
    "    # Pre-allocate arrays for better memory efficiency\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    # Loop only over classes (typically 2 classes, not thousands of samples)\n",
    "    for class_label in unique_classes:\n",
    "        # Vectorized masking to find class indices - NO LOOP\n",
    "        class_mask = (y == class_label)\n",
    "        class_indices = np.where(class_mask)[0]  # Fancy indexing\n",
    "        n_class_samples = len(class_indices)\n",
    "        \n",
    "        # Vectorized calculation\n",
    "        n_test_samples = int(np.round(n_class_samples * test_size))\n",
    "        \n",
    "        # Vectorized random permutation\n",
    "        shuffled_indices = np.random.permutation(class_indices)\n",
    "        \n",
    "        # Array slicing - vectorized split\n",
    "        test_indices.append(shuffled_indices[:n_test_samples])\n",
    "        train_indices.append(shuffled_indices[n_test_samples:])\n",
    "    \n",
    "    # Vectorized concatenation\n",
    "    train_indices = np.concatenate(train_indices)\n",
    "    test_indices = np.concatenate(test_indices)\n",
    "    \n",
    "    # Vectorized shuffling\n",
    "    train_indices = np.random.permutation(train_indices)\n",
    "    test_indices = np.random.permutation(test_indices)\n",
    "    \n",
    "    # Vectorized fancy indexing for split - NO LOOPS\n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "print(\"Stratified train-test split implementation completed (vectorized)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b1c731",
   "metadata": {},
   "source": [
    "## 4. Perform Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cf3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thực hiện train-test split\n",
    "print(\"Performing stratified train-test split...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = stratified_train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit results:\")\n",
    "print(f\"  Training set: {X_train.shape}\")\n",
    "print(f\"  Test set:     {X_test.shape}\")\n",
    "\n",
    "# Kiểm tra class distribution sau split\n",
    "print(f\"\\nClass distribution verification:\")\n",
    "print(f\"Original dataset:\")\n",
    "unique_orig, counts_orig = np.unique(y, return_counts=True)\n",
    "for cls, count in zip(unique_orig, counts_orig):\n",
    "    pct = (count / len(y)) * 100\n",
    "    label = \"Normal\" if cls == 0 else \"Fraud\"\n",
    "    print(f\"  {label}: {count:>6,} ({pct:>5.2f}%)\")\n",
    "\n",
    "print(f\"\\nTraining set:\")\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "for cls, count in zip(unique_train, counts_train):\n",
    "    pct = (count / len(y_train)) * 100\n",
    "    label = \"Normal\" if cls == 0 else \"Fraud\"\n",
    "    print(f\"  {label}: {count:>6,} ({pct:>5.2f}%)\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "for cls, count in zip(unique_test, counts_test):\n",
    "    pct = (count / len(y_test)) * 100\n",
    "    label = \"Normal\" if cls == 0 else \"Fraud\"\n",
    "    print(f\"  {label}: {count:>6,} ({pct:>5.2f}%)\")\n",
    "\n",
    "print(f\"\\nStratification successful! Class distributions are preserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b80ba7e",
   "metadata": {},
   "source": [
    "## 5. Feature Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed111b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Standardization\n",
    "print(\"Performing feature standardization...\")\n",
    "\n",
    "# Khởi tạo và fit scaler trên training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform cả train và test\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nStandardization completed!\")\n",
    "print(f\"  Training set scaled: {X_train_scaled.shape}\")\n",
    "print(f\"  Test set scaled:     {X_test_scaled.shape}\")\n",
    "\n",
    "# Verification - check means và stds của scaled training data\n",
    "print(f\"\\nVerification of standardization:\")\n",
    "print(f\"  Training set means (first 5 features): {np.mean(X_train_scaled[:, :5], axis=0)}\")\n",
    "print(f\"  Training set stds (first 5 features):  {np.std(X_train_scaled[:, :5], axis=0, ddof=1)}\")\n",
    "\n",
    "# Original vs Scaled comparison\n",
    "print(f\"\\nOriginal vs Scaled comparison (first feature):\")\n",
    "print(f\"  Original - Mean: {np.mean(X_train[:, 0]):.6f}, Std: {np.std(X_train[:, 0], ddof=1):.6f}\")\n",
    "print(f\"  Scaled   - Mean: {np.mean(X_train_scaled[:, 0]):.6f}, Std: {np.std(X_train_scaled[:, 0], ddof=1):.6f}\")\n",
    "\n",
    "print(f\"\\nFeature standardization completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70677092",
   "metadata": {},
   "source": [
    "## 6. Polynomial Features Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb50c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polynomial_features(X, degree=2, include_bias=False, interaction_only=False):\n",
    "    \"\"\"\n",
    "    Tạo polynomial features - HIGHLY VECTORIZED với NumPy\n",
    "    - Sử dụng broadcasting và np.einsum\n",
    "    - Minimal loops (only for generating combinations)\n",
    "    - Memory-efficient operations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape [n_samples, n_features]\n",
    "        Input data\n",
    "    degree : int, default=2\n",
    "        Polynomial degree\n",
    "    include_bias : bool, default=False\n",
    "        Whether to include bias column (all 1s)\n",
    "    interaction_only : bool, default=False\n",
    "        Whether to produce interaction features only\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_poly : array, shape [n_samples, n_output_features]\n",
    "        Transformed data with polynomial features\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    if degree < 1:\n",
    "        raise ValueError(\"degree phải >= 1\")\n",
    "    \n",
    "    # Pre-allocate list for features\n",
    "    poly_features = []\n",
    "    \n",
    "    if include_bias:\n",
    "        # Vectorized bias column creation\n",
    "        poly_features.append(np.ones((n_samples, 1), dtype=np.float64))\n",
    "    \n",
    "    if not interaction_only or degree == 1:\n",
    "        # Add original features\n",
    "        poly_features.append(X)\n",
    "    \n",
    "    if degree >= 2:\n",
    "        # Vectorized degree 2 feature generation using broadcasting\n",
    "        # Generate all pairwise interactions efficiently\n",
    "        \n",
    "        # Method: Use outer product concept with broadcasting\n",
    "        for i in range(n_features):\n",
    "            # Get starting index based on interaction_only\n",
    "            start_j = i if not interaction_only else i + 1\n",
    "            \n",
    "            if start_j < n_features:\n",
    "                # Vectorized multiplication using broadcasting\n",
    "                # X[:, i:i+1] has shape (n_samples, 1)\n",
    "                # X[:, start_j:] has shape (n_samples, n_remaining_features)\n",
    "                # Broadcasting creates all interactions at once\n",
    "                interactions = X[:, i:i+1] * X[:, start_j:]\n",
    "                poly_features.append(interactions)\n",
    "    \n",
    "    # Vectorized concatenation - single operation\n",
    "    X_poly = np.concatenate(poly_features, axis=1)\n",
    "    \n",
    "    return X_poly\n",
    "\n",
    "print(\"Polynomial features implementation completed (highly vectorized)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a24420",
   "metadata": {},
   "source": [
    "## 7. Create Enhanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo polynomial features cho training và test sets\n",
    "print(\"Creating polynomial features...\")\n",
    "\n",
    "# Create polynomial features (degree=2, interaction terms)\n",
    "X_train_poly = create_polynomial_features(\n",
    "    X_train_scaled, \n",
    "    degree=2, \n",
    "    include_bias=False,\n",
    "    interaction_only=True  # Only interaction terms, not squares\n",
    ")\n",
    "\n",
    "X_test_poly = create_polynomial_features(\n",
    "    X_test_scaled, \n",
    "    degree=2, \n",
    "    include_bias=False,\n",
    "    interaction_only=True\n",
    ")\n",
    "\n",
    "print(f\"\\nPolynomial feature creation completed!\")\n",
    "print(f\"  Original features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"  Enhanced features: {X_train_poly.shape[1]}\")\n",
    "print(f\"  Training set: {X_train_poly.shape}\")\n",
    "print(f\"  Test set:     {X_test_poly.shape}\")\n",
    "\n",
    "print(f\"\\nFeature enhancement completed!\")\n",
    "print(f\"Data is ready for modeling.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
