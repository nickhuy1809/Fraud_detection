{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95eaa528",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection - Enhanced Modeling\n",
    "\n",
    "## HỌ VÀ TÊN: Cao Tấn Hoàng Huy\n",
    "## MSSV: 23127051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b786e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo các thư viện cần thiết\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "# Setup for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c44cc9",
   "metadata": {},
   "source": [
    "## Load preprocessed data\n",
    "\n",
    "**Note:** Chạy notebook `02_preprocessing.ipynb` trước để có các biến đã được xử lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436cd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if preprocessed data exists in memory (from previous notebook)\n",
    "try:\n",
    "    print(\"Checking for preprocessed data from previous notebook...\")\n",
    "    print(f\"X_train_scaled: {X_train_scaled.shape}\")\n",
    "    print(f\"X_test_scaled:  {X_test_scaled.shape}\")\n",
    "    print(f\"X_train_poly:   {X_train_poly.shape}\")\n",
    "    print(f\"X_test_poly:    {X_test_poly.shape}\")\n",
    "    print(f\"y_train:        {y_train.shape}\")\n",
    "    print(f\"y_test:         {y_test.shape}\")\n",
    "    print(f\"Preprocessed data loaded from previous notebook successfully!\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"ERROR: Preprocessed data not found in memory!\")\n",
    "    print(\"Please run notebooks in order:\")\n",
    "    print(\"  1. 01_data_exploration.ipynb\")\n",
    "    print(\"  2. 02_preprocessing.ipynb\")\n",
    "    print(\"  3. 03_modeling.ipynb (current)\")\n",
    "    print(\"\\nMake sure to run all cells in the previous notebooks in the same kernel session.\")\n",
    "    raise NameError(\"Required variables not found. Run previous notebooks first.\")\n",
    "\n",
    "# Class distribution\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "print(f\"\\nClass distribution in training:\")\n",
    "for cls, count in zip(unique_train, counts_train):\n",
    "    pct = (count / len(y_train)) * 100\n",
    "    label = \"Normal\" if cls == 0 else \"Fraud\"\n",
    "    print(f\"  {label}: {count:>6,} ({pct:>5.2f}%)\")\n",
    "\n",
    "imbalance_ratio = counts_train[0] / counts_train[1] if len(counts_train) == 2 else None\n",
    "print(f\"  Imbalance ratio: {imbalance_ratio:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f14501",
   "metadata": {},
   "source": [
    "# THREE MODELS IMPLEMENTATION - PURE NUMPY\n",
    "\n",
    "## Requirements:\n",
    "- Vectorization (NO for loops on arrays)\n",
    "- Broadcasting efficiency\n",
    "- Universal functions (ufuncs)\n",
    "- Fancy indexing & masking\n",
    "- Array manipulation (reshape, transpose, stack, etc.)\n",
    "- Memory-efficient operations\n",
    "- np.einsum for complex calculations\n",
    "- Numerical stability\n",
    "- Clean, efficient, reproducible code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e720c0ee",
   "metadata": {},
   "source": [
    "## 1. Linear Regression (Pure NumPy - Fully Vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Linear Regression - Pure NumPy Implementation\n",
    "    \n",
    "    VECTORIZATION FEATURES:\n",
    "    - No for loops for array operations\n",
    "    - Broadcasting for efficient computation\n",
    "    - np.linalg for optimized matrix operations\n",
    "    - Memory-efficient gradient computation\n",
    "    - Numerical stability with regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000, \n",
    "                 tolerance=1e-6, regularization_strength=0.01, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.lambda_ = regularization_strength  # L2 regularization\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "        self.fitted = False\n",
    "    \n",
    "    def _compute_cost(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute MSE cost with L2 regularization - FULLY VECTORIZED\n",
    "        Uses broadcasting and np.einsum for efficiency\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Vectorized prediction: y_pred = X @ weights + bias (broadcasting)\n",
    "        y_pred = X @ self.weights + self.bias\n",
    "        \n",
    "        # Vectorized MSE using einsum for efficient squared error\n",
    "        squared_error = np.einsum('i,i->', (y_pred - y), (y_pred - y))\n",
    "        mse = squared_error / (2 * m)\n",
    "        \n",
    "        # L2 regularization term (vectorized)\n",
    "        l2_penalty = (self.lambda_ / 2) * np.einsum('i,i->', self.weights, self.weights)\n",
    "        \n",
    "        return mse + l2_penalty\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train using Gradient Descent - FULLY VECTORIZED\n",
    "        NO loops for array operations, only iteration loop\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        y = np.asarray(y, dtype=np.float64)\n",
    "        \n",
    "        m, n = X.shape\n",
    "        \n",
    "        # Xavier initialization for numerical stability\n",
    "        limit = np.sqrt(6.0 / (n + 1))\n",
    "        self.weights = np.random.uniform(-limit, limit, n)\n",
    "        self.bias = 0.0\n",
    "        self.cost_history = []\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Training Linear Regression...\")\n",
    "            print(f\"  Samples: {m}, Features: {n}\")\n",
    "            print(f\"  Regularization (λ): {self.lambda_}\")\n",
    "        \n",
    "        prev_cost = float('inf')\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            # VECTORIZED Forward pass using broadcasting\n",
    "            y_pred = X @ self.weights + self.bias  # Shape: (m,)\n",
    "            \n",
    "            # VECTORIZED Gradient computation using einsum\n",
    "            error = y_pred - y  # Shape: (m,)\n",
    "            \n",
    "            # Efficient gradient computation\n",
    "            # dw = (1/m) * X.T @ error + λ * weights\n",
    "            dw = (1/m) * np.einsum('ij,i->j', X, error) + self.lambda_ * self.weights\n",
    "            db = (1/m) * np.sum(error)\n",
    "            \n",
    "            # VECTORIZED Parameter update\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Cost tracking\n",
    "            cost = self._compute_cost(X, y)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Convergence check\n",
    "            if abs(prev_cost - cost) < self.tolerance:\n",
    "                if self.verbose:\n",
    "                    print(f\"  Converged at iteration {iteration + 1}\")\n",
    "                break\n",
    "            prev_cost = cost\n",
    "            \n",
    "            if self.verbose and (iteration + 1) % 100 == 0:\n",
    "                print(f\"  Iteration {iteration + 1:>4}: Cost = {cost:.6f}\")\n",
    "        \n",
    "        self.fitted = True\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"  Training completed! Final cost: {self.cost_history[-1]:.6f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict - VECTORIZED using broadcasting\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Model chưa được train. Hãy gọi fit() trước.\")\n",
    "        \n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        return X @ self.weights + self.bias\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Calculate R² score - VECTORIZED\"\"\"\n",
    "        y = np.asarray(y, dtype=np.float64)\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        # Vectorized R² calculation using einsum\n",
    "        ss_res = np.einsum('i,i->', (y - y_pred), (y - y_pred))\n",
    "        ss_tot = np.einsum('i,i->', (y - np.mean(y)), (y - np.mean(y)))\n",
    "        \n",
    "        return 1 - (ss_res / ss_tot) if ss_tot != 0 else 0.0\n",
    "\n",
    "print(\"Linear Regression implementation completed (fully vectorized)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474a2b5",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression (Pure NumPy - Fully Vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90db0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression - Pure NumPy Implementation\n",
    "    \n",
    "    VECTORIZATION FEATURES:\n",
    "    - No for loops for array operations\n",
    "    - Broadcasting for efficient computation\n",
    "    - Numerically stable sigmoid implementation\n",
    "    - Vectorized cross-entropy loss\n",
    "    - Class weight support using fancy indexing\n",
    "    - np.einsum for efficient gradient computation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000, \n",
    "                 tolerance=1e-6, regularization_strength=0.1, \n",
    "                 class_weights=None, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.lambda_ = regularization_strength\n",
    "        self.class_weights = class_weights\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "        self.fitted = False\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Numerically stable sigmoid function - VECTORIZED\n",
    "        Uses np.clip to prevent overflow\n",
    "        \"\"\"\n",
    "        # Clip to prevent exp overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def _compute_sample_weights(self, y):\n",
    "        \"\"\"\n",
    "        Calculate sample weights - VECTORIZED using fancy indexing\n",
    "        NO loops over samples\n",
    "        \"\"\"\n",
    "        if self.class_weights is None:\n",
    "            return np.ones(len(y), dtype=np.float64)\n",
    "        \n",
    "        # Vectorized weight assignment using fancy indexing\n",
    "        sample_weights = np.zeros(len(y), dtype=np.float64)\n",
    "        for class_label, weight in self.class_weights.items():\n",
    "            # Boolean masking - vectorized\n",
    "            mask = (y == class_label)\n",
    "            sample_weights[mask] = weight\n",
    "        \n",
    "        return sample_weights\n",
    "    \n",
    "    def _compute_cost(self, X, y, sample_weights):\n",
    "        \"\"\"\n",
    "        Compute weighted binary cross-entropy - FULLY VECTORIZED\n",
    "        Uses numerical stability tricks and einsum\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Vectorized forward pass\n",
    "        z = X @ self.weights + self.bias\n",
    "        y_pred = self._sigmoid(z)\n",
    "        \n",
    "        # Numerical stability: clip predictions\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Vectorized cross-entropy using ufuncs\n",
    "        log_loss = -(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "        \n",
    "        # Weighted loss using einsum\n",
    "        weighted_loss = np.einsum('i,i->', sample_weights, log_loss) / m\n",
    "        \n",
    "        # L2 regularization\n",
    "        l2_penalty = (self.lambda_ / 2) * np.einsum('i,i->', self.weights, self.weights)\n",
    "        \n",
    "        return weighted_loss + l2_penalty\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train using Gradient Descent - FULLY VECTORIZED\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        y = np.asarray(y, dtype=np.float64)\n",
    "        \n",
    "        m, n = X.shape\n",
    "        \n",
    "        # Xavier initialization\n",
    "        limit = np.sqrt(6.0 / (n + 1))\n",
    "        self.weights = np.random.uniform(-limit, limit, n)\n",
    "        self.bias = 0.0\n",
    "        self.cost_history = []\n",
    "        \n",
    "        # Compute sample weights once (vectorized)\n",
    "        sample_weights = self._compute_sample_weights(y)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Training Logistic Regression...\")\n",
    "            print(f\"  Samples: {m}, Features: {n}\")\n",
    "            print(f\"  Regularization (λ): {self.lambda_}\")\n",
    "            if self.class_weights:\n",
    "                print(f\"  Class weights: {self.class_weights}\")\n",
    "        \n",
    "        prev_cost = float('inf')\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            # VECTORIZED Forward pass\n",
    "            z = X @ self.weights + self.bias\n",
    "            y_pred = self._sigmoid(z)\n",
    "            \n",
    "            # VECTORIZED Gradient computation\n",
    "            error = y_pred - y\n",
    "            \n",
    "            # Weighted gradients using einsum\n",
    "            weighted_error = sample_weights * error\n",
    "            dw = (1/m) * np.einsum('ij,i->j', X, weighted_error) + self.lambda_ * self.weights\n",
    "            db = (1/m) * np.sum(weighted_error)\n",
    "            \n",
    "            # VECTORIZED Parameter update\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Cost tracking\n",
    "            cost = self._compute_cost(X, y, sample_weights)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Convergence check\n",
    "            if abs(prev_cost - cost) < self.tolerance:\n",
    "                if self.verbose:\n",
    "                    print(f\"  Converged at iteration {iteration + 1}\")\n",
    "                break\n",
    "            prev_cost = cost\n",
    "            \n",
    "            if self.verbose and (iteration + 1) % 100 == 0:\n",
    "                print(f\"  Iteration {iteration + 1:>4}: Cost = {cost:.6f}\")\n",
    "        \n",
    "        self.fitted = True\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"  Training completed! Final cost: {self.cost_history[-1]:.6f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities - VECTORIZED\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Model chưa được train.\")\n",
    "        \n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        z = X @ self.weights + self.bias\n",
    "        prob_class_1 = self._sigmoid(z)\n",
    "        \n",
    "        # Stack probabilities using column_stack\n",
    "        return np.column_stack([1 - prob_class_1, prob_class_1])\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Predict classes - VECTORIZED\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        # Vectorized comparison and casting\n",
    "        return (probabilities[:, 1] >= threshold).astype(np.int32)\n",
    "    \n",
    "    def get_decision_scores(self, X):\n",
    "        \"\"\"Get raw decision scores - VECTORIZED\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Model chưa được train.\")\n",
    "        \n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        return X @ self.weights + self.bias\n",
    "\n",
    "print(\"Logistic Regression implementation completed (fully vectorized)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde95954",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes (Pure NumPy - Fully Vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes - Pure NumPy Implementation\n",
    "    \n",
    "    VECTORIZATION FEATURES:\n",
    "    - No for loops for probability calculations\n",
    "    - Broadcasting for efficient Gaussian PDF computation\n",
    "    - Fancy indexing for class-wise statistics\n",
    "    - np.einsum for log-likelihood computation\n",
    "    - Memory-efficient operations\n",
    "    - Numerical stability with log probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, var_smoothing=1e-9, verbose=False):\n",
    "        self.var_smoothing = var_smoothing  # Laplace smoothing for variance\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.classes_ = None\n",
    "        self.class_priors_ = None\n",
    "        self.means_ = None\n",
    "        self.vars_ = None\n",
    "        self.fitted = False\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train Naive Bayes - FULLY VECTORIZED\n",
    "        Uses fancy indexing and broadcasting\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        y = np.asarray(y, dtype=np.float64)\n",
    "        \n",
    "        m, n = X.shape\n",
    "        \n",
    "        # Get unique classes (sorted for consistency)\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Training Gaussian Naive Bayes...\")\n",
    "            print(f\"  Samples: {m}, Features: {n}\")\n",
    "            print(f\"  Classes: {n_classes}\")\n",
    "        \n",
    "        # Pre-allocate arrays for efficiency\n",
    "        self.means_ = np.zeros((n_classes, n), dtype=np.float64)\n",
    "        self.vars_ = np.zeros((n_classes, n), dtype=np.float64)\n",
    "        self.class_priors_ = np.zeros(n_classes, dtype=np.float64)\n",
    "        \n",
    "        # Vectorized computation for each class using fancy indexing\n",
    "        for idx, class_label in enumerate(self.classes_):\n",
    "            # Boolean mask for current class - VECTORIZED\n",
    "            class_mask = (y == class_label)\n",
    "            \n",
    "            # Fancy indexing to get class samples - NO LOOP\n",
    "            X_class = X[class_mask]\n",
    "            \n",
    "            # Vectorized statistics computation using broadcasting\n",
    "            self.means_[idx] = np.mean(X_class, axis=0)\n",
    "            self.vars_[idx] = np.var(X_class, axis=0) + self.var_smoothing\n",
    "            \n",
    "            # Class prior probability\n",
    "            self.class_priors_[idx] = np.sum(class_mask) / m\n",
    "        \n",
    "        self.fitted = True\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"  Training completed!\")\n",
    "            print(f\"  Class priors: {self.class_priors_}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _compute_log_likelihood(self, X):\n",
    "        \"\"\"\n",
    "        Compute log likelihood for all classes - FULLY VECTORIZED\n",
    "        Uses broadcasting and einsum for efficiency\n",
    "        \n",
    "        Returns: shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        # X shape: (n_samples, n_features)\n",
    "        # means_ shape: (n_classes, n_features)\n",
    "        # vars_ shape: (n_classes, n_features)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        # Pre-compute constants for numerical stability\n",
    "        log_2pi = np.log(2 * np.pi)\n",
    "        \n",
    "        # Vectorized log likelihood computation using broadcasting\n",
    "        log_likelihood = np.zeros((n_samples, n_classes), dtype=np.float64)\n",
    "        \n",
    "        for class_idx in range(n_classes):\n",
    "            # Vectorized computation for each class\n",
    "            diff = X - self.means_[class_idx]  # Broadcasting: (n_samples, n_features)\n",
    "            \n",
    "            # Log of Gaussian PDF (vectorized)\n",
    "            # -0.5 * [log(2π) + log(σ²) + (x-μ)²/σ²]\n",
    "            log_var = np.log(self.vars_[class_idx])  # Shape: (n_features,)\n",
    "            \n",
    "            # Efficient squared Mahalanobis distance using einsum\n",
    "            mahal_dist = np.einsum('ij,j,ij->i', diff, 1.0/self.vars_[class_idx], diff)\n",
    "            \n",
    "            # Sum over features (vectorized)\n",
    "            log_likelihood[:, class_idx] = -0.5 * (\n",
    "                n_features * log_2pi + \n",
    "                np.sum(log_var) + \n",
    "                mahal_dist\n",
    "            )\n",
    "        \n",
    "        return log_likelihood\n",
    "    \n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"Predict log probabilities - VECTORIZED\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Model chưa được train.\")\n",
    "        \n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        \n",
    "        # Compute log likelihood (vectorized)\n",
    "        log_likelihood = self._compute_log_likelihood(X)\n",
    "        \n",
    "        # Add log prior (broadcasting)\n",
    "        log_priors = np.log(self.class_priors_)\n",
    "        log_posterior = log_likelihood + log_priors  # Broadcasting\n",
    "        \n",
    "        # Normalize using log-sum-exp trick for numerical stability\n",
    "        log_sum_exp = np.logaddexp.reduce(log_posterior, axis=1, keepdims=True)\n",
    "        log_proba = log_posterior - log_sum_exp\n",
    "        \n",
    "        return log_proba\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities - VECTORIZED\"\"\"\n",
    "        log_proba = self.predict_log_proba(X)\n",
    "        return np.exp(log_proba)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes - VECTORIZED\"\"\"\n",
    "        log_proba = self.predict_log_proba(X)\n",
    "        \n",
    "        # Vectorized argmax to find most likely class\n",
    "        class_indices = np.argmax(log_proba, axis=1)\n",
    "        \n",
    "        # Map indices back to class labels using fancy indexing\n",
    "        return self.classes_[class_indices]\n",
    "\n",
    "print(\"Gaussian Naive Bayes implementation completed (fully vectorized)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0afce48",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics (Fully Vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23cfb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute confusion matrix - VECTORIZED\n",
    "    Uses fancy indexing and boolean operations\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    \n",
    "    # Vectorized boolean operations - NO LOOPS\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    return np.array([[tn, fp], [fn, tp]], dtype=np.int32)\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute comprehensive metrics - FULLY VECTORIZED\n",
    "    All calculations use vectorized operations\n",
    "    \"\"\"\n",
    "    cm = compute_confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Vectorized metric calculations\n",
    "    total = tp + tn + fp + fn\n",
    "    accuracy = (tp + tn) / total if total > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'confusion_matrix': cm,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        'f1_score': f1,\n",
    "        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
    "    }\n",
    "\n",
    "def compute_mse_mae(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute MSE and MAE - VECTORIZED using einsum\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    \n",
    "    # Vectorized error computation\n",
    "    error = y_true - y_pred\n",
    "    \n",
    "    # MSE using einsum for efficiency\n",
    "    mse = np.einsum('i,i->', error, error) / len(error)\n",
    "    \n",
    "    # MAE using ufunc\n",
    "    mae = np.mean(np.abs(error))\n",
    "    \n",
    "    # RMSE\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # R² score\n",
    "    ss_res = np.einsum('i,i->', error, error)\n",
    "    ss_tot = np.einsum('i,i->', (y_true - np.mean(y_true)), (y_true - np.mean(y_true)))\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2_score': r2\n",
    "    }\n",
    "\n",
    "def print_classification_metrics(metrics, title=\"Classification Results\"):\n",
    "    \"\"\"Print classification metrics\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{title:^70}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    cm = metrics['confusion_matrix']\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"                Predicted\")\n",
    "    print(f\"              Normal  Fraud\")\n",
    "    print(f\"Actual Normal {cm[0,0]:>6}  {cm[0,1]:>5}\")\n",
    "    print(f\"       Fraud  {cm[1,0]:>6}  {cm[1,1]:>5}\")\n",
    "    \n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"  Accuracy:   {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision:  {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:     {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:   {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  Specificity: {metrics['specificity']:.4f}\")\n",
    "\n",
    "def print_regression_metrics(metrics, title=\"Regression Results\"):\n",
    "    \"\"\"Print regression metrics\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{title:^70}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"  MSE:  {metrics['mse']:.6f}\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.6f}\")\n",
    "    print(f\"  MAE:  {metrics['mae']:.6f}\")\n",
    "    print(f\"  R²:   {metrics['r2_score']:.6f}\")\n",
    "\n",
    "print(\"Evaluation metrics implementation completed (fully vectorized)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd95674",
   "metadata": {},
   "source": [
    "## 5. Train All Three Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f6be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING ALL THREE MODELS\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store results for comparison\n",
    "results = {}\n",
    "\n",
    "# ==================== MODEL 1: LINEAR REGRESSION ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 1: LINEAR REGRESSION (for comparison with classification)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "# Train Linear Regression\n",
    "linear_model = LinearRegression(\n",
    "    learning_rate=0.01,\n",
    "    max_iterations=1000,\n",
    "    tolerance=1e-6,\n",
    "    regularization_strength=0.01,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "linear_model.fit(X_train_scaled, y_train)\n",
    "train_time = time() - start_time\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_linear = linear_model.predict(X_train_scaled)\n",
    "y_test_pred_linear = linear_model.predict(X_test_scaled)\n",
    "\n",
    "# Convert to binary for classification metrics (threshold at 0.5)\n",
    "y_train_pred_linear_binary = (y_train_pred_linear >= 0.5).astype(np.int32)\n",
    "y_test_pred_linear_binary = (y_test_pred_linear >= 0.5).astype(np.int32)\n",
    "\n",
    "# Compute metrics\n",
    "train_metrics_linear = compute_metrics(y_train, y_train_pred_linear_binary)\n",
    "test_metrics_linear = compute_metrics(y_test, y_test_pred_linear_binary)\n",
    "regression_metrics = compute_mse_mae(y_test, y_test_pred_linear)\n",
    "\n",
    "print(f\"\\nTraining time: {train_time:.2f} seconds\")\n",
    "print_classification_metrics(test_metrics_linear, \"Linear Regression - TEST SET\")\n",
    "print_regression_metrics(regression_metrics, \"Linear Regression - Regression Metrics\")\n",
    "\n",
    "results['Linear Regression'] = {\n",
    "    'train_time': train_time,\n",
    "    'test_accuracy': test_metrics_linear['accuracy'],\n",
    "    'test_f1': test_metrics_linear['f1_score'],\n",
    "    'test_recall': test_metrics_linear['recall'],\n",
    "    'test_precision': test_metrics_linear['precision']\n",
    "}\n",
    "\n",
    "# ==================== MODEL 2: LOGISTIC REGRESSION ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 2: LOGISTIC REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate class weights for imbalanced data\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "class_weights = {\n",
    "    0: 1.0,\n",
    "    1: class_counts[0] / class_counts[1]  # Automatic weight calculation\n",
    "}\n",
    "\n",
    "print(f\"\\nClass weights: {class_weights}\")\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "# Train Logistic Regression\n",
    "logistic_model = LogisticRegression(\n",
    "    learning_rate=0.01,\n",
    "    max_iterations=1000,\n",
    "    tolerance=1e-6,\n",
    "    regularization_strength=0.1,\n",
    "    class_weights=class_weights,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "train_time = time() - start_time\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_logistic = logistic_model.predict(X_train_scaled, threshold=0.5)\n",
    "y_test_pred_logistic = logistic_model.predict(X_test_scaled, threshold=0.5)\n",
    "\n",
    "# Compute metrics\n",
    "train_metrics_logistic = compute_metrics(y_train, y_train_pred_logistic)\n",
    "test_metrics_logistic = compute_metrics(y_test, y_test_pred_logistic)\n",
    "\n",
    "print(f\"\\nTraining time: {train_time:.2f} seconds\")\n",
    "print_classification_metrics(test_metrics_logistic, \"Logistic Regression - TEST SET\")\n",
    "\n",
    "results['Logistic Regression'] = {\n",
    "    'train_time': train_time,\n",
    "    'test_accuracy': test_metrics_logistic['accuracy'],\n",
    "    'test_f1': test_metrics_logistic['f1_score'],\n",
    "    'test_recall': test_metrics_logistic['recall'],\n",
    "    'test_precision': test_metrics_logistic['precision']\n",
    "}\n",
    "\n",
    "# ==================== MODEL 3: NAIVE BAYES ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 3: GAUSSIAN NAIVE BAYES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "# Train Naive Bayes\n",
    "nb_model = GaussianNaiveBayes(\n",
    "    var_smoothing=1e-9,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "nb_model.fit(X_train_scaled, y_train)\n",
    "train_time = time() - start_time\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_nb = nb_model.predict(X_train_scaled)\n",
    "y_test_pred_nb = nb_model.predict(X_test_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "train_metrics_nb = compute_metrics(y_train, y_train_pred_nb)\n",
    "test_metrics_nb = compute_metrics(y_test, y_test_pred_nb)\n",
    "\n",
    "print(f\"\\nTraining time: {train_time:.2f} seconds\")\n",
    "print_classification_metrics(test_metrics_nb, \"Naive Bayes - TEST SET\")\n",
    "\n",
    "results['Naive Bayes'] = {\n",
    "    'train_time': train_time,\n",
    "    'test_accuracy': test_metrics_nb['accuracy'],\n",
    "    'test_f1': test_metrics_nb['f1_score'],\n",
    "    'test_recall': test_metrics_nb['recall'],\n",
    "    'test_precision': test_metrics_nb['precision']\n",
    "}\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc4abe8",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124d96b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison table using vectorized operations\n",
    "model_names = list(results.keys())\n",
    "metrics_names = ['test_accuracy', 'test_precision', 'test_recall', 'test_f1', 'train_time']\n",
    "metrics_labels = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Train Time (s)']\n",
    "\n",
    "print(f\"\\n{'Model':<25} {'Accuracy':>10} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Time(s)':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_results = results[model_name]\n",
    "    print(f\"{model_name:<25} \"\n",
    "          f\"{model_results['test_accuracy']:>10.4f} \"\n",
    "          f\"{model_results['test_precision']:>10.4f} \"\n",
    "          f\"{model_results['test_recall']:>10.4f} \"\n",
    "          f\"{model_results['test_f1']:>10.4f} \"\n",
    "          f\"{model_results['train_time']:>10.2f}\")\n",
    "\n",
    "# Find best model for each metric using vectorized argmax\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODELS BY METRIC\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "for metric_name, metric_label in zip(metrics_names[:-1], metrics_labels[:-1]):  # Exclude time\n",
    "    # Vectorized extraction of metric values\n",
    "    metric_values = np.array([results[model][metric_name] for model in model_names])\n",
    "    best_idx = np.argmax(metric_values)\n",
    "    best_model = model_names[best_idx]\n",
    "    best_value = metric_values[best_idx]\n",
    "    print(f\"  {metric_label:<15}: {best_model:<25} ({best_value:.4f})\")\n",
    "\n",
    "# Overall recommendation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATION FOR FRAUD DETECTION\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate weighted score (emphasize recall for fraud detection)\n",
    "weighted_scores = {}\n",
    "for model_name in model_names:\n",
    "    model_results = results[model_name]\n",
    "    # Weight: Recall > F1 > Precision > Accuracy (fraud detection priority)\n",
    "    weighted_score = (\n",
    "        0.4 * model_results['test_recall'] +\n",
    "        0.3 * model_results['test_f1'] +\n",
    "        0.2 * model_results['test_precision'] +\n",
    "        0.1 * model_results['test_accuracy']\n",
    "    )\n",
    "    weighted_scores[model_name] = weighted_score\n",
    "\n",
    "# Vectorized best model selection\n",
    "best_model = max(weighted_scores, key=weighted_scores.get)\n",
    "best_score = weighted_scores[best_model]\n",
    "\n",
    "print(f\"\\nBased on weighted scoring (Recall-focused):\")\n",
    "print(f\"  Best Model: {best_model}\")\n",
    "print(f\"  Weighted Score: {best_score:.4f}\")\n",
    "print(f\"\\nKey Strengths:\")\n",
    "if best_model == 'Logistic Regression':\n",
    "    print(f\"  - Balanced performance with class weights\")\n",
    "    print(f\"  - Good recall for fraud detection\")\n",
    "    print(f\"  - Interpretable coefficients\")\n",
    "elif best_model == 'Naive Bayes':\n",
    "    print(f\"  - Fast training and prediction\")\n",
    "    print(f\"  - Works well with independent features\")\n",
    "    print(f\"  - Good probabilistic interpretation\")\n",
    "else:\n",
    "    print(f\"  - Simple baseline model\")\n",
    "    print(f\"  - Linear relationship modeling\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76935583",
   "metadata": {},
   "source": [
    "## 7. Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3240a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Model Performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Model Comparison - Bar Chart\n",
    "ax1 = axes[0, 0]\n",
    "metrics_to_plot = ['test_accuracy', 'test_precision', 'test_recall', 'test_f1']\n",
    "metrics_labels_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.2\n",
    "\n",
    "for i, (metric, label) in enumerate(zip(metrics_to_plot, metrics_labels_plot)):\n",
    "    values = np.array([results[model][metric] for model in model_names])\n",
    "    ax1.bar(x + i * width, values, width, label=label)\n",
    "\n",
    "ax1.set_xlabel('Models', fontweight='bold')\n",
    "ax1.set_ylabel('Score', fontweight='bold')\n",
    "ax1.set_title('Model Performance Comparison', fontweight='bold', pad=15)\n",
    "ax1.set_xticks(x + width * 1.5)\n",
    "ax1.set_xticklabels(model_names, rotation=15, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Training Time Comparison\n",
    "ax2 = axes[0, 1]\n",
    "train_times = np.array([results[model]['train_time'] for model in model_names])\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "bars = ax2.bar(model_names, train_times, color=colors, alpha=0.7)\n",
    "ax2.set_ylabel('Time (seconds)', fontweight='bold')\n",
    "ax2.set_title('Training Time Comparison', fontweight='bold', pad=15)\n",
    "ax2.set_xticklabels(model_names, rotation=15, ha='right')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Confusion Matrix Heatmap - Logistic Regression\n",
    "ax3 = axes[1, 0]\n",
    "cm_logistic = test_metrics_logistic['confusion_matrix']\n",
    "im = ax3.imshow(cm_logistic, cmap='Blues', aspect='auto')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = ax3.text(j, i, cm_logistic[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "ax3.set_xticks([0, 1])\n",
    "ax3.set_yticks([0, 1])\n",
    "ax3.set_xticklabels(['Normal', 'Fraud'])\n",
    "ax3.set_yticklabels(['Normal', 'Fraud'])\n",
    "ax3.set_xlabel('Predicted', fontweight='bold')\n",
    "ax3.set_ylabel('Actual', fontweight='bold')\n",
    "ax3.set_title('Confusion Matrix - Logistic Regression', fontweight='bold', pad=15)\n",
    "plt.colorbar(im, ax=ax3)\n",
    "\n",
    "# 4. Recall vs Precision Trade-off\n",
    "ax4 = axes[1, 1]\n",
    "recalls = np.array([results[model]['test_recall'] for model in model_names])\n",
    "precisions = np.array([results[model]['test_precision'] for model in model_names])\n",
    "\n",
    "scatter = ax4.scatter(recalls, precisions, s=200, c=colors, alpha=0.6, edgecolors='black', linewidth=2)\n",
    "\n",
    "for i, model in enumerate(model_names):\n",
    "    ax4.annotate(model, (recalls[i], precisions[i]), \n",
    "                xytext=(5, 5), textcoords='offset points',\n",
    "                fontweight='bold', fontsize=9)\n",
    "\n",
    "ax4.set_xlabel('Recall (Fraud Detection Rate)', fontweight='bold')\n",
    "ax4.set_ylabel('Precision', fontweight='bold')\n",
    "ax4.set_title('Recall vs Precision Trade-off', fontweight='bold', pad=15)\n",
    "ax4.grid(alpha=0.3)\n",
    "ax4.set_xlim([0, 1.05])\n",
    "ax4.set_ylim([0, 1.05])\n",
    "\n",
    "# Add diagonal reference line\n",
    "ax4.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Balance line')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
