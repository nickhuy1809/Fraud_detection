{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95eaa528",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection - Enhanced Modeling\n",
    "\n",
    "## HỌ VÀ TÊN: Cao Tấn Hoàng Huy\n",
    "## MSSV: 23127051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b786e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo các thư viện cần thiết\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "# Setup for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c44cc9",
   "metadata": {},
   "source": [
    "## Load preprocessed data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436cd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load preprocessed data from memory, or process from scratch\n",
    "try:\n",
    "    # Check if data exists from previous notebook\n",
    "    print(\"Checking for preprocessed data from previous notebook...\")\n",
    "    print(f\"X_train_scaled: {X_train_scaled.shape}\")\n",
    "    print(f\"X_test_scaled:  {X_test_scaled.shape}\")\n",
    "    print(f\"X_train_poly:   {X_train_poly.shape}\")\n",
    "    print(f\"X_test_poly:    {X_test_poly.shape}\")\n",
    "    print(f\"y_train:        {y_train.shape}\")\n",
    "    print(f\"y_test:         {y_test.shape}\")\n",
    "    print(f\"✓ Preprocessed data loaded from previous notebook successfully!\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"Preprocessed data not found in memory. Loading and processing from CSV...\")\n",
    "    print(\"This will take a moment...\\n\")\n",
    "    \n",
    "    # ========== LOAD DATA FROM CSV ==========\n",
    "    print(\"Step 1/5: Loading data from CSV...\")\n",
    "    with open('../data/creditcard.csv', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    header = lines[0].strip().replace('\"', '').split(',')\n",
    "    data_list = []\n",
    "    \n",
    "    for i, line in enumerate(lines[1:], 1):\n",
    "        try:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            values = line.split(',')\n",
    "            float_values = [float(val.strip().strip('\"')) for val in values]\n",
    "            if len(float_values) == 31:\n",
    "                data_list.append(float_values)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    data = np.array(data_list, dtype=np.float64)\n",
    "    column_names = header\n",
    "    print(f\"   Data loaded: {data.shape}\")\n",
    "    \n",
    "    # ========== SPLIT FEATURES AND TARGET ==========\n",
    "    print(\"Step 2/5: Splitting features and target...\")\n",
    "    X = data[:, :-1]  # All columns except last\n",
    "    y = data[:, -1]   # Last column (Class)\n",
    "    print(f\"   X: {X.shape}, y: {y.shape}\")\n",
    "    \n",
    "    # ========== TRAIN-TEST SPLIT ==========\n",
    "    print(\"Step 3/5: Train-test split (80-20)...\")\n",
    "    np.random.seed(42)\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    split_idx = int(0.8 * n_samples)\n",
    "    train_indices = indices[:split_idx]\n",
    "    test_indices = indices[split_idx:]\n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "    print(f\"   Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    # ========== STANDARDIZATION ==========\n",
    "    print(\"Step 4/5: Standardizing features...\")\n",
    "    # Compute mean and std from training data\n",
    "    train_mean = np.mean(X_train, axis=0)\n",
    "    train_std = np.std(X_train, axis=0)\n",
    "    train_std[train_std == 0] = 1.0  # Avoid division by zero\n",
    "    \n",
    "    # Apply to both train and test\n",
    "    X_train_scaled = (X_train - train_mean) / train_std\n",
    "    X_test_scaled = (X_test - train_mean) / train_std\n",
    "    print(f\"   Scaled data ready\")\n",
    "    \n",
    "    # ========== CREATE POLYNOMIAL FEATURES ==========\n",
    "    print(\"Step 5/5: Creating polynomial features (degree 2)...\")\n",
    "    # We'll create a simple version with key features only\n",
    "    n_features = X_train_scaled.shape[1]\n",
    "    \n",
    "    # Select top features (Amount, V1-V10 as example)\n",
    "    key_features_idx = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 28]  # Time, V1-V10, Amount\n",
    "    X_train_key = X_train_scaled[:, key_features_idx]\n",
    "    X_test_key = X_test_scaled[:, key_features_idx]\n",
    "    \n",
    "    # Create polynomial: [original features, squared features, interaction terms]\n",
    "    X_train_squared = X_train_key ** 2\n",
    "    X_test_squared = X_test_key ** 2\n",
    "    \n",
    "    X_train_poly = np.hstack([X_train_scaled, X_train_squared])\n",
    "    X_test_poly = np.hstack([X_test_scaled, X_test_squared])\n",
    "    \n",
    "    print(f\"   Polynomial features: {X_train_poly.shape}\")\n",
    "    \n",
    "    print(\"\\n✓ All data loaded and preprocessed successfully!\")\n",
    "    print(f\"\\nFinal shapes:\")\n",
    "    print(f\"  X_train_scaled: {X_train_scaled.shape}\")\n",
    "    print(f\"  X_test_scaled:  {X_test_scaled.shape}\")\n",
    "    print(f\"  X_train_poly:   {X_train_poly.shape}\")\n",
    "    print(f\"  X_test_poly:    {X_test_poly.shape}\")\n",
    "    print(f\"  y_train:        {y_train.shape}\")\n",
    "    print(f\"  y_test:         {y_test.shape}\")\n",
    "\n",
    "# Class distribution\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "print(f\"\\nClass distribution in training:\")\n",
    "for cls, count in zip(unique_train, counts_train):\n",
    "    pct = (count / len(y_train)) * 100\n",
    "    label = \"Normal\" if cls == 0 else \"Fraud\"\n",
    "    print(f\"  {label}: {count:>6,} ({pct:>5.2f}%)\")\n",
    "\n",
    "imbalance_ratio = counts_train[0] / counts_train[1] if len(counts_train) == 2 else None\n",
    "print(f\"  Imbalance ratio: {imbalance_ratio:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f14501",
   "metadata": {},
   "source": [
    "# THREE MODELS IMPLEMENTATION - PURE NUMPY\n",
    "\n",
    "## Requirements:\n",
    "- Vectorization (NO for loops on arrays)\n",
    "- Broadcasting efficiency\n",
    "- Universal functions (ufuncs)\n",
    "- Fancy indexing & masking\n",
    "- Array manipulation (reshape, transpose, stack, etc.)\n",
    "- Memory-efficient operations\n",
    "- np.einsum for complex calculations\n",
    "- Numerical stability\n",
    "- Clean, efficient, reproducible code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e720c0ee",
   "metadata": {},
   "source": [
    "## 1. Linear Regression (Pure NumPy - Fully Vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cfb5d8",
   "metadata": {},
   "source": [
    "### THUẬT TOÁN LINEAR REGRESSION\n",
    "\n",
    "**Mô tả:** Linear Regression dự đoán giá trị liên tục bằng cách tìm đường thẳng (hoặc hyperplane) fit tốt nhất với dữ liệu.\n",
    "\n",
    "**Công thức chính:**\n",
    "```\n",
    "ŷ = X · w + b\n",
    "```\n",
    "Trong đó:\n",
    "- `X`: Ma trận features (m samples × n features)\n",
    "- `w`: Vector weights (n features)\n",
    "- `b`: Bias (scalar)\n",
    "- `ŷ`: Predictions\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 1: INITIALIZATION (Khởi tạo parameters)**\n",
    "\n",
    "```python\n",
    "# Xavier Initialization cho numerical stability\n",
    "limit = sqrt(6.0 / (n_features + 1))\n",
    "weights = random.uniform(-limit, limit, n_features)\n",
    "bias = 0.0\n",
    "```\n",
    "\n",
    "**Mục đích:** \n",
    "- Random weights trong khoảng hợp lý\n",
    "- Tránh exploding/vanishing gradients\n",
    "- Xavier initialization phù hợp với linear activation\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 2: FORWARD PASS (Tính predictions)**\n",
    "\n",
    "```python\n",
    "# Vectorized computation - NO LOOPS\n",
    "y_pred = X @ weights + bias\n",
    "```\n",
    "\n",
    "**Giải thích:**\n",
    "- `X @ weights`: Matrix multiplication (m×n) @ (n,) = (m,)\n",
    "- `+ bias`: Broadcasting - cộng bias vào mọi prediction\n",
    "- Kết quả: Array (m,) chứa predictions cho m samples\n",
    "\n",
    "**Ví dụ:**\n",
    "```\n",
    "X.shape = (1000, 30)  # 1000 samples, 30 features\n",
    "weights.shape = (30,)\n",
    "y_pred.shape = (1000,)  # 1000 predictions\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 3: COMPUTE COST (Tính loss function)**\n",
    "\n",
    "**Loss Function: MSE + L2 Regularization**\n",
    "\n",
    "```python\n",
    "# Mean Squared Error\n",
    "squared_error = (y_pred - y) ** 2\n",
    "mse = sum(squared_error) / (2 * m)\n",
    "\n",
    "# L2 Regularization (Ridge)\n",
    "l2_penalty = (λ / 2) * sum(weights ** 2)\n",
    "\n",
    "# Total Cost\n",
    "cost = mse + l2_penalty\n",
    "```\n",
    "\n",
    "**Vectorized với einsum (hiệu quả hơn):**\n",
    "```python\n",
    "error = y_pred - y\n",
    "squared_error = np.einsum('i,i->', error, error)  # Dot product\n",
    "mse = squared_error / (2 * m)\n",
    "l2_penalty = (λ / 2) * np.einsum('i,i->', weights, weights)\n",
    "cost = mse + l2_penalty\n",
    "```\n",
    "\n",
    "**Tại sao L2 Regularization?**\n",
    "- Tránh overfitting\n",
    "- Penalize weights có giá trị lớn\n",
    "- Giúp model generalize tốt hơn\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 4: BACKWARD PASS (Tính gradients)**\n",
    "\n",
    "**Gradient của Cost theo weights và bias:**\n",
    "\n",
    "```python\n",
    "error = y_pred - y  # Shape: (m,)\n",
    "\n",
    "# Gradient của weights\n",
    "dw = (1/m) * X.T @ error + λ * weights\n",
    "# Giải thích: \n",
    "# - X.T @ error: (n, m) @ (m,) = (n,) - gradient từ MSE\n",
    "# - λ * weights: gradient từ L2 regularization\n",
    "\n",
    "# Gradient của bias\n",
    "db = (1/m) * sum(error)\n",
    "```\n",
    "\n",
    "**Vectorized với einsum:**\n",
    "```python\n",
    "dw = (1/m) * np.einsum('ij,i->j', X, error) + λ * weights\n",
    "# 'ij,i->j' nghĩa: sum over i (samples), keep j (features)\n",
    "```\n",
    "\n",
    "**Công thức toán học:**\n",
    "```\n",
    "∂J/∂w = (1/m) * X^T · (ŷ - y) + λ · w\n",
    "∂J/∂b = (1/m) * sum(ŷ - y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 5: PARAMETER UPDATE (Gradient Descent)**\n",
    "\n",
    "```python\n",
    "# Update weights và bias theo hướng ngược với gradient\n",
    "weights = weights - learning_rate * dw\n",
    "bias = bias - learning_rate * db\n",
    "```\n",
    "\n",
    "**Mục đích:**\n",
    "- Di chuyển weights theo hướng giảm cost\n",
    "- Learning rate (α) điều chỉnh tốc độ học\n",
    "- Nếu α quá lớn → overshooting, α quá nhỏ → học chậm\n",
    "\n",
    "**Ví dụ:**\n",
    "```\n",
    "weights = [0.5, -0.3, 0.8]\n",
    "dw = [0.1, -0.05, 0.2]\n",
    "learning_rate = 0.01\n",
    "\n",
    "weights_new = [0.5 - 0.01*0.1, -0.3 - 0.01*(-0.05), 0.8 - 0.01*0.2]\n",
    "            = [0.499, -0.2995, 0.798]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 6: CONVERGENCE CHECK (Kiểm tra hội tụ)**\n",
    "\n",
    "```python\n",
    "if abs(previous_cost - current_cost) < tolerance:\n",
    "    # Cost không thay đổi nhiều → đã hội tụ\n",
    "    break\n",
    "```\n",
    "\n",
    "**3 điều kiện dừng:**\n",
    "1. Cost thay đổi < tolerance (đã hội tụ)\n",
    "2. Đạt max_iterations (tránh chạy vô hạn)\n",
    "3. Cost tăng liên tục (learning rate quá lớn)\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 7: PREDICTION (Dự đoán cho dữ liệu mới)**\n",
    "\n",
    "```python\n",
    "# Sử dụng weights đã học\n",
    "y_pred = X_new @ weights + bias\n",
    "\n",
    "# Convert sang binary cho classification\n",
    "y_binary = (y_pred >= 0.5).astype(int)\n",
    "# >= 0.5 → Class 1 (Fraud)\n",
    "# < 0.5  → Class 0 (Normal)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Linear Regression - Pure NumPy Implementation\n",
    "    \n",
    "    VECTORIZATION FEATURES:\n",
    "    - No for loops for array operations\n",
    "    - Broadcasting for efficient computation\n",
    "    - np.linalg for optimized matrix operations\n",
    "    - Memory-efficient gradient computation\n",
    "    - Numerical stability with regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000, \n",
    "                 tolerance=1e-6, regularization_strength=0.01, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.lambda_ = regularization_strength  # L2 regularization\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "        self.fitted = False\n",
    "    \n",
    "    def _compute_cost(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute MSE cost with L2 regularization - FULLY VECTORIZED\n",
    "        Uses broadcasting and np.einsum for efficiency\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Vectorized prediction: y_pred = X @ weights + bias (broadcasting)\n",
    "        y_pred = X @ self.weights + self.bias\n",
    "        \n",
    "        # Vectorized MSE using einsum for efficient squared error\n",
    "        squared_error = np.einsum('i,i->', (y_pred - y), (y_pred - y))\n",
    "        mse = squared_error / (2 * m)\n",
    "        \n",
    "        # L2 regularization term (vectorized)\n",
    "        l2_penalty = (self.lambda_ / 2) * np.einsum('i,i->', self.weights, self.weights)\n",
    "        \n",
    "        return mse + l2_penalty\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train using Gradient Descent - FULLY VECTORIZED\n",
    "        NO loops for array operations, only iteration loop\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        y = np.asarray(y, dtype=np.float64)\n",
    "        \n",
    "        m, n = X.shape\n",
    "        \n",
    "        # Xavier initialization for numerical stability\n",
    "        limit = np.sqrt(6.0 / (n + 1))\n",
    "        self.weights = np.random.uniform(-limit, limit, n)\n",
    "        self.bias = 0.0\n",
    "        self.cost_history = []\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Training Linear Regression...\")\n",
    "            print(f\"  Samples: {m}, Features: {n}\")\n",
    "            print(f\"  Regularization (λ): {self.lambda_}\")\n",
    "        \n",
    "        prev_cost = float('inf')\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            # VECTORIZED Forward pass using broadcasting\n",
    "            y_pred = X @ self.weights + self.bias  # Shape: (m,)\n",
    "            \n",
    "            # VECTORIZED Gradient computation using einsum\n",
    "            error = y_pred - y  # Shape: (m,)\n",
    "            \n",
    "            # Efficient gradient computation\n",
    "            # dw = (1/m) * X.T @ error + λ * weights\n",
    "            dw = (1/m) * np.einsum('ij,i->j', X, error) + self.lambda_ * self.weights\n",
    "            db = (1/m) * np.sum(error)\n",
    "            \n",
    "            # VECTORIZED Parameter update\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Cost tracking\n",
    "            cost = self._compute_cost(X, y)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Convergence check\n",
    "            if abs(prev_cost - cost) < self.tolerance:\n",
    "                if self.verbose:\n",
    "                    print(f\"  Converged at iteration {iteration + 1}\")\n",
    "                break\n",
    "            prev_cost = cost\n",
    "            \n",
    "            if self.verbose and (iteration + 1) % 100 == 0:\n",
    "                print(f\"  Iteration {iteration + 1:>4}: Cost = {cost:.6f}\")\n",
    "        \n",
    "        self.fitted = True\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"  Training completed! Final cost: {self.cost_history[-1]:.6f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict - VECTORIZED using broadcasting\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Model chưa được train. Hãy gọi fit() trước.\")\n",
    "        \n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        return X @ self.weights + self.bias\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Calculate R² score - VECTORIZED\"\"\"\n",
    "        y = np.asarray(y, dtype=np.float64)\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        # Vectorized R² calculation using einsum\n",
    "        ss_res = np.einsum('i,i->', (y - y_pred), (y - y_pred))\n",
    "        ss_tot = np.einsum('i,i->', (y - np.mean(y)), (y - np.mean(y)))\n",
    "        \n",
    "        return 1 - (ss_res / ss_tot) if ss_tot != 0 else 0.0\n",
    "\n",
    "print(\"Linear Regression implementation completed (fully vectorized)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474a2b5",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression (Pure NumPy - Fully Vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab30f21",
   "metadata": {},
   "source": [
    "### THUẬT TOÁN LOGISTIC REGRESSION\n",
    "\n",
    "**Mô tả:** Logistic Regression là thuật toán classification dự đoán xác suất một sample thuộc class 1 (Fraud) sử dụng Sigmoid function.\n",
    "\n",
    "**Công thức chính:**\n",
    "```\n",
    "z = X · w + b\n",
    "ŷ = σ(z) = 1 / (1 + e^(-z))\n",
    "```\n",
    "\n",
    "Trong đó:\n",
    "- `z`: Linear combination (logits)\n",
    "- `σ`: Sigmoid function (chuyển z về xác suất 0-1)\n",
    "- `ŷ`: Xác suất dự đoán (0 = Normal, 1 = Fraud)\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 1: SIGMOID FUNCTION (Numerical Stability)**\n",
    "\n",
    "**Công thức:**\n",
    "```\n",
    "σ(z) = 1 / (1 + e^(-z))\n",
    "```\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "# Naive version - CÓ VẤN ĐỀ\n",
    "sigmoid = 1.0 / (1.0 + np.exp(-z))\n",
    "# Vấn đề: Nếu z quá lớn (>700) → exp(-z) = 0 → OK\n",
    "#         Nếu z quá nhỏ (<-700) → exp(-z) = infinity → ERROR!\n",
    "\n",
    "# Stable version - ĐÚNG\n",
    "z = np.clip(z, -500, 500)  # Clip trước khi exp\n",
    "sigmoid = 1.0 / (1.0 + np.exp(-z))\n",
    "```\n",
    "\n",
    "**Tại sao Sigmoid?**\n",
    "- Map giá trị bất kỳ (-∞, +∞) về (0, 1)\n",
    "- Output có thể hiểu như xác suất\n",
    "- Smooth, differentiable → dễ tính gradient\n",
    "- σ'(z) = σ(z) · (1 - σ(z)) (gradient đơn giản)\n",
    "\n",
    "**Đặc điểm:**\n",
    "```\n",
    "σ(0) = 0.5    (ngưỡng quyết định)\n",
    "σ(+∞) → 1     (chắc chắn là class 1)\n",
    "σ(-∞) → 0     (chắc chắn là class 0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 2: CLASS WEIGHTS (Xử lý Imbalanced Data)**\n",
    "\n",
    "**Vấn đề:**\n",
    "- Normal: 284,315 samples (99.83%)\n",
    "- Fraud: 492 samples (0.17%)\n",
    "- Model sẽ bias về Normal!\n",
    "\n",
    "**Giải pháp: Sample Weights**\n",
    "```python\n",
    "class_weights = {\n",
    "    0: 1.0,                    # Normal: weight = 1\n",
    "    1: 284315 / 492 = 577.7    # Fraud: weight = 577.7\n",
    "}\n",
    "```\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "# Tạo sample weights array\n",
    "sample_weights = np.zeros(m)\n",
    "for class_label, weight in class_weights.items():\n",
    "    mask = (y == class_label)  # Boolean mask\n",
    "    sample_weights[mask] = weight  # Fancy indexing\n",
    "\n",
    "# Kết quả:\n",
    "# - Normal samples: weight = 1.0\n",
    "# - Fraud samples: weight = 577.7\n",
    "```\n",
    "\n",
    "**Hiệu quả:**\n",
    "- Model penalty nhiều hơn khi predict sai Fraud\n",
    "- Mỗi fraud sample \"đóng góp\" bằng 577 normal samples\n",
    "- Cân bằng ảnh hưởng của 2 classes trong loss\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 3: FORWARD PASS (Tính predictions)**\n",
    "\n",
    "```python\n",
    "# Linear combination\n",
    "z = X @ weights + bias  # Shape: (m,)\n",
    "\n",
    "# Sigmoid activation\n",
    "y_pred = sigmoid(z)     # Shape: (m,), values in [0, 1]\n",
    "```\n",
    "\n",
    "**Giải thích:**\n",
    "```\n",
    "Sample 1: z = 2.5  → σ(2.5) = 0.924  → ~92% Fraud\n",
    "Sample 2: z = -1.0 → σ(-1.0) = 0.269 → ~27% Fraud\n",
    "Sample 3: z = 0.1  → σ(0.1) = 0.525  → ~52% Fraud\n",
    "```\n",
    "\n",
    "**Decision rule:**\n",
    "```python\n",
    "if y_pred >= 0.5: predict Class 1 (Fraud)\n",
    "else: predict Class 0 (Normal)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 4: COMPUTE COST (Binary Cross-Entropy + Weights)**\n",
    "\n",
    "**Loss Function:**\n",
    "```\n",
    "J = -1/m * Σ w_i[y_i·log(ŷ_i) + (1-y_i)·log(1-ŷ_i)] + λ/2·||w||²\n",
    "```\n",
    "\n",
    "**Giải thích từng phần:**\n",
    "\n",
    "**4.1. Binary Cross-Entropy:**\n",
    "```python\n",
    "# Clip để tránh log(0)\n",
    "epsilon = 1e-15\n",
    "y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "# Cross-entropy cho từng sample\n",
    "log_loss = -(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "```\n",
    "\n",
    "**Tại sao công thức này?**\n",
    "```\n",
    "Nếu y = 1 (Fraud):\n",
    "    loss = -log(ŷ)\n",
    "    - Nếu ŷ = 1 (predict đúng) → loss = 0 (tốt!)\n",
    "    - Nếu ŷ = 0 (predict sai)  → loss = ∞ (penalty lớn!)\n",
    "\n",
    "Nếu y = 0 (Normal):\n",
    "    loss = -log(1 - ŷ)\n",
    "    - Nếu ŷ = 0 (predict đúng) → loss = 0 (tốt!)\n",
    "    - Nếu ŷ = 1 (predict sai)  → loss = ∞ (penalty lớn!)\n",
    "```\n",
    "\n",
    "**4.2. Weighted Loss:**\n",
    "```python\n",
    "# Apply sample weights\n",
    "weighted_loss = np.einsum('i,i->', sample_weights, log_loss) / m\n",
    "# 'i,i->' nghĩa: element-wise multiply rồi sum\n",
    "```\n",
    "\n",
    "**4.3. L2 Regularization:**\n",
    "```python\n",
    "l2_penalty = (λ / 2) * np.einsum('i,i->', weights, weights)\n",
    "```\n",
    "\n",
    "**Total Cost:**\n",
    "```python\n",
    "cost = weighted_loss + l2_penalty\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 5: BACKWARD PASS (Tính gradients)**\n",
    "\n",
    "**Gradient computation:**\n",
    "```python\n",
    "# Error signal\n",
    "error = y_pred - y  # Shape: (m,)\n",
    "\n",
    "# Weighted error (incorporate sample weights)\n",
    "weighted_error = sample_weights * error  # Element-wise multiply\n",
    "\n",
    "# Gradients\n",
    "dw = (1/m) * X.T @ weighted_error + λ * weights\n",
    "#    ↑ Gradient từ weighted loss  ↑ Gradient từ L2\n",
    "\n",
    "db = (1/m) * sum(weighted_error)\n",
    "```\n",
    "\n",
    "**Vectorized với einsum:**\n",
    "```python\n",
    "dw = (1/m) * np.einsum('ij,i->j', X, weighted_error) + λ * weights\n",
    "```\n",
    "\n",
    "**Công thức toán học:**\n",
    "```\n",
    "∂J/∂w = (1/m) * X^T · (w_sample ⊙ (σ(X·w+b) - y)) + λ·w\n",
    "∂J/∂b = (1/m) * sum(w_sample ⊙ (σ(X·w+b) - y))\n",
    "```\n",
    "Trong đó `⊙` là element-wise multiplication.\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 6: PARAMETER UPDATE (Gradient Descent)**\n",
    "\n",
    "```python\n",
    "weights = weights - learning_rate * dw\n",
    "bias = bias - learning_rate * db\n",
    "```\n",
    "\n",
    "**Adaptive learning rate (optional):**\n",
    "```python\n",
    "# Learning rate decay\n",
    "if iteration % 100 == 0:\n",
    "    learning_rate *= 0.95  # Giảm 5% mỗi 100 iterations\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 7: CONVERGENCE CHECK**\n",
    "\n",
    "```python\n",
    "# Track cost history\n",
    "cost_history.append(cost)\n",
    "\n",
    "# Check convergence\n",
    "if abs(previous_cost - current_cost) < tolerance:\n",
    "    print(f\"Converged at iteration {iteration}\")\n",
    "    break\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 8: PREDICTION**\n",
    "\n",
    "**8.1. Predict Probabilities:**\n",
    "```python\n",
    "# Output xác suất cho cả 2 classes\n",
    "z = X_new @ weights + bias\n",
    "prob_fraud = sigmoid(z)         # P(y=1|X)\n",
    "prob_normal = 1 - prob_fraud    # P(y=0|X)\n",
    "\n",
    "probabilities = np.column_stack([prob_normal, prob_fraud])\n",
    "# Shape: (n_samples, 2)\n",
    "```\n",
    "\n",
    "**8.2. Predict Classes:**\n",
    "```python\n",
    "# Sử dụng threshold (mặc định 0.5)\n",
    "predictions = (prob_fraud >= threshold).astype(int)\n",
    "\n",
    "# Có thể điều chỉnh threshold:\n",
    "threshold = 0.3  # Sensitive hơn với fraud (recall cao hơn)\n",
    "predictions = (prob_fraud >= 0.3).astype(int)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90db0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression - Pure NumPy Implementation\n",
    "    \n",
    "    VECTORIZATION FEATURES:\n",
    "    - No for loops for array operations\n",
    "    - Broadcasting for efficient computation\n",
    "    - Numerically stable sigmoid implementation\n",
    "    - Vectorized cross-entropy loss\n",
    "    - Class weight support using fancy indexing\n",
    "    - np.einsum for efficient gradient computation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000, \n",
    "                 tolerance=1e-6, regularization_strength=0.1, \n",
    "                 class_weights=None, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.lambda_ = regularization_strength\n",
    "        self.class_weights = class_weights\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "        self.fitted = False\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Numerically stable sigmoid function - VECTORIZED\n",
    "        Uses np.clip to prevent overflow\n",
    "        \"\"\"\n",
    "        # Clip to prevent exp overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def _compute_sample_weights(self, y):\n",
    "        \"\"\"\n",
    "        Calculate sample weights - VECTORIZED using fancy indexing\n",
    "        NO loops over samples\n",
    "        \"\"\"\n",
    "        if self.class_weights is None:\n",
    "            return np.ones(len(y), dtype=np.float64)\n",
    "        \n",
    "        # Vectorized weight assignment using fancy indexing\n",
    "        sample_weights = np.zeros(len(y), dtype=np.float64)\n",
    "        for class_label, weight in self.class_weights.items():\n",
    "            # Boolean masking - vectorized\n",
    "            mask = (y == class_label)\n",
    "            sample_weights[mask] = weight\n",
    "        \n",
    "        return sample_weights\n",
    "    \n",
    "    def _compute_cost(self, X, y, sample_weights):\n",
    "        \"\"\"\n",
    "        Compute weighted binary cross-entropy - FULLY VECTORIZED\n",
    "        Uses numerical stability tricks and einsum\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Vectorized forward pass\n",
    "        z = X @ self.weights + self.bias\n",
    "        y_pred = self._sigmoid(z)\n",
    "        \n",
    "        # Numerical stability: clip predictions\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Vectorized cross-entropy using ufuncs\n",
    "        log_loss = -(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "        \n",
    "        # Weighted loss using einsum\n",
    "        weighted_loss = np.einsum('i,i->', sample_weights, log_loss) / m\n",
    "        \n",
    "        # L2 regularization\n",
    "        l2_penalty = (self.lambda_ / 2) * np.einsum('i,i->', self.weights, self.weights)\n",
    "        \n",
    "        return weighted_loss + l2_penalty\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train using Gradient Descent - FULLY VECTORIZED\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        y = np.asarray(y, dtype=np.float64)\n",
    "        \n",
    "        m, n = X.shape\n",
    "        \n",
    "        # Xavier initialization\n",
    "        limit = np.sqrt(6.0 / (n + 1))\n",
    "        self.weights = np.random.uniform(-limit, limit, n)\n",
    "        self.bias = 0.0\n",
    "        self.cost_history = []\n",
    "        \n",
    "        # Compute sample weights once (vectorized)\n",
    "        sample_weights = self._compute_sample_weights(y)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Training Logistic Regression...\")\n",
    "            print(f\"  Samples: {m}, Features: {n}\")\n",
    "            print(f\"  Regularization (λ): {self.lambda_}\")\n",
    "            if self.class_weights:\n",
    "                print(f\"  Class weights: {self.class_weights}\")\n",
    "        \n",
    "        prev_cost = float('inf')\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            # VECTORIZED Forward pass\n",
    "            z = X @ self.weights + self.bias\n",
    "            y_pred = self._sigmoid(z)\n",
    "            \n",
    "            # VECTORIZED Gradient computation\n",
    "            error = y_pred - y\n",
    "            \n",
    "            # Weighted gradients using einsum\n",
    "            weighted_error = sample_weights * error\n",
    "            dw = (1/m) * np.einsum('ij,i->j', X, weighted_error) + self.lambda_ * self.weights\n",
    "            db = (1/m) * np.sum(weighted_error)\n",
    "            \n",
    "            # VECTORIZED Parameter update\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Cost tracking\n",
    "            cost = self._compute_cost(X, y, sample_weights)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Convergence check\n",
    "            if abs(prev_cost - cost) < self.tolerance:\n",
    "                if self.verbose:\n",
    "                    print(f\"  Converged at iteration {iteration + 1}\")\n",
    "                break\n",
    "            prev_cost = cost\n",
    "            \n",
    "            if self.verbose and (iteration + 1) % 100 == 0:\n",
    "                print(f\"  Iteration {iteration + 1:>4}: Cost = {cost:.6f}\")\n",
    "        \n",
    "        self.fitted = True\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"  Training completed! Final cost: {self.cost_history[-1]:.6f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities - VECTORIZED\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Model chưa được train.\")\n",
    "        \n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        z = X @ self.weights + self.bias\n",
    "        prob_class_1 = self._sigmoid(z)\n",
    "        \n",
    "        # Stack probabilities using column_stack\n",
    "        return np.column_stack([1 - prob_class_1, prob_class_1])\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Predict classes - VECTORIZED\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        # Vectorized comparison and casting\n",
    "        return (probabilities[:, 1] >= threshold).astype(np.int32)\n",
    "    \n",
    "    def get_decision_scores(self, X):\n",
    "        \"\"\"Get raw decision scores - VECTORIZED\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Model chưa được train.\")\n",
    "        \n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        return X @ self.weights + self.bias\n",
    "\n",
    "print(\"Logistic Regression implementation completed (fully vectorized)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde95954",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes (Pure NumPy - Fully Vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5268d",
   "metadata": {},
   "source": [
    "### THUẬT TOÁN GAUSSIAN NAIVE BAYES\n",
    "\n",
    "**Mô tả:** Naive Bayes là thuật toán probabilistic dựa trên Bayes' Theorem, giả định các features độc lập với nhau (naive assumption).\n",
    "\n",
    "**Bayes' Theorem:**\n",
    "```\n",
    "P(y|X) = P(X|y) · P(y) / P(X)\n",
    "\n",
    "Posterior = Likelihood × Prior / Evidence\n",
    "```\n",
    "\n",
    "**Simplified (bỏ qua P(X) vì constant):**\n",
    "```\n",
    "P(y|X) ∝ P(X|y) · P(y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 1: UNDERSTANDING BAYES' THEOREM**\n",
    "\n",
    "**Ví dụ trực quan:**\n",
    "\n",
    "Giả sử có 1 giao dịch với Amount = $50:\n",
    "```\n",
    "P(Fraud | Amount=$50) = ?\n",
    "\n",
    "Cần tính:\n",
    "1. P(Fraud): Tỷ lệ fraud trong training data (prior)\n",
    "2. P(Amount=$50 | Fraud): Xác suất Amount=$50 khi là fraud (likelihood)\n",
    "3. P(Amount=$50 | Normal): Xác suất Amount=$50 khi là normal (likelihood)\n",
    "\n",
    "So sánh:\n",
    "- P(Fraud | Amount=$50) ∝ P(Amount=$50 | Fraud) × P(Fraud)\n",
    "- P(Normal | Amount=$50) ∝ P(Amount=$50 | Normal) × P(Normal)\n",
    "\n",
    "Chọn class có P cao hơn!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 2: GAUSSIAN ASSUMPTION**\n",
    "\n",
    "**Giả định:** Mỗi feature (với mỗi class) tuân theo phân phối Gaussian (Normal Distribution).\n",
    "\n",
    "**Công thức Gaussian PDF:**\n",
    "```\n",
    "P(x | y) = (1 / √(2π·σ²)) · exp(-(x - μ)² / (2σ²))\n",
    "```\n",
    "\n",
    "Trong đó:\n",
    "- `μ` (mu): Mean (trung bình)\n",
    "- `σ²` (sigma squared): Variance (phương sai)\n",
    "---\n",
    "\n",
    "#### **BƯỚC 3: NAIVE INDEPENDENCE ASSUMPTION**\n",
    "\n",
    "**\"Naive\" nghĩa gì?**\n",
    "\n",
    "Giả định: Các features ĐỘC LẬP với nhau khi biết class.\n",
    "\n",
    "```\n",
    "P(X | y) = P(x₁, x₂, ..., xₙ | y)\n",
    "         = P(x₁|y) · P(x₂|y) · ... · P(xₙ|y)  [Naive assumption]\n",
    "```\n",
    "\n",
    "**Ví dụ:**\n",
    "```\n",
    "X = [Time, Amount, V1, V2, ..., V28]\n",
    "\n",
    "P(X | Fraud) = P(Time|Fraud) × P(Amount|Fraud) × P(V1|Fraud) × ...\n",
    "```\n",
    "---\n",
    "\n",
    "#### **BƯỚC 4: TRAINING PHASE (Tính statistics)**\n",
    "\n",
    "**Mục tiêu:** Tính mean (μ) và variance (σ²) cho mỗi feature với mỗi class.\n",
    "\n",
    "```python\n",
    "classes = [0, 1]  # Normal, Fraud\n",
    "\n",
    "for each class:\n",
    "    # Step 4.1: Lọc samples của class này\n",
    "    class_mask = (y == class)  # Boolean array\n",
    "    X_class = X[class_mask]    # Fancy indexing\n",
    "    \n",
    "    # Step 4.2: Tính statistics (vectorized)\n",
    "    means[class] = np.mean(X_class, axis=0)  # Mean của mỗi feature\n",
    "    vars[class] = np.var(X_class, axis=0)    # Variance của mỗi feature\n",
    "    \n",
    "    # Step 4.3: Laplace smoothing\n",
    "    vars[class] += var_smoothing  # Tránh variance = 0\n",
    "    \n",
    "    # Step 4.4: Prior probability\n",
    "    class_priors[class] = len(X_class) / len(X)\n",
    "```\n",
    "---\n",
    "\n",
    "#### **BƯỚC 5: LOG LIKELIHOOD COMPUTATION (Tính xác suất)**\n",
    "\n",
    "**Tại sao dùng LOG?**\n",
    "\n",
    "**Vấn đề:** Xác suất rất nhỏ (10⁻⁵⁰) → underflow!\n",
    "```\n",
    "P(X|y) = P(x₁|y) × P(x₂|y) × ... × P(x₃₀|y)\n",
    "       = 0.01 × 0.02 × ... × 0.03\n",
    "       ≈ 10⁻⁵⁰ → UNDERFLOW!\n",
    "```\n",
    "\n",
    "**Giải pháp:** Dùng logarithm\n",
    "```\n",
    "log P(X|y) = log P(x₁|y) + log P(x₂|y) + ... + log P(x₃₀|y)\n",
    "           = -10 + (-8) + ... + (-12)\n",
    "           = -300 (OK! Không bị underflow)\n",
    "```\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "**5.1. Log Gaussian PDF:**\n",
    "```python\n",
    "# Công thức: log(Gaussian PDF)\n",
    "# = -0.5 * [log(2π) + log(σ²) + (x-μ)²/σ²]\n",
    "\n",
    "log_2pi = np.log(2 * np.pi)\n",
    "\n",
    "for each class:\n",
    "    # Vectorized computation cho tất cả features\n",
    "    diff = X - means[class]  # Broadcasting: (m, n) - (n,)\n",
    "    log_var = np.log(vars[class])\n",
    "    \n",
    "    # Mahalanobis distance (efficient với einsum)\n",
    "    mahal_dist = np.einsum('ij,j,ij->i', diff, 1/vars[class], diff)\n",
    "    # 'ij,j,ij->i': \n",
    "    #   - 'ij': X - means (m samples, n features)\n",
    "    #   - 'j': 1/variance (n features)\n",
    "    #   - 'ij': X - means (m samples, n features)\n",
    "    #   - '->i': sum over j, result (m samples)\n",
    "    \n",
    "    # Log likelihood\n",
    "    log_likelihood[class] = -0.5 * (\n",
    "        n_features * log_2pi +\n",
    "        np.sum(log_var) +\n",
    "        mahal_dist\n",
    "    )\n",
    "```\n",
    "\n",
    "**Giải thích `mahal_dist`:**\n",
    "```python\n",
    "# Thay vì:\n",
    "for i in range(m):  # for each sample\n",
    "    for j in range(n):  # for each feature\n",
    "        result[i] += ((X[i,j] - mean[j]) ** 2) / var[j]\n",
    "\n",
    "# Dùng einsum:\n",
    "mahal_dist = np.einsum('ij,j,ij->i', diff, 1/vars, diff)\n",
    "# → MUCH FASTER!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 6: POSTERIOR PROBABILITY (Kết hợp likelihood + prior)**\n",
    "\n",
    "```python\n",
    "# Log prior\n",
    "log_priors = np.log(class_priors)  # [log(P(Normal)), log(P(Fraud))]\n",
    "\n",
    "# Log posterior = log likelihood + log prior\n",
    "log_posterior = log_likelihood + log_priors  # Broadcasting\n",
    "# Shape: (m, 2) where 2 = n_classes\n",
    "```\n",
    "\n",
    "**Ví dụ cho 1 sample:**\n",
    "```\n",
    "Class: Normal\n",
    "    log_likelihood = -150.5\n",
    "    log_prior = log(0.9983) = -0.0017\n",
    "    log_posterior = -150.5 + (-0.0017) = -150.5017\n",
    "\n",
    "Class: Fraud\n",
    "    log_likelihood = -145.2\n",
    "    log_prior = log(0.0017) = -6.38\n",
    "    log_posterior = -145.2 + (-6.38) = -151.58\n",
    "\n",
    "→ Normal có log_posterior cao hơn → Predict Normal\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 7: NORMALIZE PROBABILITIES (Log-Sum-Exp Trick)**\n",
    "\n",
    "**Mục tiêu:** Convert log probabilities về probabilities thực.\n",
    "\n",
    "**Vấn đề:**\n",
    "```python\n",
    "# Naive way - CÓ VẤN ĐỀ\n",
    "proba = np.exp(log_posterior) / np.sum(np.exp(log_posterior))\n",
    "# exp(-150) ≈ 10⁻⁶⁵ → UNDERFLOW!\n",
    "```\n",
    "\n",
    "**Giải pháp: Log-Sum-Exp Trick**\n",
    "```python\n",
    "# Normalize trong log space trước\n",
    "log_sum_exp = np.logaddexp.reduce(log_posterior, axis=1, keepdims=True)\n",
    "log_proba = log_posterior - log_sum_exp\n",
    "\n",
    "# Convert về probability\n",
    "proba = np.exp(log_proba)\n",
    "```\n",
    "\n",
    "**np.logaddexp:**\n",
    "```python\n",
    "# Compute log(exp(a) + exp(b)) stably\n",
    "np.logaddexp(a, b) = log(exp(a) + exp(b))\n",
    "# Numerically stable even for large negative a, b\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **BƯỚC 8: PREDICTION**\n",
    "\n",
    "**8.1. Predict Classes:**\n",
    "```python\n",
    "# Find class với highest posterior probability\n",
    "class_indices = np.argmax(log_posterior, axis=1)\n",
    "# argmax in log space = argmax in probability space\n",
    "\n",
    "# Map indices back to class labels\n",
    "predictions = classes[class_indices]  # Fancy indexing\n",
    "```\n",
    "\n",
    "**8.2. Predict Probabilities:**\n",
    "```python\n",
    "# Return normalized probabilities\n",
    "probabilities = np.exp(log_proba)\n",
    "# Shape: (m, 2)\n",
    "# Column 0: P(Normal|X)\n",
    "# Column 1: P(Fraud|X)\n",
    "```\n",
    "\n",
    "**New transaction: Amount = $50**\n",
    "\n",
    "**Step 1: Likelihoods**\n",
    "```\n",
    "P(50 | Normal) = Gaussian(50, μ=25, σ²=125) = 0.0035\n",
    "P(50 | Fraud) = Gaussian(50, μ=150, σ²=2500) = 0.0079\n",
    "```\n",
    "\n",
    "**Step 2: Posteriors**\n",
    "```\n",
    "P(Normal | 50) ∝ 0.0035 × 0.8 = 0.0028\n",
    "P(Fraud | 50) ∝ 0.0079 × 0.2 = 0.0016\n",
    "```\n",
    "\n",
    "**Step 3: Normalize**\n",
    "```\n",
    "Total = 0.0028 + 0.0016 = 0.0044\n",
    "P(Normal | 50) = 0.0028 / 0.0044 = 0.636 (63.6%)\n",
    "P(Fraud | 50) = 0.0016 / 0.0044 = 0.364 (36.4%)\n",
    "```\n",
    "\n",
    "**Prediction: Normal** (vì 63.6% > 36.4%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes - Pure NumPy Implementation\n",
    "    \n",
    "    VECTORIZATION FEATURES:\n",
    "    - No for loops for probability calculations\n",
    "    - Broadcasting for efficient Gaussian PDF computation\n",
    "    - Fancy indexing for class-wise statistics\n",
    "    - np.einsum for log-likelihood computation\n",
    "    - Memory-efficient operations\n",
    "    - Numerical stability with log probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, var_smoothing=1e-9, verbose=False):\n",
    "        self.var_smoothing = var_smoothing  # Laplace smoothing for variance\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.classes_ = None\n",
    "        self.class_priors_ = None\n",
    "        self.means_ = None\n",
    "        self.vars_ = None\n",
    "        self.fitted = False\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train Naive Bayes - FULLY VECTORIZED\n",
    "        Uses fancy indexing and broadcasting\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        y = np.asarray(y, dtype=np.float64)\n",
    "        \n",
    "        m, n = X.shape\n",
    "        \n",
    "        # Get unique classes (sorted for consistency)\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Training Gaussian Naive Bayes...\")\n",
    "            print(f\"  Samples: {m}, Features: {n}\")\n",
    "            print(f\"  Classes: {n_classes}\")\n",
    "        \n",
    "        # Pre-allocate arrays for efficiency\n",
    "        self.means_ = np.zeros((n_classes, n), dtype=np.float64)\n",
    "        self.vars_ = np.zeros((n_classes, n), dtype=np.float64)\n",
    "        self.class_priors_ = np.zeros(n_classes, dtype=np.float64)\n",
    "        \n",
    "        # Vectorized computation for each class using fancy indexing\n",
    "        for idx, class_label in enumerate(self.classes_):\n",
    "            # Boolean mask for current class - VECTORIZED\n",
    "            class_mask = (y == class_label)\n",
    "            \n",
    "            # Fancy indexing to get class samples - NO LOOP\n",
    "            X_class = X[class_mask]\n",
    "            \n",
    "            # Vectorized statistics computation using broadcasting\n",
    "            self.means_[idx] = np.mean(X_class, axis=0)\n",
    "            self.vars_[idx] = np.var(X_class, axis=0) + self.var_smoothing\n",
    "            \n",
    "            # Class prior probability\n",
    "            self.class_priors_[idx] = np.sum(class_mask) / m\n",
    "        \n",
    "        self.fitted = True\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"  Training completed!\")\n",
    "            print(f\"  Class priors: {self.class_priors_}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _compute_log_likelihood(self, X):\n",
    "        \"\"\"\n",
    "        Compute log likelihood for all classes - FULLY VECTORIZED\n",
    "        Uses broadcasting and einsum for efficiency\n",
    "        \n",
    "        Returns: shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        # X shape: (n_samples, n_features)\n",
    "        # means_ shape: (n_classes, n_features)\n",
    "        # vars_ shape: (n_classes, n_features)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        # Pre-compute constants for numerical stability\n",
    "        log_2pi = np.log(2 * np.pi)\n",
    "        \n",
    "        # Vectorized log likelihood computation using broadcasting\n",
    "        log_likelihood = np.zeros((n_samples, n_classes), dtype=np.float64)\n",
    "        \n",
    "        for class_idx in range(n_classes):\n",
    "            # Vectorized computation for each class\n",
    "            diff = X - self.means_[class_idx]  # Broadcasting: (n_samples, n_features)\n",
    "            \n",
    "            # Log of Gaussian PDF (vectorized)\n",
    "            # -0.5 * [log(2π) + log(σ²) + (x-μ)²/σ²]\n",
    "            log_var = np.log(self.vars_[class_idx])  # Shape: (n_features,)\n",
    "            \n",
    "            # Efficient squared Mahalanobis distance using einsum\n",
    "            mahal_dist = np.einsum('ij,j,ij->i', diff, 1.0/self.vars_[class_idx], diff)\n",
    "            \n",
    "            # Sum over features (vectorized)\n",
    "            log_likelihood[:, class_idx] = -0.5 * (\n",
    "                n_features * log_2pi + \n",
    "                np.sum(log_var) + \n",
    "                mahal_dist\n",
    "            )\n",
    "        \n",
    "        return log_likelihood\n",
    "    \n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"Predict log probabilities - VECTORIZED\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Model chưa được train.\")\n",
    "        \n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        \n",
    "        # Compute log likelihood (vectorized)\n",
    "        log_likelihood = self._compute_log_likelihood(X)\n",
    "        \n",
    "        # Add log prior (broadcasting)\n",
    "        log_priors = np.log(self.class_priors_)\n",
    "        log_posterior = log_likelihood + log_priors  # Broadcasting\n",
    "        \n",
    "        # Normalize using log-sum-exp trick for numerical stability\n",
    "        log_sum_exp = np.logaddexp.reduce(log_posterior, axis=1, keepdims=True)\n",
    "        log_proba = log_posterior - log_sum_exp\n",
    "        \n",
    "        return log_proba\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities - VECTORIZED\"\"\"\n",
    "        log_proba = self.predict_log_proba(X)\n",
    "        return np.exp(log_proba)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes - VECTORIZED\"\"\"\n",
    "        log_proba = self.predict_log_proba(X)\n",
    "        \n",
    "        # Vectorized argmax to find most likely class\n",
    "        class_indices = np.argmax(log_proba, axis=1)\n",
    "        \n",
    "        # Map indices back to class labels using fancy indexing\n",
    "        return self.classes_[class_indices]\n",
    "\n",
    "print(\"Gaussian Naive Bayes implementation completed (fully vectorized)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0afce48",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics (Fully Vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23cfb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute confusion matrix - VECTORIZED\n",
    "    Uses fancy indexing and boolean operations\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    \n",
    "    # Vectorized boolean operations - NO LOOPS\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    return np.array([[tn, fp], [fn, tp]], dtype=np.int32)\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute comprehensive metrics - FULLY VECTORIZED\n",
    "    All calculations use vectorized operations\n",
    "    \"\"\"\n",
    "    cm = compute_confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Vectorized metric calculations\n",
    "    total = tp + tn + fp + fn\n",
    "    accuracy = (tp + tn) / total if total > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'confusion_matrix': cm,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        'f1_score': f1,\n",
    "        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
    "    }\n",
    "\n",
    "def compute_mse_mae(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute MSE and MAE - VECTORIZED using einsum\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    \n",
    "    # Vectorized error computation\n",
    "    error = y_true - y_pred\n",
    "    \n",
    "    # MSE using einsum for efficiency\n",
    "    mse = np.einsum('i,i->', error, error) / len(error)\n",
    "    \n",
    "    # MAE using ufunc\n",
    "    mae = np.mean(np.abs(error))\n",
    "    \n",
    "    # RMSE\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # R² score\n",
    "    ss_res = np.einsum('i,i->', error, error)\n",
    "    ss_tot = np.einsum('i,i->', (y_true - np.mean(y_true)), (y_true - np.mean(y_true)))\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2_score': r2\n",
    "    }\n",
    "\n",
    "def print_classification_metrics(metrics, title=\"Classification Results\"):\n",
    "    \"\"\"Print classification metrics\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{title:^70}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    cm = metrics['confusion_matrix']\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"                Predicted\")\n",
    "    print(f\"              Normal  Fraud\")\n",
    "    print(f\"Actual Normal {cm[0,0]:>6}  {cm[0,1]:>5}\")\n",
    "    print(f\"       Fraud  {cm[1,0]:>6}  {cm[1,1]:>5}\")\n",
    "    \n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"  Accuracy:   {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision:  {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:     {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:   {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  Specificity: {metrics['specificity']:.4f}\")\n",
    "\n",
    "def print_regression_metrics(metrics, title=\"Regression Results\"):\n",
    "    \"\"\"Print regression metrics\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{title:^70}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"  MSE:  {metrics['mse']:.6f}\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.6f}\")\n",
    "    print(f\"  MAE:  {metrics['mae']:.6f}\")\n",
    "    print(f\"  R²:   {metrics['r2_score']:.6f}\")\n",
    "\n",
    "print(\"Evaluation metrics implementation completed (fully vectorized)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd95674",
   "metadata": {},
   "source": [
    "## 5. Train All Three Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f6be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING ALL THREE MODELS\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store results for comparison\n",
    "results = {}\n",
    "\n",
    "# ==================== MODEL 1: LINEAR REGRESSION ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 1: LINEAR REGRESSION (for comparison with classification)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "# Train Linear Regression\n",
    "linear_model = LinearRegression(\n",
    "    learning_rate=0.01,\n",
    "    max_iterations=1000,\n",
    "    tolerance=1e-6,\n",
    "    regularization_strength=0.01,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "linear_model.fit(X_train_scaled, y_train)\n",
    "train_time = time() - start_time\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_linear = linear_model.predict(X_train_scaled)\n",
    "y_test_pred_linear = linear_model.predict(X_test_scaled)\n",
    "\n",
    "# Convert to binary for classification metrics (threshold at 0.5)\n",
    "y_train_pred_linear_binary = (y_train_pred_linear >= 0.5).astype(np.int32)\n",
    "y_test_pred_linear_binary = (y_test_pred_linear >= 0.5).astype(np.int32)\n",
    "\n",
    "# Compute metrics\n",
    "train_metrics_linear = compute_metrics(y_train, y_train_pred_linear_binary)\n",
    "test_metrics_linear = compute_metrics(y_test, y_test_pred_linear_binary)\n",
    "regression_metrics = compute_mse_mae(y_test, y_test_pred_linear)\n",
    "\n",
    "print(f\"\\nTraining time: {train_time:.2f} seconds\")\n",
    "print_classification_metrics(test_metrics_linear, \"Linear Regression - TEST SET\")\n",
    "print_regression_metrics(regression_metrics, \"Linear Regression - Regression Metrics\")\n",
    "\n",
    "results['Linear Regression'] = {\n",
    "    'train_time': train_time,\n",
    "    'test_accuracy': test_metrics_linear['accuracy'],\n",
    "    'test_f1': test_metrics_linear['f1_score'],\n",
    "    'test_recall': test_metrics_linear['recall'],\n",
    "    'test_precision': test_metrics_linear['precision']\n",
    "}\n",
    "\n",
    "# ==================== MODEL 2: LOGISTIC REGRESSION ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 2: LOGISTIC REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate class weights for imbalanced data\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "class_weights = {\n",
    "    0: 1.0,\n",
    "    1: class_counts[0] / class_counts[1]  # Automatic weight calculation\n",
    "}\n",
    "\n",
    "print(f\"\\nClass weights: {class_weights}\")\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "# Train Logistic Regression\n",
    "logistic_model = LogisticRegression(\n",
    "    learning_rate=0.01,\n",
    "    max_iterations=1000,\n",
    "    tolerance=1e-6,\n",
    "    regularization_strength=0.1,\n",
    "    class_weights=class_weights,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "train_time = time() - start_time\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_logistic = logistic_model.predict(X_train_scaled, threshold=0.5)\n",
    "y_test_pred_logistic = logistic_model.predict(X_test_scaled, threshold=0.5)\n",
    "\n",
    "# Compute metrics\n",
    "train_metrics_logistic = compute_metrics(y_train, y_train_pred_logistic)\n",
    "test_metrics_logistic = compute_metrics(y_test, y_test_pred_logistic)\n",
    "\n",
    "print(f\"\\nTraining time: {train_time:.2f} seconds\")\n",
    "print_classification_metrics(test_metrics_logistic, \"Logistic Regression - TEST SET\")\n",
    "\n",
    "results['Logistic Regression'] = {\n",
    "    'train_time': train_time,\n",
    "    'test_accuracy': test_metrics_logistic['accuracy'],\n",
    "    'test_f1': test_metrics_logistic['f1_score'],\n",
    "    'test_recall': test_metrics_logistic['recall'],\n",
    "    'test_precision': test_metrics_logistic['precision']\n",
    "}\n",
    "\n",
    "# ==================== MODEL 3: NAIVE BAYES ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 3: GAUSSIAN NAIVE BAYES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "# Train Naive Bayes\n",
    "nb_model = GaussianNaiveBayes(\n",
    "    var_smoothing=1e-9,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "nb_model.fit(X_train_scaled, y_train)\n",
    "train_time = time() - start_time\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_nb = nb_model.predict(X_train_scaled)\n",
    "y_test_pred_nb = nb_model.predict(X_test_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "train_metrics_nb = compute_metrics(y_train, y_train_pred_nb)\n",
    "test_metrics_nb = compute_metrics(y_test, y_test_pred_nb)\n",
    "\n",
    "print(f\"\\nTraining time: {train_time:.2f} seconds\")\n",
    "print_classification_metrics(test_metrics_nb, \"Naive Bayes - TEST SET\")\n",
    "\n",
    "results['Naive Bayes'] = {\n",
    "    'train_time': train_time,\n",
    "    'test_accuracy': test_metrics_nb['accuracy'],\n",
    "    'test_f1': test_metrics_nb['f1_score'],\n",
    "    'test_recall': test_metrics_nb['recall'],\n",
    "    'test_precision': test_metrics_nb['precision']\n",
    "}\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc4abe8",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124d96b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison table using vectorized operations\n",
    "model_names = list(results.keys())\n",
    "metrics_names = ['test_accuracy', 'test_precision', 'test_recall', 'test_f1', 'train_time']\n",
    "metrics_labels = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Train Time (s)']\n",
    "\n",
    "print(f\"\\n{'Model':<25} {'Accuracy':>10} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Time(s)':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_results = results[model_name]\n",
    "    print(f\"{model_name:<25} \"\n",
    "          f\"{model_results['test_accuracy']:>10.4f} \"\n",
    "          f\"{model_results['test_precision']:>10.4f} \"\n",
    "          f\"{model_results['test_recall']:>10.4f} \"\n",
    "          f\"{model_results['test_f1']:>10.4f} \"\n",
    "          f\"{model_results['train_time']:>10.2f}\")\n",
    "\n",
    "# Find best model for each metric using vectorized argmax\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODELS BY METRIC\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "for metric_name, metric_label in zip(metrics_names[:-1], metrics_labels[:-1]):  # Exclude time\n",
    "    # Vectorized extraction of metric values\n",
    "    metric_values = np.array([results[model][metric_name] for model in model_names])\n",
    "    best_idx = np.argmax(metric_values)\n",
    "    best_model = model_names[best_idx]\n",
    "    best_value = metric_values[best_idx]\n",
    "    print(f\"  {metric_label:<15}: {best_model:<25} ({best_value:.4f})\")\n",
    "\n",
    "# Overall recommendation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATION FOR FRAUD DETECTION\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate weighted score (emphasize recall for fraud detection)\n",
    "weighted_scores = {}\n",
    "for model_name in model_names:\n",
    "    model_results = results[model_name]\n",
    "    # Weight: Recall > F1 > Precision > Accuracy (fraud detection priority)\n",
    "    weighted_score = (\n",
    "        0.4 * model_results['test_recall'] +\n",
    "        0.3 * model_results['test_f1'] +\n",
    "        0.2 * model_results['test_precision'] +\n",
    "        0.1 * model_results['test_accuracy']\n",
    "    )\n",
    "    weighted_scores[model_name] = weighted_score\n",
    "\n",
    "# Vectorized best model selection\n",
    "best_model = max(weighted_scores, key=weighted_scores.get)\n",
    "best_score = weighted_scores[best_model]\n",
    "\n",
    "print(f\"\\nBased on weighted scoring (Recall-focused):\")\n",
    "print(f\"  Best Model: {best_model}\")\n",
    "print(f\"  Weighted Score: {best_score:.4f}\")\n",
    "print(f\"\\nKey Strengths:\")\n",
    "if best_model == 'Logistic Regression':\n",
    "    print(f\"  - Balanced performance with class weights\")\n",
    "    print(f\"  - Good recall for fraud detection\")\n",
    "    print(f\"  - Interpretable coefficients\")\n",
    "elif best_model == 'Naive Bayes':\n",
    "    print(f\"  - Fast training and prediction\")\n",
    "    print(f\"  - Works well with independent features\")\n",
    "    print(f\"  - Good probabilistic interpretation\")\n",
    "else:\n",
    "    print(f\"  - Simple baseline model\")\n",
    "    print(f\"  - Linear relationship modeling\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76935583",
   "metadata": {},
   "source": [
    "## 7. Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3240a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Model Performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Model Comparison - Bar Chart\n",
    "ax1 = axes[0, 0]\n",
    "metrics_to_plot = ['test_accuracy', 'test_precision', 'test_recall', 'test_f1']\n",
    "metrics_labels_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.2\n",
    "\n",
    "for i, (metric, label) in enumerate(zip(metrics_to_plot, metrics_labels_plot)):\n",
    "    values = np.array([results[model][metric] for model in model_names])\n",
    "    ax1.bar(x + i * width, values, width, label=label)\n",
    "\n",
    "ax1.set_xlabel('Models', fontweight='bold')\n",
    "ax1.set_ylabel('Score', fontweight='bold')\n",
    "ax1.set_title('Model Performance Comparison', fontweight='bold', pad=15)\n",
    "ax1.set_xticks(x + width * 1.5)\n",
    "ax1.set_xticklabels(model_names, rotation=15, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Training Time Comparison\n",
    "ax2 = axes[0, 1]\n",
    "train_times = np.array([results[model]['train_time'] for model in model_names])\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "bars = ax2.bar(model_names, train_times, color=colors, alpha=0.7)\n",
    "ax2.set_ylabel('Time (seconds)', fontweight='bold')\n",
    "ax2.set_title('Training Time Comparison', fontweight='bold', pad=15)\n",
    "ax2.set_xticklabels(model_names, rotation=15, ha='right')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Confusion Matrix Heatmap - Logistic Regression\n",
    "ax3 = axes[1, 0]\n",
    "cm_logistic = test_metrics_logistic['confusion_matrix']\n",
    "im = ax3.imshow(cm_logistic, cmap='Blues', aspect='auto')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = ax3.text(j, i, cm_logistic[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "ax3.set_xticks([0, 1])\n",
    "ax3.set_yticks([0, 1])\n",
    "ax3.set_xticklabels(['Normal', 'Fraud'])\n",
    "ax3.set_yticklabels(['Normal', 'Fraud'])\n",
    "ax3.set_xlabel('Predicted', fontweight='bold')\n",
    "ax3.set_ylabel('Actual', fontweight='bold')\n",
    "ax3.set_title('Confusion Matrix - Logistic Regression', fontweight='bold', pad=15)\n",
    "plt.colorbar(im, ax=ax3)\n",
    "\n",
    "# 4. Recall vs Precision Trade-off\n",
    "ax4 = axes[1, 1]\n",
    "recalls = np.array([results[model]['test_recall'] for model in model_names])\n",
    "precisions = np.array([results[model]['test_precision'] for model in model_names])\n",
    "\n",
    "scatter = ax4.scatter(recalls, precisions, s=200, c=colors, alpha=0.6, edgecolors='black', linewidth=2)\n",
    "\n",
    "for i, model in enumerate(model_names):\n",
    "    ax4.annotate(model, (recalls[i], precisions[i]), \n",
    "                xytext=(5, 5), textcoords='offset points',\n",
    "                fontweight='bold', fontsize=9)\n",
    "\n",
    "ax4.set_xlabel('Recall (Fraud Detection Rate)', fontweight='bold')\n",
    "ax4.set_ylabel('Precision', fontweight='bold')\n",
    "ax4.set_title('Recall vs Precision Trade-off', fontweight='bold', pad=15)\n",
    "ax4.grid(alpha=0.3)\n",
    "ax4.set_xlim([0, 1.05])\n",
    "ax4.set_ylim([0, 1.05])\n",
    "\n",
    "# Add diagonal reference line\n",
    "ax4.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Balance line')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
