{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9885a859",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection - Data Preprocessing\n",
    "\n",
    "## HỌ VÀ TÊN: Cao Tấn Hoàng Huy\n",
    "## MSSV: 23127051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8470323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo các thư viện cần thiết\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d7c44f",
   "metadata": {},
   "source": [
    "## Load data from exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db215266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from exploration notebook\n",
    "print(\"Loading data from exploration phase...\")\n",
    "\n",
    "try:\n",
    "    # Load from saved files if available\n",
    "    data = np.load('data_exploration_data.npy')\n",
    "    column_names = np.load('data_exploration_column_names.npy', allow_pickle=True).tolist()\n",
    "    print(f\"Data loaded from saved files: {data.shape}\")\n",
    "except:\n",
    "    # Load fresh if saved files don't exist\n",
    "    print(\"Loading fresh data from CSV...\")\n",
    "    \n",
    "    with open('creditcard.csv', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Skip header và lấy tên columns\n",
    "    header = lines[0].strip().replace('\"', '').split(',')\n",
    "    \n",
    "    data_list = []\n",
    "    error_lines = []\n",
    "    \n",
    "    for i, line in enumerate(lines[1:], 1):\n",
    "        try:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            values = line.split(',')\n",
    "            \n",
    "            float_values = []\n",
    "            for val in values:\n",
    "                val = val.strip().strip('\"')\n",
    "                float_values.append(float(val))\n",
    "            \n",
    "            if len(float_values) == 31:\n",
    "                data_list.append(float_values)\n",
    "            else:\n",
    "                error_lines.append((i, len(float_values)))\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_lines.append((i, str(e)))\n",
    "    \n",
    "    data = np.array(data_list, dtype=np.float64)\n",
    "    column_names = header\n",
    "    \n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"  Shape: {data.shape}\")\n",
    "print(f\"  Columns: {len(column_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d789e",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5000a9b",
   "metadata": {},
   "source": [
    "## 1. Tách features và target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c208caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tách features và target variable\n",
    "print(\"Separating features and target variable...\")\n",
    "\n",
    "# Tách X (features) và y (target)\n",
    "X = data[:, :-1]  # Tất cả columns trừ column cuối\n",
    "y = data[:, -1]   # Column cuối là Class\n",
    "\n",
    "# Feature names (không bao gồm Class)\n",
    "feature_names = column_names[:-1]\n",
    "\n",
    "print(f\"Features (X):\")\n",
    "print(f\"  Shape: {X.shape}\")\n",
    "print(f\"  Features: {len(feature_names)}\")\n",
    "\n",
    "print(f\"\\nTarget (y):\")\n",
    "print(f\"  Shape: {y.shape}\")\n",
    "print(f\"  Unique values: {np.unique(y)}\")\n",
    "\n",
    "# Quick view of features\n",
    "print(f\"\\nFeature names:\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"  {i:2d}: {name}\")\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    percentage = (count / len(y)) * 100\n",
    "    label = \"Normal\" if cls == 0.0 else \"Fraud\"\n",
    "    print(f\"  {label} ({cls}): {count:>6,} samples ({percentage:>5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d59d60d",
   "metadata": {},
   "source": [
    "## 2. Standardization Implementation (Pure NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1010264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler implementation với NumPy only\n",
    "class StandardScaler:\n",
    "    def __init__(self):\n",
    "        self.mean_ = None\n",
    "        self.std_ = None\n",
    "        self.n_features_ = None\n",
    "        self.fitted = False\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Tính mean và std từ training data\"\"\"\n",
    "        X = np.array(X, dtype=np.float64)\n",
    "        self.n_features_ = X.shape[1]\n",
    "        \n",
    "        # Tính mean và std cho mỗi feature\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        self.std_ = np.std(X, axis=0, ddof=1)  # ddof=1 cho sample std\n",
    "        \n",
    "        # Tránh division by zero\n",
    "        self.std_ = np.where(self.std_ == 0, 1, self.std_)\n",
    "        \n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply standardization using fitted parameters\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Scaler chưa được fit. Hãy gọi fit() trước.\")\n",
    "        \n",
    "        X = np.array(X, dtype=np.float64)\n",
    "        \n",
    "        if X.shape[1] != self.n_features_:\n",
    "            raise ValueError(f\"Số features không khớp. Expected {self.n_features_}, got {X.shape[1]}\")\n",
    "        \n",
    "        # Standardize: (x - mean) / std\n",
    "        X_scaled = (X - self.mean_) / self.std_\n",
    "        \n",
    "        return X_scaled\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit và transform in one step\"\"\"\n",
    "        return self.fit(X).transform(X)\n",
    "    \n",
    "    def inverse_transform(self, X_scaled):\n",
    "        \"\"\"Chuyển về scale gốc\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Scaler chưa được fit.\")\n",
    "        \n",
    "        X_scaled = np.array(X_scaled, dtype=np.float64)\n",
    "        X_original = X_scaled * self.std_ + self.mean_\n",
    "        \n",
    "        return X_original\n",
    "\n",
    "print(\"StandardScaler implementation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be9a032",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split Implementation (Pure NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Stratified train-test split implementation với NumPy only\n",
    "    Đảm bảo tỷ lệ các class được giữ nguyên trong cả train và test set\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    n_samples = len(X)\n",
    "    unique_classes = np.unique(y)\n",
    "    \n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    for class_label in unique_classes:\n",
    "        # Tìm indices của class này\n",
    "        class_indices = np.where(y == class_label)[0]\n",
    "        n_class_samples = len(class_indices)\n",
    "        \n",
    "        # Tính số samples cho test set của class này\n",
    "        n_test_samples = int(np.round(n_class_samples * test_size))\n",
    "        \n",
    "        # Random shuffle indices của class này\n",
    "        shuffled_indices = np.random.permutation(class_indices)\n",
    "        \n",
    "        # Split thành train và test\n",
    "        test_indices.extend(shuffled_indices[:n_test_samples])\n",
    "        train_indices.extend(shuffled_indices[n_test_samples:])\n",
    "    \n",
    "    # Convert về numpy arrays\n",
    "    train_indices = np.array(train_indices)\n",
    "    test_indices = np.array(test_indices)\n",
    "    \n",
    "    # Shuffle lại để không bị grouped by class\n",
    "    train_indices = np.random.permutation(train_indices)\n",
    "    test_indices = np.random.permutation(test_indices)\n",
    "    \n",
    "    # Split data\n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "print(\"Stratified train-test split implementation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b1c731",
   "metadata": {},
   "source": [
    "## 4. Perform Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cf3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thực hiện train-test split\n",
    "print(\"Performing stratified train-test split...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = stratified_train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit results:\")\n",
    "print(f\"  Training set: {X_train.shape}\")\n",
    "print(f\"  Test set:     {X_test.shape}\")\n",
    "\n",
    "# Kiểm tra class distribution sau split\n",
    "print(f\"\\nClass distribution verification:\")\n",
    "print(f\"Original dataset:\")\n",
    "unique_orig, counts_orig = np.unique(y, return_counts=True)\n",
    "for cls, count in zip(unique_orig, counts_orig):\n",
    "    pct = (count / len(y)) * 100\n",
    "    label = \"Normal\" if cls == 0 else \"Fraud\"\n",
    "    print(f\"  {label}: {count:>6,} ({pct:>5.2f}%)\")\n",
    "\n",
    "print(f\"\\nTraining set:\")\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "for cls, count in zip(unique_train, counts_train):\n",
    "    pct = (count / len(y_train)) * 100\n",
    "    label = \"Normal\" if cls == 0 else \"Fraud\"\n",
    "    print(f\"  {label}: {count:>6,} ({pct:>5.2f}%)\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "for cls, count in zip(unique_test, counts_test):\n",
    "    pct = (count / len(y_test)) * 100\n",
    "    label = \"Normal\" if cls == 0 else \"Fraud\"\n",
    "    print(f\"  {label}: {count:>6,} ({pct:>5.2f}%)\")\n",
    "\n",
    "print(f\"\\nStratification successful! Class distributions are preserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b80ba7e",
   "metadata": {},
   "source": [
    "## 5. Feature Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed111b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Standardization\n",
    "print(\"Performing feature standardization...\")\n",
    "\n",
    "# Khởi tạo và fit scaler trên training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform cả train và test\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nStandardization completed!\")\n",
    "print(f\"  Training set scaled: {X_train_scaled.shape}\")\n",
    "print(f\"  Test set scaled:     {X_test_scaled.shape}\")\n",
    "\n",
    "# Verification - check means và stds của scaled training data\n",
    "print(f\"\\nVerification of standardization:\")\n",
    "print(f\"  Training set means (first 5 features): {np.mean(X_train_scaled[:, :5], axis=0)}\")\n",
    "print(f\"  Training set stds (first 5 features):  {np.std(X_train_scaled[:, :5], axis=0, ddof=1)}\")\n",
    "\n",
    "# Original vs Scaled comparison\n",
    "print(f\"\\nOriginal vs Scaled comparison (first feature):\")\n",
    "print(f\"  Original - Mean: {np.mean(X_train[:, 0]):.6f}, Std: {np.std(X_train[:, 0], ddof=1):.6f}\")\n",
    "print(f\"  Scaled   - Mean: {np.mean(X_train_scaled[:, 0]):.6f}, Std: {np.std(X_train_scaled[:, 0], ddof=1):.6f}\")\n",
    "\n",
    "print(f\"\\nFeature standardization completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70677092",
   "metadata": {},
   "source": [
    "## 6. Polynomial Features Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb50c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polynomial_features(X, degree=2, include_bias=False, interaction_only=False):\n",
    "    \"\"\"\n",
    "    Tạo polynomial features với NumPy only\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape [n_samples, n_features]\n",
    "        Input data\n",
    "    degree : int, default=2\n",
    "        Polynomial degree\n",
    "    include_bias : bool, default=False\n",
    "        Whether to include bias column (all 1s)\n",
    "    interaction_only : bool, default=False\n",
    "        Whether to produce interaction features only\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_poly : array, shape [n_samples, n_output_features]\n",
    "        Transformed data with polynomial features\n",
    "    \"\"\"\n",
    "    X = np.array(X, dtype=np.float64)\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    if degree < 1:\n",
    "        raise ValueError(\"degree phải >= 1\")\n",
    "    \n",
    "    # Start with original features\n",
    "    poly_features = []\n",
    "    \n",
    "    if include_bias:\n",
    "        # Add bias column\n",
    "        poly_features.append(np.ones((n_samples, 1)))\n",
    "    \n",
    "    if not interaction_only or degree == 1:\n",
    "        # Add original features\n",
    "        poly_features.append(X)\n",
    "    \n",
    "    if degree >= 2:\n",
    "        # Add degree 2 features\n",
    "        for i in range(n_features):\n",
    "            for j in range(i, n_features):\n",
    "                if i == j and interaction_only:\n",
    "                    continue  # Skip self-interactions in interaction_only mode\n",
    "                feature = (X[:, i] * X[:, j]).reshape(-1, 1)\n",
    "                poly_features.append(feature)\n",
    "    \n",
    "    # Concatenate all features\n",
    "    X_poly = np.concatenate(poly_features, axis=1)\n",
    "    \n",
    "    return X_poly\n",
    "\n",
    "print(\"Polynomial features implementation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a24420",
   "metadata": {},
   "source": [
    "## 7. Create Enhanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo polynomial features cho training và test sets\n",
    "print(\"Creating polynomial features...\")\n",
    "\n",
    "# Create polynomial features (degree=2, interaction terms)\n",
    "X_train_poly = create_polynomial_features(\n",
    "    X_train_scaled, \n",
    "    degree=2, \n",
    "    include_bias=False,\n",
    "    interaction_only=True  # Only interaction terms, not squares\n",
    ")\n",
    "\n",
    "X_test_poly = create_polynomial_features(\n",
    "    X_test_scaled, \n",
    "    degree=2, \n",
    "    include_bias=False,\n",
    "    interaction_only=True\n",
    ")\n",
    "\n",
    "print(f\"\\nPolynomial feature creation completed!\")\n",
    "print(f\"  Original features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"  Enhanced features: {X_train_poly.shape[1]}\")\n",
    "print(f\"  Training set: {X_train_poly.shape}\")\n",
    "print(f\"  Test set:     {X_test_poly.shape}\")\n",
    "\n",
    "print(f\"\\nFeature enhancement completed!\")\n",
    "print(f\"Data is ready for modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed9167a",
   "metadata": {},
   "source": [
    "## Save preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de483186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data for modeling\n",
    "print(\"\\nSaving preprocessed data for modeling...\")\n",
    "\n",
    "# Save train-test splits\n",
    "np.save('X_train_scaled.npy', X_train_scaled)\n",
    "np.save('X_test_scaled.npy', X_test_scaled)\n",
    "np.save('X_train_poly.npy', X_train_poly)\n",
    "np.save('X_test_poly.npy', X_test_poly)\n",
    "np.save('y_train.npy', y_train)\n",
    "np.save('y_test.npy', y_test)\n",
    "\n",
    "# Save feature names and scaler parameters\n",
    "np.save('feature_names.npy', feature_names)\n",
    "np.save('scaler_mean.npy', scaler.mean_)\n",
    "np.save('scaler_std.npy', scaler.std_)\n",
    "\n",
    "print(f\"\\nPreprocessing completed!\")\n",
    "print(f\"Files saved:\")\n",
    "print(f\"  - X_train_scaled.npy: {X_train_scaled.shape}\")\n",
    "print(f\"  - X_test_scaled.npy:  {X_test_scaled.shape}\")\n",
    "print(f\"  - X_train_poly.npy:   {X_train_poly.shape}\")\n",
    "print(f\"  - X_test_poly.npy:    {X_test_poly.shape}\")\n",
    "print(f\"  - y_train.npy:        {y_train.shape}\")\n",
    "print(f\"  - y_test.npy:         {y_test.shape}\")\n",
    "print(f\"  - feature_names.npy\")\n",
    "print(f\"  - scaler_mean.npy\")\n",
    "print(f\"  - scaler_std.npy\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
